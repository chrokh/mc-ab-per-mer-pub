---
title: Time for public pharma?
subtitle: Supplementary material
author: Christopher Okhravi
date: 2019
output:
  html_document:
    toc: true
    #toc_float:
    #  collapsed: false
    theme: cosmo
  pdf_document:
    toc: true
    latex_engine: xelatex
---

<!--
# TODO: Something I haven't taken into consideration is that one can assume that a prize has a fixed number of max-recipients but still assume that all developers will believe that they will be the ones who get it. This should significantly reduce the cost per output antibiotic.
# TODO: I should make all Sertkaya data separate data sets. I.e. use all the different indications.
-->

```{r, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, collapse=TRUE)
```

```{r, libraries, include=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(triangle)
```

# Introduction
This document serves as supplementary material for the paper "Time for public pharma?".
This is a monte carlo simulation that explores direct versus indirect funding of antibiotics research and development (R&D).
Direct funding is here used to mean that a benefactor pays for antibiotics R&D at cost.
Indirect funding is here used to mean that a benefactor issues non-dilutive prizes to whoever completes a particular phase, with the intent of incentivizing private developers to undertake said and prior phases.
This analysis is attempting to estimate which of the two options would be cheaper, ceteris paribus, for the benefactor.

# Simulation and input data set

Let us begin by setting `N` to the number of samples that we want per data set.
This number represents the number of random samples that we'll draw from a stochastic representation of a hypothetical antibiotic R&D project.
The distribution of the stochastic project depends on the data set we're using, and in a simple attempt to avoid drawing data set specific conclusions, we'll look at multiple data sets.
Meaning that if one considers all the data sets at once then the number of random projects drawn is `N` multiplied with the number of data sets.

```{r}
N = 1000
```

## Sertkaya et. al (2014)

The following is an approximation of the data used by Sertkaya et. al (2014) as they make a few assumptions that are not reconcilable with the way we have chosen to model antibiotics R&D.
The points of differentiation are outlined as comments in the code chunk below.

<div class="alert alert-danger">
**Sertkaya uses triangular distributions, but we have used uniform distributions. This must be changed.**
</div>

```{r}
pc <- data.frame(
  phase = 'PC',
  time  = runif(N, min=4.3, max=6),
  cost  = runif(N, min=19, max=23.2),
  prob  = runif(N, min=0.175, max=0.69),
  sales = 0)
p1 <- data.frame(
  phase = 'P1',
  time  = runif(N, min=0.75, max=1.8),
  prob  = runif(N, min=0.25, max=0.837),
  cost  = runif(N, min=7.3, max=12),
  sales = 0)
p2 <- data.frame(
  phase = 'P2',
  time  = runif(N, min=0.75, max=2.5),
  cost  = runif(N, min=7.12, max=18.72),
  prob  = runif(N, min=0.34, max=0.74),
  sales = 0)
p3 <- data.frame(
  phase = 'P3',
  cost  = runif(N, min=26.88, max=121.68),
  prob  = runif(N, min=0.314, max=0.786),
  time  = runif(N, min=0.83, max=3.9),
  sales = 0)
p4 <- data.frame(
  phase = 'P4',
  time  = runif(N, min=0.5, max=1.04),
  prob  = runif(N, min=0.83, max=0.99),
  cost  = 98.297168,
  sales = 0)
mp <- data.frame(
  phase = 'MP',
  time  = 10,
  prob  = 1,
  cost  = 0,
  sales = runif(N, min=218, max=2500))

# Combine all phases into single dataset
sertkaya2014 <- rbind(pc, p1, p2, p3, p4, mp)
sertkaya2014$subject <- 1:N

# Discount rate is the same across all phases
sertkaya2014$discount_rate_priv <- runif(N, 0.09, 0.24)

# Set source name
sertkaya2014$src <- 'Sertkaya et. al (2014)'
```

## DRIVE-AB (2018)
This is an approximation of the data used in DRIVE-AB final report (2018).
Some deviations (reported as comments in the code chunk below) have been made as some of the DRIVE-AB assumptions are not compatible with our assumptions.

```{r}
pc <- data.frame(
  phase = 'PC',
  time  = rtriangle(N, 4.33, 6, 5.5),
  cost  = rtriangle(N, 14.25, 29, 21.1),
  prob  = rtriangle(N, 0.175, 0.69, 0.352),
  sales = 0)
p1 <- data.frame(
  phase = 'P1',
  time  = rtriangle(N, 0.75, 1.8, 0.875),
  prob  = rtriangle(N, 0.25, 0.837, 0.33),
  cost  = rtriangle(N, 13.1, 37.96, 24),
  sales = 0)
p2 <- data.frame(
  phase = 'P2',
  time  = rtriangle(N, 0.75, 2.5, 1.08),
  prob  = rtriangle(N, 0.34, 0.74, 0.5),
  cost  = rtriangle(N, 4.55, 46.36, 12.95), # TODO: Cost is incorrectly reported in DRIVE-AB report!
  sales = 0)
p3 <- data.frame(
  phase = 'P3',
  time  = rtriangle(N, 0.83, 3.83, 1.82),
  prob  = rtriangle(N, 0.314, 0.786, 0.67),
  cost  = rtriangle(N, 10, 47, 21.8),
  sales = 0)
p4 <- data.frame(
  phase = 'P4',
  time  = rtriangle(N, 0.5, 1.04, 0.75),
  prob  = rtriangle(N, 0.83, 0.99, 0.85),
  cost  = rtriangle(N, 55.5, 127.91, 88.35),
  sales = 0)
mp <- data.frame(
  phase = 'MP',
  time  = 10,
  prob  = 1,
  cost  = 0,
  sales = rtriangle(N, 0, 4336.00, 2559.5)) # TODO: Does not model DRIVE-AB report correctly as the report does not assume that y1 has 0 sales.

# Combine all phases into single dataset
driveab2018 <- rbind(pc, p1, p2, p3, p4, mp)
driveab2018$subject <- 1:N

# Discount rate is the same across all phases
driveab2018$discount_rate_priv <- runif(N, 0.05, 0.30)

# Set source name
driveab2018$src <- 'DRIVE-AB (2018)'
```


## Combining all datasets
Before proceeding, we'll combine all datasets (sources) into a single dataset containing all sources.

```{r}
phases <- rbind(sertkaya2014, driveab2018)
```

```{r, include=FALSE}
# To avoid recomputing all the time we'll also store the names of the different sources:
sources <- unique(phases$src)
# To avoid misspellings of factors, lets' use levels:
metrics <- c('cashflow', 'ev', 'pv', 'epv')
cum_metrics <- c('cum', 'env', 'npv', 'enpv')
```

We must also convert `phase` into an ordered factor so that we can assume that `PC < P1 < P2 < P3 < P4 < MP`.

```{r}
phase_names <- c('PC','P1','P2','P3','P4','MP')
phase_levels <- factor(phase_names, levels=phase_names, ordered=TRUE)
phases$phase <- factor(phases$phase, levels=phase_names, ordered=TRUE)
```

Later we will apply interventions to this dataset and thereby create multiple permutations/versions of every observation.
This means that we must keep track of which intervention we're currently looking at, and as such we'll add that column immediately, and declare all the current data as suffering from "no intervention".
In terms of a randomized controlled trial, this is the "control group".
When we apply different interventions to the control group we will thus create different "treatment groups".
As the interventions are applied to the control group rather than to completely new samples, we are able to perform "within subject analysis" rather than having to resort to "across subject analysis".

```{r}
phases$prizes <- 0
phases$intervention <- 'NONE'
```

Finally, let us add a "public discount rate".
Meaning the cost of capital for the benefactor, i.e. for the body that pays the intervention with no expectation of monetary return.
This parameter is used in the analysis, where its raison d'etre is also further explained.

```{r}
publ_dr_min = 0.035
publ_dr_max = 0.045
```

We assume that the benefactor is the public sector and use a uniformly distributed discount rate between `r publ_dr_min * 100`% and `r publ_dr_max * 100`%.
Public discount rate varies across subjects but not within.
The same subject has the same public discount rate across all datasets (i.e. sources).

```{r}
phases$discount_rate_publ <- runif(N, publ_dr_min, publ_dr_max)
```

## Summary statistics
Let us plot the phase parameters as frequency polygons (i.e. histograms) for each of the sources.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
p1 <- ggplot(phases, aes(cost, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab('Cost (million USD)')
p2 <- ggplot(phases, aes(time, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab('Duration (months)')
p3 <- ggplot(phases, aes(prob*100, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab('Probability (%)')
p4 <- ggplot(phases, aes(sales, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  xlab('Sales (million USD)')
p5 <- ggplot(phases, aes(discount_rate_priv*100, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  xlab('Discount rate (%)')
ggarrange(p1, p2, p3, p4, p5, ncol=3, nrow=2, common.legend=TRUE, legend='bottom')
```

Interestingly, some parameters are quite dispersly distributed between phases.
Consider e.g. how almost all of the cost is incurred in some phase(s) while all the time is spent in another.
The two following figures plot what percentage of the total of some property is spent in a given phase, per project.
The first plot is grouped by property, while the second by phase.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Transform: phase properties to long from wide
phase_props <- phases %>%
  filter(phase != 'MP') %>%
  select(src, subject, phase, cost, time, prob) %>%
  gather(key='prop', value='value', -src, -subject, -phase) %>%
  group_by(src, subject, prop) %>%
  mutate(total = sum(value),
         ratio = value / total) # NOTE: will cause NaN if 0/0

for (curr in sources) {
  p1 <- phase_props %>%
    filter(src == curr) %>%
    filter(!is.na(ratio)) %>%
    ggplot(aes(x=phase,y=ratio*100, fill=phase)) +
    geom_violin(draw_quantiles=c(0.25, 0.5, 0.75)) +
    facet_grid(~ prop) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          axis.ticks.x=element_blank()) +
      ylab('% of property in phase') + xlab('') +
      guides(fill = FALSE)

  p2 <- phase_props %>%
    filter(src == curr) %>%
    ggplot(aes(x=prop,y=ratio*100, fill=prop)) +
    geom_violin(draw_quantiles=c(0.25, 0.5, 0.75)) +
    facet_grid(~ phase) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          axis.ticks.x=element_blank()) +
        ylab('% of property in phase') + xlab('') +
        guides(fill = FALSE)

  grid.arrange(p1, p2, ncol=1, top=curr)
}
```

Computing and plotting the mean value perhaps tells the same story a bit more simply.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
for (curr in sources) {
  phase_props_summary <- phase_props %>%
    filter(src == curr) %>%
    group_by(phase, prop) %>%
    summarise(ratio.mean = mean(ratio))

  p1 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=prop, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=phase), position='dodge') +
    ylab('Mean % of property in phase') +
    guides(fill = FALSE) +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  p2 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=prop, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=phase), position='stack') +
    labs(fill='') +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  p3 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=phase, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=prop), position='dodge') +
    labs(fill='') +
    ylab('Mean % of property in phase') +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  grid.arrange(p2, p3, ncol=2, top=curr)
}
```



# Valuation metrics
We employ the following financial metrics for cashflows:

- Non-capitalized value / Out-of-pocket value
- Risk-adjusted value (rV) / Expected value (EV)
- Capitalized value / Present value (PV)
- Risk-adjusted present value (rPV) / Expected present value (EPV)

Symmetrically, when cumulating cashflows, the above financial metrics are known as:

- Cumulative non-capitalized value / Cumulative out-of-pocket value
- Cumulative risk-adjusted value (Cumulative rV) / Cumulative expected value (Cumulative EV)
- Net present value (NPV)
- Risk-adjusted net present value (rNPV) / Expected net present value (ENPV)

We compute **Expected Value (EV)** as:

$$
EV_t = (R_t - C_t) * P_t
$$

where $R_t$ and $C_t$ are the revenues and costs (respectively) at time $t$, and $P_t$ the probability of reaching the cashflow from the point of evaluation.
The probability of reaching a given timestep $t_n$ from a point of evaluation $t_0$ is simply computed as: $P_t = \prod_{t_0}^{t^n}$.
Next, we compute **Present Value (PV)** as:

$$
\mathit{PV}_t = \frac{R_t - C_t}{(1 + i)^t}
$$

where $i$ is the discount rate of the evaluator, and $t$ is the time to the phase from the point of evaluation.
We compute **Expected Present Value (EPV)** as:

$$
\mathit{EPV}_t = \frac{R_t - C_t}{(1 + i)^t} * P_t
$$

Moving on to the cumulative valuations, we compute **Net Expected Value (ENV)**, **Net Present Value (NPV)**, and **Expected Net Present Value (ENPV)** as:
$\sum_{t\in T} \mathit{EV}_t$

$$
\mathit{ENV_T} = \sum_{t\in T} EV_t\\
$$

$$
\mathit{NPV_T} = \sum_{t\in T} \mathit{PV}_t\\
$$

$$
\mathit{ENPV_T} = \sum_{t\in T} \mathit{EPV}_t\\
$$

respectively, where $t$ is the time to the phase, and $T$ is the times to all timesteps of all phases for the project in evaluation.

<div class="alert alert-danger">
TODO: Verify that the probability portion of ENPV is actually computed accordingly!
</div>


# Valuation methods
To compute valuation metrics for a given project we must make an assumption as to how a hypothetical evaluator chooses to transform a sequence of discrete phases into a sequence of discrete and uncertain cashflows.
We make the assumption that the evaluator converts every phase into a series of years in order to properly discount the cashflow of the given phase.
This method yields slightly different results when compared to simply applying financial valuation metrics to the sequence of phases under the assumption that all cashflows for a given phase occur immediately upon entering that phase.s when compared to simply applying financial valuation metrics to the sequence of phases under the assumption that all cashflows for a given phase occur immediately upon entering that phase.
To elucidate the consequence of these differences we will first apply financial metrics directly to the phase-based data, then transform the original data to a year-based form and apply the same metrics, and then finally compare the two.


## Phase-based method
As phase durations are long, the time value of money not only greatly reduces the attractiveness of revenues, but also dampen the pain of costs.
If we disregard the fact that some phases (such as e.g. pre-clinical) are lengthy, and assume that the cashflows related to a phase all happen immediately upon phase entry, then we can trivially compute Expected Value (EV), Present Value (PV), and Expected Present Value (EPV) of all phases from the perspective of all phases.
We can then cumulate these metrics in order to, respectively, compute cumulative EV, Net Present Value (NPV), and Expected Net Present Value (ENPV) from any phase to any phase.

However, before performing this computation we must make an assumption as to how the free market revenues are distributed over the time spent in market.
Assuming that all revenues are secured directly upon market entry is a wildly inadequate assumption that will significantly skew the results due to discounting (i.e. the time value of money).

Instead, we will assume that revenues linearly increase over time for a period of 10 years.
Specifically, we assume that the revenues of year 0 is 0 and then linearly increase until the area under the curve is equal to the total global net sales.
The first year will thus be non-zero if the market revenues of the project is non-zero.
We compute the yearly revenues by adding yearly observations to our datasets and then removing the old market observations, as follows:

```{r}
phasely <- data.frame(phases)
market_years <- phases %>% filter(phase == 'MP')
sales_slope <- (market_years$sales * 2 / (market_years$time + 1)) / market_years$time
for (yr in 0:max(market_years$time)) {
  myr <- data.frame(market_years)
  myr$phase <- paste('Y', yr, sep = '')
  myr$time  <- 1
  myr$prob  <- myr$prob ^ (1 / myr$time)
  myr$cost  <- myr$cost / myr$time
  myr$sales <- ifelse(myr$time <= yr, yr * sales_slope, 0)
  phasely <- rbind(phasely, myr)
}
# Remove market phases since we've now added market years:
phasely <- filter(phasely, phase != 'MP')
# Re-code to ordered factor:
myr_names <- c('Y0','Y1','Y2','Y3','Y4','Y5','Y6','Y7','Y8','Y9','Y10')
phasely$phase <- factor(phasely$phase, levels=c(phase_names, myr_names), ordered=TRUE)
```

We can now compute the valuation metrics discussed above as follows:

```{r}
# TODO: Use the same function for all valuations.
# Should not matter whether it's phasely or yearly.
phase_based_valuation <- function(phs) {
  result <- tibble()
  for (from in phase_levels) {
    rows <- phs %>%
      filter(phase >= from) %>%
      group_by(src, intervention, subject) %>%
      arrange(src, intervention, subject, phase) %>%
      mutate(from = factor(from, levels=phase_names, ordered=TRUE),
             time_to = cumsum(time) - time,
             cum_prob = cumprod(prob),
             prob_to = cum_prob / prob,
             # cashflows
             cashflow = sales + prizes - cost,
             ev = cashflow * prob_to,
             pv = cashflow / ((1 + discount_rate_priv) ^ time_to),
             epv = pv * prob_to,
             # cumulatives
             cum = cumsum(cashflow),
             env = cumsum(ev),
             npv = cumsum(pv),
             enpv = cumsum(epv),
      )
      result <- bind_rows(result, rows)
  }
  result
}
```

Before plotting we must do a bit of data wrangling.

```{r}
phasely_phases_from_phases <- phase_based_valuation(phasely)

# Long
phasely_phases_from_phases_long <- phasely_phases_from_phases %>%
  select(src, intervention, subject, from, phase,
         cashflow, ev, pv, epv,
         cum, env, npv, enpv) %>%
  gather('valuation', 'value', -src, -intervention, -subject, -from, -phase) %>%
  transform(valuation = factor(valuation, levels = c(metrics, cum_metrics)))

# Long summary
phasely_phases_from_phases_long_sum <- phasely_phases_from_phases_long %>%
  group_by(src, from, phase, valuation) %>%
  mutate(mu  = mean(value),
         med = median(value),
         std = sd(value))

# Final long
phasely_final_from_phases_long <- phasely_phases_from_phases_long %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, subject, from, valuation) %>%
  arrange(phase) %>%
  summarize(value = tail(value, n=1))

# Final long summary
phasely_final_from_phases_long_sum <- phasely_phases_from_phases_long_sum %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, from, valuation) %>%
  arrange(phase) %>%
  summarize(mu  = tail(mu, n=1),
            med = tail(med, n=1),
            std = tail(std, n=1))
```

Let us then plot the value of taking the project from beginning (i.e. pre-clinical) to end (i.e. the final market year), using our cumulative valuation metrics.
Note the usage of different scales across the different metrics due the vastly different values.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
phasely_final_from_phases_long %>%
  filter(from == 'PC' & valuation %in% cum_metrics) %>%
  ggplot(aes(valuation, value, fill=src)) +
  geom_boxplot() +
  xlab(element_blank()) +
  ylab(element_blank()) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  facet_wrap(. ~ valuation, scale='free', nrow=1)
```

Another way to look at the data is to consider how starting at different phases alter the value of the project.
Below we plot the mean value of bringing the project to completion from various starting phases, using the four cumulative metrics.
The vertical lines delimit +/- 1 standard deviation from the sample mean (i.e. ~68% of the data).

```{r, echo=FALSE}
pos <- position_dodge(0.4)
phasely_final_from_phases_long_sum %>%
  ggplot(aes(from, mu, color=valuation, group=valuation)) +
  geom_linerange(aes(ymin=mu-std, ymax=mu+std), position=pos, size=1) +
  geom_line(position=pos, size=1, linetype='dotted') +
  geom_point(position=pos, size=2) +
  facet_wrap(. ~ src, ncol=2) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab(element_blank()) + ylab(element_blank())
```

## Year-based method
To consider the fact that all cashflows of a phase do not occur immediately upon phase-entry we convert our phase-based data to year-based data and make assumptions as to how the values are interpolated over the years of a phase.

We assume that all properties are constantly distributed over the time of the phase except for revenues.
We assume that the revenues of year 0 is 0 and then linearly increase until the area under the curve is equal to the total revenues of the phase.
The first year will thus be non-zero if the total revenues of the project is non-zero.

We use the following function to convert from phase-form to year-form:

```{r, results='hide'}
#' Converts a data set in phase-form to year-form.
#'
#' @param phases Dataframe in phase-form.
#' @return Dataframe in year-form
to_years <- function (phases) {

  # Convert phases to ordered factor
  phases$phase <- factor(phases$phase, levels=phase_levels, ordered=TRUE)

  # Add timestamps to every observation
  phases <- phases %>%
    group_by(src, intervention, subject) %>%
    arrange(src, intervention, subject, phase) %>%
    mutate(t = cumsum(time) - time)

  # Compute cost steps
  phases$cost_step <- (phases$cost / phases$time)
  phases$cost_remainder <- phases$cost - phases$cost_step * floor(phases$time)

  # Compute prob steps
  phases$prob_step <- phases$prob ^ (1 / phases$time)
  phases$prob_remainder <- phases$prob / (phases$prob_step ^ floor(phases$time))

  # Compute sales slopes
  phases$sales_slope <- (phases$sales * 2 / (phases$time + 1)) / phases$time

  # Transform: To cashflows over time (phase yearly)
  phase_years <- tibble()
  for (x in 1:ceiling(max(phases$time) + 1)) {

    # Compute step-based properties
    whole_step <- x <= phases$time
    cost <- ifelse(whole_step, phases$cost_step, phases$cost_remainder)
    sales <- ifelse(whole_step, phases$sales_slope * x, 0)
    prob <- ifelse(whole_step, phases$prob_step, phases$prob_remainder)
    prizes <- if (x == 1) phases$prizes else 0 # Immediate lump-sum

    # Make tibble
    has_decimals <- phases$time - floor(phases$time) != 0
    part_of_phase <- x <= phases$time | (x <= phases$time + 1 & has_decimals)
    year <- tibble(part_of_phase,
                   src = phases$src,
                   intervention = phases$intervention,
                   subject = phases$subject,
                   phase_year = x,
                   t = phases$t + x - 1,
                   phase = phases$phase,
                   discount_rate_publ = phases$discount_rate_publ,
                   cost,
                   sales,
                   prizes,
                   prob,
                   time = phases$time,
                   discount_rate_priv = phases$discount_rate_priv
                   ) %>%
    filter(x <= time | (x <= time + 1 & has_decimals)) %>% # Only years in phase
    select(-c('part_of_phase')) # Remove temporary column

    # Append
    phase_years <- bind_rows(phase_years, year)
  }

  phase_years
}
```

Using this function we can now convert our phase-based data to year-based data.
It should be noted that the function does not distribute the phase-based data over a series of equidistant years.
Data points are only equidistant within a phase, but not necessarily across.
In other words, if P1 entry would occur after 5.3 years then we will distribute PC properties over the 6 first years.
If the duration of P1 is 2.5 years then P2 would start at year 5.3 and end at year 7.8.
While we would divide P1 into three equidistant (years) points (years), that start from year 5.3, 6.3, and 7.3 (respectively) they are not equidistant from the PC steps.

```{r}
years <- to_years(phases)
```

We can now compute the same valuation metrics we computed for the phase-based method but this time for the year-based method:

```{r}
years_from_phases <- tibble()
for (from in phase_levels) {
  pyfp <- years %>%
    filter(phase >= from) %>%
    group_by(src, subject, intervention) %>%
    arrange(t) %>%
    mutate(from     = factor(from, levels=phase_levels, ordered=TRUE),
           time_to  = t - min(t),
           year     = floor(time_to),
           cum_prob = cumprod(prob),
           prob_to  = cum_prob / prob,
           # cashflows
           cashflow = sales + prizes - cost,
           ev       = cashflow * prob_to,
           pv       = cashflow / ((1 + discount_rate_priv) ^ time_to),
           epv      = pv * prob_to,
           # cumulatives
           cum      = cumsum(cashflow),
           env      = cumsum(ev),
           npv      = cumsum(pv),
           enpv     = cumsum(epv))
    years_from_phases <- bind_rows(years_from_phases, pyfp)
}
```

Again, we must follow this up with some data wrangling to prepare for analysis.

```{r}
# Long
years_from_phases_long <- years_from_phases %>%
  select(src, intervention, subject, from, year,
         cashflow, ev, pv, epv,
         cum, env, npv, enpv) %>%
  gather('valuation', 'value', -src, -intervention, -subject, -from, -year) %>%
  transform(valuation = factor(valuation, levels = c(metrics, cum_metrics)))

# Long summary
years_from_phases_long_sum <- years_from_phases_long %>%
  group_by(src, from, year, valuation) %>%
  summarize(mu  = mean(value),
            med = median(value),
            std = sd(value))

# Final long
final_year_from_phases_long <- years_from_phases_long %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, subject, from, valuation) %>%
  arrange(year) %>%
  summarize(value = tail(value, n=1))

# Final long summary
final_year_from_phases_long_sum <- years_from_phases_long_sum %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, from, valuation) %>%
  arrange(year) %>%
  summarize(mu  = tail(mu, n=1),
            med = tail(med, n=1),
            std = tail(std, n=1))
```


As with the phase-based method, let us also do some descriptive statistics.
We begin with the value of taking the project from beginning (i.e. pre-clinical) to end (i.e. the final market year), using our cumulative valuation metrics.
Again, note the usage of different scales across the different metrics due the vastly different values.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
final_year_from_phases_long %>%
  filter(from == 'PC' & valuation %in% cum_metrics) %>%
  ggplot(aes(valuation, value, fill=src)) +
  geom_boxplot() +
  xlab(element_blank()) +
  ylab(element_blank()) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  facet_wrap(. ~ valuation, scale='free', nrow=1)
```

When we've transformed the data to year-based we've moved from an ordinal to a ratio scale.
As such it makes sense to explore how these valuations emerge as a consequence of the yearly cashflows of a project.
From top-left to bottom-right: out-of-pocket cashflows, risk-adjusted/expected value (EV), capitalized/present value (PV), risk-adjusted/expected capitalized/present value (EPV).
The middle line in each ribbon tracks the mean value, while the edges capture all values within 2 standard deviations of the mean (meaning 95% of the data).

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
years_from_phases_long_sum %>%
  filter(from == 'PC') %>%
  filter(valuation %in% metrics) %>%
  ggplot(aes(year, med)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_ribbon(aes(x=year, ymin=(mu-std*2), ymax=(mu+std*2), fill=src), alpha=.4) +
  geom_line(aes(color=src)) +
  facet_wrap(. ~ valuation, scale='free_y', ncol=2)
```

If we cumulate these values over time, we get the cumulative versions of these metrics.
These can be thought of as the value of running the project for x years from the start.
From top-left to bottom right: cumulative out-of-pocket cashflows, cumulative risk-adjusted/expected value (EV), capitalized/net present value (NPV), and risk-adjusted/expected capitalized/net present value (ENPV).

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
years_from_phases_long_sum %>%
  filter(from == 'PC') %>%
  filter(valuation %in% cum_metrics) %>%
  ggplot(aes(year, med)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_ribbon(aes(x=year, ymin=(mu-std*2), ymax=(mu+std*2), fill=src), alpha=.4) +
  geom_line(aes(color=src)) +
  facet_wrap(. ~ valuation, scale='free_y', ncol=2)
```

Another way to look at the data is to consider how starting at different phases alter the value of the project.
We begin by plotting the mean value of bringing the project to completion from whatever starting phase we're currently considering.
Using the four metrics previously applied.
The vertical lines delimit +/- 1 standard deviations from the sample mean (i.e. ~68% of the data).

<div class="alert alert-danger">
TODO: Why is cumulative decreasing in P1? That should be impossible? This must be an error?
</div>

```{r}
pos <- position_dodge(0.4)
final_year_from_phases_long_sum %>%
  ggplot(aes(from, mu, color=valuation, group=valuation)) +
  geom_linerange(aes(ymin=mu-std, ymax=mu+std), position=pos, size=1) +
  geom_line(position=pos, size=1, linetype='dotted') +
  geom_point(position=pos, size=2)  +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  facet_wrap(. ~ src, ncol=2) +
  xlab(element_blank()) + ylab(element_blank())
```

The table below summarizes the valuation metrics from the start of pre-clinical.

```{r, echo=FALSE}
sub <- final_year_from_phases_long_sum %>%
  ungroup %>%
  filter(from=='PC') %>%
  arrange(valuation)
knitr::kable(sub)
```

## Comparing methods
Let us compare the two methods by looking at the cumulative valuations from different starting points.
In the first two plots we observe that the simple valuation metrics cumulative cashflows and ENV, does not significantly differ between the two methods in any of the datasets.
In fact, the cumulative metric should yield exactly the same results in both methods as neither discounting nor probability is taken into consideration.
When computing ENV however, we take probability into account, and as a consequence, the outcome could in theory be different, but in practice, i.e. in the plot below, we observe that the difference is insignificant.

```{r, echo=FALSE, warning=FALSE}
pwise <- phasely_final_from_phases_long
ywise <- final_year_from_phases_long
methods <- c('phase', 'year')
pwise$method <- factor('phase')
ywise$method <- factor('year')
both <- bind_rows(pwise, ywise)
# TODO: Yields warning cuz coercing src to character vector (or similar)
# TODO: Use the phrases valuation metric and valuation method to differentiate
# whether we're talking about NPV/ENPV or phase-based/year-based.
```

```{r, echo=FALSE}
p <- both %>%
  filter(valuation %in% c('cum', 'env')) %>%
  ungroup %>%
  mutate(src = recode(src,
                      'DRIVE-AB (2018)' = 'drive-ab',
                      'Sertkaya et. al (2014)' = 'sertkaya')) %>%
  ggplot(aes(src, value, color=method, fill=method)) +
  geom_boxplot(alpha=0.3) +
  facet_wrap(valuation ~ from, scales='free', ncol=6) +
  ylab(element_blank()) +
  xlab(element_blank()) +
  theme(axis.text.x = element_text(angle=90))
print(p)
```

Taking discounting into consideration however, makes the two methods yield slightly different results.
The following two plots illustrate that difference for the metrics NPV and ENPV.
Again, in all datasets, from multiple starting phases.


```{r, echo=FALSE}
for (curr in c('npv', 'enpv')) {
  p <- both %>%
    filter(valuation == curr) %>%
    ungroup %>%
    mutate(src = recode(src,
                        'DRIVE-AB (2018)' = 'drive-ab',
                        'Sertkaya et. al (2014)' = 'sertkaya')) %>%
    ggplot(aes(src, value, color=method, fill=method)) +
    geom_boxplot(alpha=0.3) +
    facet_wrap(valuation ~ from, scales='free', ncol=6) +
    ylab(element_blank()) +
    xlab(element_blank()) +
    theme(axis.text.x = element_text(angle=90))
  print(p)
}
```


# Interventions
We will now introduce five interventions (treatments) to our base data sets and later analyze their effects on the valuation metrics.
We model these interventions as qualitatively (categorically) different, as they operate on different R&D phases, but theoretically they could be considered the same intervention that is quantitatively (numerically) different in terms of their actuation time.
The intervention can be described as a non-dilutive and unconditional prize.
This intervention can be considered a generalization of what is commonly referred to as either a market entry reward or a phase entry reward, where we've generalized the time of actuation.
We implement the intervention as a function `intervene` as follows:

```{r}
log10_sample <- function (min, max, magnitude_min, magnitude_max) {
  runif(N, min, max) * (10 ^ runif(N, magnitude_min, magnitude_max))
}

intervene <- function (phases, target_phase, intervention_name) {
  data.frame(phases) %>%
    mutate(intervention = intervention_name,
           prizes = ifelse(phase == target_phase,
                           log10_sample(1, 9, 1, 3),
                           prizes))
}
```

We can then treat copies of the original phase-based data set and merge it into one larger data set, like this:

```{r}
intervened <- rbind(phases,
  intervene(phases, 'P1', 'P1ER'),
  intervene(phases, 'P2', 'P2ER'),
  intervene(phases, 'P3', 'P3ER'),
  intervene(phases, 'P4', 'P4ER'),
  intervene(phases, 'MP', 'PDMER'))
```

Note that we are logarithmically sampling prize sizes.
This is because we assume that the absolute difference in effect will be much smaller when prize sizes are very small, as compared to when prize sizes are very large, and hence need more samples at the "bottom" to properly saturate the space.
In the analysis we at all times "control for" prizes which means that the chosen distribution does not affect any of the conclusions beyond sample saturation.
The sampled prizes are summarized in the histogram below.

```{r, echo=FALSE, message=FALSE}
print(intervened %>%
      filter(prizes > 0) %>%
      ggplot(aes(prizes, fill=phase)) +
      geom_histogram() +
      theme(axis.title.y=element_blank()) +
      facet_wrap(interaction(intervention, phase)~., ncol=3) +
      guides(fill = FALSE))
```

We then convert the data to yearly, and restructure it so that every row with an intervenetion is matched with the corresponding row without an intervention.

```{r}
intervened_years <- to_years(intervened)
control <- filter(intervened_years, intervention == 'NONE')
treated <- filter(intervened_years, intervention != 'NONE')
comparable_years <-
  semi_join(treated, control, by=c('src', 'subject', 'phase', 't'))
```


# Analysis

The question we want to explore is whether it is cheaper for the benefactor to directly or indirectly fund antibiotics R&D.
Directly here refers to the idea of simply paying for development "at cost".
Indirectly here refers to the idea of issuing prizes that encourage private developers to undertake a given activity with the hope of winning said prize.

We will compare these two different approaches by comparing their expected costs when facing a hypothetical project entering pre-clinical.
We assume that the valuation metric that private actors employ is ENPV and that the valuation method is yearly.
We will refer to this value as *private ENPV.*

```{r, echo=FALSE}
public_discount_rate_min = 0.035
public_discount_rate_max = 0.045
```

In order to reason about the cost for the benefactor of issuing a prize we will compute the necessarily negative ENPV of issuing said prize.
ENPV is an appropriate measure, since the prize is probabilistically issued in the future.
As previously described, we assume that the benefactor is the public sector and employ a uniformly distributed discount rate between `r publ_dr_min * 100`% and `r publ_dr_max * 100`%.
We refer to the expected cost of a prize (i.e. an intervention) as *indirect ENPV*.
This can be thought of as the expected cost for the benefactor.

If indirect ENPV is the cost of the intervention for the public, then we must also compute the expected cost of simply paying for the project in question at cost.
To take both cost of capital and project risk into consideration, we will again compute the negative ENPV.
The probabilistic cashflows used in the ENPV calculation in this case is the project's cost without any expectation of revenues.
We assume that the public payer is the same as the benefactor and hence employ discount rates from the same distribution
(`r publ_dr_min * 100`% - `r publ_dr_max * 100`%).
We refer to the expected cost of paying for the project at cost as *direct ENPV*.

We compute private, direct, and indirect ENPV like this:

```{r}
comparable_years <- comparable_years %>%
  arrange(t) %>%
  group_by(src, subject, intervention) %>%
  mutate(
         prob_to = cumprod(prob) / prob, # TODO: Is this correct?
         intervention_size = sum(prizes),
         cashflow_bef = sales - cost,
         cashflow_aft = sales - cost + prizes,
         enpv_prv_bef = cumsum((cashflow_bef / ((1 + discount_rate_priv) ^ t)) * prob_to),
         enpv_prv_aft = cumsum((cashflow_aft / ((1 + discount_rate_priv) ^ t)) * prob_to),
         enpv_dir = cumsum((-cost / ((1 + discount_rate_publ) ^ t)) * prob_to),
         enpv_ind = cumsum((-prizes / ((1 + discount_rate_publ) ^ t)) * prob_to),
         ) %>%
  select(-prob_to)
```

We extract the different ENPV values from the perspective of pre-clinical, like this:

```{r}
finals <- comparable_years %>%
  arrange(t) %>%
  group_by(src, subject, intervention) %>%
  summarise(
         prizes = sum(prizes),
         enpv_prv_bef = tail(enpv_prv_bef, n=1),
         enpv_prv_aft = tail(enpv_prv_aft, n=1),
         enpv_dir = tail(enpv_dir, n=1),
         enpv_ind = tail(enpv_ind, n=1))
```

## Prizes and private ENPV
Before we proceed further, let us first ensure that the issued prizes (i.e. the interventions) actually do have an effect on private ENPV.
As such there should be a correlation between prize size and private ENPV in all datasets.

```{r, echo=FALSE}
print(finals %>%
  ggplot(aes(prizes, enpv_prv_aft, color=src)) +
  geom_point(alpha=1, shape=1) +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ intervention, ncol=3, scale='free') +
  xlab('Prize') +
  ylab('Private ENPV') +
  theme(legend.position = 'top')
)
```

While correlation, the data seems more suitable to plot on a logarithmic scale.
As some observations have negative ENPV however we instead choose to plot the improvement in private ENPV with and without the intervention.

```{r, echo=FALSE}
print(finals %>%
  ggplot(aes(prizes, enpv_prv_aft-enpv_prv_bef, color = src)) +
  geom_point(alpha=0.8, shape=1) +
  facet_wrap(. ~ intervention, ncol=3, scale='free') +
  xlab('Prize') +
  ylab('Private ENPV improvement') +
  scale_x_continuous(trans='log10') +
  scale_y_continuous(trans='log10') +
  theme(legend.position = 'top')
)
```

Intervention prize and private ENPV, expectedly, seem correlated.
As other studies have shown, the later a prize is awarded, the higher the size must be to achieve a comparative increase in ENPV.
This phenomena is clearly visible in our model when plotting the correlation between prize size and private ENPV for all interventions on the same scale.

```{r, echo=FALSE, warning=FALSE}
print(finals %>%
  ggplot(aes(prizes, enpv_prv_aft - enpv_prv_bef, color=intervention)) +
  geom_smooth(method='lm', se=FALSE) +
  geom_point(shape=1) +
  xlab('Prize') +
  ylab('Private ENPV improvement') +
  scale_y_continuous(trans='log10',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  scale_x_continuous(trans='log10',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  facet_wrap(. ~ src, ncol=2, 'free') +
  theme(legend.position = 'top')
)
# TODO: This plot yields warnings
```

## Prizes and indirect ENPV
In order to gauge the actual cost of how much a benefactor will have to pay when issuing a particular prize, we choose to compute ENPV.
This metric takes both the opportunity cost (cost of capital) and risk of a project into consideration.
Taking opportunity cost into consideration is important as the benefactor looses the opportunity to activate their money elsewhere when making a payment to a beneficiary.
Taking risk into consideration is important as the benefactor will not necessarily pay the promised prize size, due to projects being highly risky.
We will now explore the correlation between prize size and the negative ENPV for the benefactor.
Note that ENPV for the benefactor necessarily will be negative as the prize payout is the only cashflow considered.

```{r, echo=FALSE}
finals %>%
  ggplot(aes(prizes, -enpv_ind, color=intervention)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ src, ncol=3) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:3)),
                     minor_breaks = c(1:9 %o% 10^(0:3))) +
  xlab('Prize (log)') + ylab('-Indirect ENPV')
```

<div class="alert alert-danger">
TODO: Change above to improvement in ENPV.
</div>

## Indirect ENPV and Private ENPV
How much it costs the benefactor compared to how much it benefits the beneficiary.

```{r, echo=FALSE}
ggplot(finals, aes(y=enpv_prv_aft, x=-enpv_ind, color=intervention)) +
  geom_point() +
  facet_wrap(. ~ src, ncol=3) +
  geom_smooth(method='lm', se=FALSE) +
  coord_cartesian(xlim = c(0, 500), ylim = c(-25, 200)) +
  ylab('Private ENPV') + xlab('-Indirect ENPV')
```

## Prizes and go-decisions
Assuming that a go-decision is reached in pre-clinical whenever private ENPV is greater than or equal to 0, we can use logistic regression to predict whether a particular prize size will yield a go or no-go decision.
We first compute go-decisions:
```{r}
finals$go <- factor(finals$enpv_prv_aft >= 0, levels=c(TRUE, FALSE))
```

then construct the model and use it to predict values as follows:
```{r}
prize_model <- glm(data = finals,
             go ~ prizes * interaction(src, intervention),
             family = binomial(link='logit'))
finals$pred_go_prize <- predict(prize_model, type='response')
```

Finally, we can plot the predicted decision against different prize sizes.
```{r, echo=FALSE}
finals %>%
  ggplot(aes(prizes, (1-pred_go_prize)*100, color=intervention)) +
  geom_line() +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
      xlab('Prize (log)') + ylab('Probability of go-decision') +
      facet_wrap(. ~ src, ncol=3)
```


## Indirect ENPV and go-decisions
In the same fashion, we can construct a model that predicts go-decisions from the benefactor's ENPV.

```{r}
benf_model <- glm(data=finals,
             go ~ enpv_ind * interaction(src, intervention),
             family=binomial(link='logit'))
finals$pred_go_benf <- predict(benf_model, type='response')
```

This can help us understand how much the benefactor will have to spend in order to achieve a particular go-decision probability.
```{r, echo=FALSE}
finals %>%
  ggplot(aes(-enpv_ind, (1-pred_go_benf)*100, color=intervention)) +
  geom_line() +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
      xlab('-Indirect ENPV (log)') + ylab('Probability of go-decision') +
      facet_wrap(. ~ src, ncol=3)
```

# Conclusion
