---
title: Time for public pharma?
subtitle: Supplementary material
author: Christopher Okhravi
date: 2019
output:
  bookdown::gitbook:
    css: style.css
    split_by: section
    config:
      toc:
        collapse: subsection
---


# Preface {-}
This document contains the full analysis that underpins the paper paper "Time for public pharma?".
This is a monte carlo simulation that explores direct versus indirect funding of antibiotics research and development (R&D).
Direct funding is here used to mean that a benefactor pays for antibiotics R&D at cost.
Indirect funding is here used to mean that a benefactor issues non-dilutive prizes to whoever completes a particular phase, with the intent of incentivizing private developers to undertake said and prior phases.
This analysis estimates which of the two, ceteris paribus, is cheaper for the benefactor.

## How to compile this document {-}
If you're reading this in an html format then the document is already compiled.
You can compile the Rmd file(s) like this:

```bash
Rscript -e "bookdown::render_book('index.html')"
```

Note: While some of the code that is used to perform the analysis is included in the rendered (i.e. output) document, some is not.
If you need further details, please consult the source code used to generate the output document.


# Input data
We begin by setting `N` which will dictate the number of project samples that we'll draw from each dataset.
Meaning that if one considers all the data sets at once then the number of random projects drawn in total is `N` times the number of data sets considered.
In other words, `N` is the number of random samples that we'll draw from a stochastic representation of a hypothetical antibiotic R&D project.
The distribution of the stochastic project depends on the data set we're using, and in a simple attempt to avoid drawing data set specific conclusions, we'll look at multiple data sets.

```{r, set-n}
N = 1500
```

```{r, basics, include=FALSE}
# Dependencies
library(knitr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(triangle)
library(scales)
library(cowplot)
library(broom)
library(purrr)
library(forcats)

# Cache code chunks
knitr::opts_chunk$set(cache=TRUE, collapse=TRUE)

# Set seed for reproducibility
set.seed(3)

# NOTE: I can't figure out how to sample from multiple triangular distributions
# at the same time using rtriangle, so here's my sloppy quickfix implementation.
# Usage is the same as rtriangle, but you can pass vectors in mins, maxs and mids.
rtriangle2 <- function(n, mins, maxs, mids) {
  xs <- c()
  while (length(xs) < n)
    for (x in 1:length(mins))
      xs <- c(xs, rtriangle(1, mins[x], maxs[x], mids[x]))
  xs
}

log10_sample <- function (n, min, max, magnitude_min, magnitude_max)
  runif(n, min, max) * (10 ^ runif(n, magnitude_min, magnitude_max))

# The phases we'll be working with
DEVELOPMENT_PHASES <- c('PC','P1','P2','P3','P4')
MARKET_PHASES <- paste('M', 1:20, sep='')
PHASES <- c(DEVELOPMENT_PHASES, MARKET_PHASES)
```

The data in this analysis stem from two publications, Sertkaya et. al (2014) and DRIVE-AB (2018).
The latter considers only a single hypothetical antibiotic, while the former considers six hypothetical antibiotics targeting different indications.
Our analysis is thus applied to seven data sets stemming from two publications.
Disclaimer: The data in DRIVE-AB (2018) is heavily based on Sertkaya et. al (2014) and is thus not an independently collected dataset.
Nevertheless the spread of data across the seven datasets means that a wide range of input is explored.



## Sertkaya distribution

The following is an approximation of the data used by Sertkaya et. al (2014).
We say approximation as a few of their assumptions are not reconcilable with our modeling choices.
These compromises are described when encountered.

Sertkaya et. al (2014) differentiates between antibiotics on the basis of target indication.
In our analysis we sometimes use the term dataset source (or src for short) to refer to indication since we treat each indication as a separate dataset.

The table below summarize the distribution of development data as per 3.2.2 -- 3.2.5 in Sertkaya et. al (2014).
Time is expressed in months, cost and revenues in million USD, and probabilities and market shares in percentage.

We use the term phase 4 (abbreviated P4) while Sertkaya et. al (2014) use the term NDA/BLA.

```{r, dist-sertkaya, echo=FALSE}
INDICATIONS <- c('ABOM', 'ABSSSI', 'CABP', 'CIAI', 'CUTI', 'HABP/VABP')

sertkaya2014_phase_dist <-
  rbind(data.frame(indication = INDICATIONS,
                   phase = 'PC',
                   time_min = 52,
                   time_mid = 66,
                   time_max = 72,
                   cost_min = 19,
                   cost_mid = 21.1,
                   cost_max = 23.2,
                   prob_min = 17.5,
                   prob_mid = 35.2,
                   prob_max = 69
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P1',
                   time_min = 9,
                   time_mid = 10.5,
                   time_max = 21.6,
                   cost_min = 7.3,
                   cost_mid = 9.7,
                   cost_max = 12,
                   prob_min = 25,
                   prob_mid = 33,
                   prob_max = 83.7
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P2',
                   time_min = c(12, 9, 12, 10, 10, 16),
                   time_mid = c(15, 10, 15, 11, 11, 18),
                   time_max = 30,
                   cost_min = c(7.4, 7.12, 7.28, 7.68, 7.28, 12.48),
                   cost_mid = c(9.2, 8.9, 9.1, 9.6, 9.1, 15.6),
                   cost_max = c(11, 10.68, 10.92, 11.52, 10.92, 18.72),
                   prob_min = 34,
                   prob_mid = 50,
                   prob_max = 74
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P3',
                   time_min = c(20, 10, 10, 17, 17, 35),
                   time_mid = c(24, 12.5, 12.5, 21.5, 21.5, 39),
                   time_max = 47,
                   cost_min = c(33.36, 26.88, 31.04, 40.48, 35.04, 81.12),
                   cost_mid = c(41.7, 33.6, 38.8, 50.6, 43.8, 101.4),
                   cost_max = c(50.04, 40.32, 46.56, 60.72, 52.56, 121.68),
                   prob_min = 31.4,
                   prob_mid = 67,
                   prob_max = 78.6
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P4',
                   time_min = 6,
                   time_mid = 9,
                   time_max = 12.5,
                   cost_min = 1.9588, # NDA/BLA submission cost
                   cost_mid = 1.9588, # NDA/BLA submission cost
                   cost_max = 1.9588, # NDA/BLA submission cost,
                   prob_min = 83,
                   prob_mid = 85,
                   prob_max = 99
                   )
        ) %>% arrange(phase, indication)
kable(sertkaya2014_phase_dist, caption='sertkaya2014_phase_dist')
```

Sertkaya et. al (2014) seems to not directly sample the market share distribution, but rather to sample a product launch success probability distribution, and then apply that sample to reach an estimate for every year's market share.
This ensures that the market share does not vary widely between the lower and upper bound on a yearly basis.
Instead, the point between the lower and upper bound remains constant, while the lower and upper bounds themselves vary.
Also, this ensures that no year has a lower market share than the previous year before peak year sales.

```{r, sertkaya2014-market-dist, echo=FALSE}
sertkaya2014_market_size_dist <-
  data.frame(indication = INDICATIONS,
             min = c(2720, 3070, 2290, 2530, 5760, 1780),
             max = 9230)

sertkaya2014_market_share_dist <-
  data.frame(year = 1:20,
             min = c(0.05, 0.87, 1.57, 2.57, 3.92, 5.79, 7.52, 8.52, 10.10, rep(12.27, 11)),
             max = c(0.11, 1.91, 3.47, 5.68, 8.64, 12.77, 16.59, 18.80, 22.30, rep(27.08, 11)))

kable(sertkaya2014_market_size_dist, caption='sertkaya2014_market_size_dist')
kable(sertkaya2014_market_share_dist, caption='sertkaya2014_market_share_dist')
```

In line with 3.2.11 of Sertkaya et. al (2014) we assume a total product life (i.e. market life) of 20 years.
In line with 3.2.9-3.2.10 of Sertkaya et. al (2014) we assume that patent expiry leads to a reduction in revenues due to generic entry.
The time (year) of generic entry (i.e. patent expiry) and the corresponding reduction of revenues are distributed as per the table below.
In summary this means that the captured market share will increase from year 1 to year 10, and then remain constant until generic entry (i.e. patent expiry), upon which it will be reduced to a lower constant until year 20.

The table below also reports the discount rates which stem from 3.2.1 of Sertkaya et. al (2014) who use the term real opportunity cost of capital.

```{r, echo=FALSE}
sertkaya2014_market_reduction_min <- 25 # percent
sertkaya2014_market_reduction_mid <- 50 # percent
sertkaya2014_market_reduction_max <- 75 # percent
sertkaya2014_generic_entry_min <- 10 # years
sertkaya2014_generic_entry_mid <- 12 # years
sertkaya2014_generic_entry_max <- 14 # years
sertkaya2014_launch_success_min <- 40 # percent
sertkaya2014_launch_success_mid <- 60 # percent
sertkaya2014_launch_success_max <- 80 # percent
sertkaya2014_private_discount_rate_min <- 9 # percent
sertkaya2014_private_discount_rate_mid <- 11 # percent
sertkaya2014_private_discount_rate_max <- 24 # percent

tmp <- data.frame(parameter = c('Product launch success probability (%)',
                                'Market reduction due to generic entry (%)',
                                'Generic entry (year)',
                                'Private developer discount rate (%)'),
                  min = c(sertkaya2014_launch_success_min,
                          sertkaya2014_market_reduction_min,
                          sertkaya2014_generic_entry_min,
                          sertkaya2014_private_discount_rate_min),
                  mid = c(sertkaya2014_launch_success_mid,
                          sertkaya2014_market_reduction_mid,
                          sertkaya2014_generic_entry_mid,
                          sertkaya2014_private_discount_rate_mid),
                  max = c(sertkaya2014_launch_success_max,
                          sertkaya2014_market_reduction_max,
                          sertkaya2014_generic_entry_max,
                          sertkaya2014_private_discount_rate_max))
kable(tmp)
```

Sertkaya et. al (2014) also consider costs for a few additional (1) supply chain activities, (2) non-clinical work, and (3) post-approval studies as listed below.
The costs are spread across various phases as indicated by the percentages under the corresponding phase columns.
These figures and percentages stem from Table 9, section 3.2.7, and section 3.2.8 of Sertkaya et. al (2014) respectively.

Sertkaya et. al (2014) report that the cost of post-approval studies may last up to three years following market entry.
As such we assume that the cost is evenly distributed over three years.

```{r, sertkaya-additional-dist, echo=FALSE}
activity <-
  c('Sample preparation for animal/human studies',
    'Process research/development/design',
    'Plant design',
    'Plant build',
    'Non-clinical work',
    'Post-approval studies')
sertkaya2014_additional_dist <-
  data.frame(activity,
             min = c(2.4, 18.7, 10.7, 69.6, 3.4, 8),
             mid = c(2.7, 26.8, 13.4, 83,   3.7, 10),
             max = c(2.9, 34.8, 16.1, 96.3, 4,   12),
             PC = 0,
             P1 = c(1/3, 0.5, 0,    0, 0,   0),
             P2 = c(1/3, 0.5, 0,    0, 1/3, 0),
             P3 = c(1/3, 0,   0.75, 0, 1/3, 0),
             P4 = c(0,   0,   0.25, 1, 1/3, 0),
             M1 = c(0,   0,   0,    0, 0,   1/3),
             M2 = c(0,   0,   0,    0, 0,   1/3),
             M3 = c(0,   0,   0,    0, 0,   1/3)
  )

sertkaya2014_additional_dist %>%
  mutate(P1 = round(P1, 2) * 100,
         P2 = round(P2, 2) * 100,
         P3 = round(P3, 2) * 100,
         P4 = round(P4, 2) * 100,
         M1 = round(M1, 2) * 100,
         M2 = round(M2, 2) * 100,
         M3 = round(M3, 2) * 100) %>%
  kable(caption='sertkaya2014_additional_dist (million usd and rounded percentages)')
```



## DRIVE-AB distribution
The following is an approximation of the data used in DRIVE-AB final report (2018).
Some deviations (reported below) have been made as some of the DRIVE-AB assumptions are not compatible with our assumptions.

In the table below, time is reported in months, cost in million USD, and probability in percentages.

```{r, driveab-dist, echo=FALSE}
driveab2018_phase_dist <-
  data.frame(phase = DEVELOPMENT_PHASES,
             time_min = c(52, 9, 9, 10, 6),
             time_mid = c(66, 10.5, 13.33, 21.8, 9),
             time_max = c(72, 21.6, 30, 47, 12.5),
             # TODO: Cost of P2 is incorrectly reported in DRIVE-AB Final Report!
             # We're using the numbers from Okhravi et al. (2018)
             cost_min = c(14.25, 13.1,  12.95,  27.99, 55.5),
             cost_mid = c(21.1,  24,    24.55,  62.6,  88.35),
             cost_max = c(29,    37.96, 46.36,  168.4, 127.91),
             prob_min = c(17.5, 25, 34, 31.4, 83),
             prob_mid = c(35.2, 33, 50, 67, 85),
             prob_max = c(69, 83.7, 74, 78.6, 99))

kable(driveab2018_phase_dist, caption = 'driveab2018_phase_dist')
```

```{r, driveab-additional-params, echo=FALSE}
driveab2018_tot_sales_min = 0
driveab2018_tot_sales_mid = 2559.5
driveab2018_tot_sales_max = 4336
driveab2018_market_years = 10
driveab2018_priv_discount_rate_min = 0.05 * 100
driveab2018_priv_discount_rate_max = 0.3 * 100
```

In line with the assumptions of DRIVE-AB (2018) we assume that sales revenue linearly increase over a period of `r driveab2018_market_years` years.
Patent expiry is assumed to occur at this point and we assume that all revenues drop to 0 beyond this point.
Specifically, we assume that the revenues of year 0 is 0 and then linearly increase until the area under the curve is equal to the total global net sales.
The first year will thus be non-zero if the market revenues of the project is non-zero.

```{r, print-driveab-additional-params, echo=FALSE}
data.frame(parameter = 'Total sales (million usd)',
           min = driveab2018_tot_sales_min,
           mid = driveab2018_tot_sales_mid,
           max = driveab2018_tot_sales_max) %>% kable

data.frame(parameter = 'Private developer discount rate (%)',
           min = driveab2018_priv_discount_rate_min,
           max = driveab2018_priv_discount_rate_max) %>% kable
```

## Benefactor parameters
To perform our analysis we require two additional parameters.
First, a "public" or "social" discount rate that is representative of the cost of capital of the benefactor, i.e. the body that pays for the intervention with no expectation of monetary return.
Second, an inefficiency parameter that will be used to measure the fact that many argue that agents with lower expectation of monetary returns (e.g. states) are often assumed to be less efficient.
We uniformly distribute these parameters as per the table below.

```{r, additional-dist, echo=FALSE}
public_discount_rate_min <- 0.035
public_discount_rate_max <- 0.045

public_inefficiency_min <- 0
public_inefficiency_max <- 1

data.frame(parameter = c('Public discount rate (%)',
                         'Public inefficiency (%)'),
           min = c(public_discount_rate_min * 100,
                   public_inefficiency_min * 100),
           max = c(public_discount_rate_max * 100,
                   public_inefficiency_max * 100)) %>% kable
```

## Interventions distribution
We model interventions as qualitatively (categorically) different, as they operate on different R&D phases, but theoretically they could be considered the same intervention that is quantitatively (numerically) different in terms of its actuation time.
The intervention can be described as a unconditional, non-dilutive prize.
This intervention can be considered a generalization of what is commonly referred to as either a market entry reward or a phase entry reward, where we've generalized the time of actuation.
Prize size is quantitatively varied stochastically.

We are logarithmically sampling prize sizes.
This is because we assume that the absolute difference in effect will be much smaller when prize sizes are very small, as compared to when prize sizes are very large, and hence need more samples at the "bottom" to properly saturate the space.
In the analysis we at all times "control for" prizes which means that the chosen distribution does not affect any of the conclusions beyond sample saturation.
The table below show the intervention distribution that we are sampling from.

```{r, intervention-dist, echo=FALSE}
intervention_dist_prize_min <- 1
intervention_dist_prize_max <- 9.999999999
intervention_dist_magn_min <- 5
intervention_dist_magn_max <- 9

# Print as table
# TODO: More descriptive table headers
data.frame(parameter = c('Multiplier', 'Exponent'),
           min = c(intervention_dist_prize_min,
                   intervention_dist_magn_min),
           max = c(intervention_dist_prize_max,
                   intervention_dist_magn_max)) %>%
  kable(caption='Interventions')
```

While the intervention prize size can be thought of as the treatment dosage, the intervention phase can be thought of as the treatment type.
The table below report which phases we apply interventions to.

```{r, prize-phase-distribution, echo=FALSE}
intervention_phase_dist <-
  data.frame(prize_phase = factor(c('P1', 'P2', 'P3', 'P4', 'M1'),
                                  levels=PHASES, ordered=TRUE))
kable(intervention_phase_dist, caption='Intervention target phases')
```

# Sampling
In this chapter, we sample the distributions outlined in the last chapter.

## Sertkaya sample
We first sample development data from the Sertkaya distributions.

```{r, sample-sertkaya-dev}
sertkaya2014 <- sertkaya2014_phase_dist %>%
  # Replicate every row N number of times
  uncount(N) %>%
  # Add subject ids but group by phase so identifiers are reused across phases
  group_by(phase) %>% mutate(subject = 1:n()) %>% ungroup() %>%
  # Sample distributions
  mutate(time = rtriangle2(n(), time_min, time_max, time_mid),
         cost = rtriangle2(n(), cost_min, cost_max, cost_mid),
         prob = rtriangle2(n(), prob_min, prob_max, prob_mid)) %>%
  # Normalize sampled data
  mutate(cost = cost * 10^6,    # Convert million usd to usd
         time = time / 12,      # Convert months to years
         prob = prob / 100) %>% # Convert percentage to fraction
  # Add hard-coded development data
  mutate(sales = 0) %>%
  # Rename indication column and remove temp cols
  rename(src = indication) %>%
  select(subject, src, phase, time, cost, prob, sales)
```

We then sample market data from the Sertkaya distributions and merge it with our development phases sample.
<!-- First, we pair up every subject with the market size distribution of its indication.... -->

```{r, sample-sertkaya-market}
sertkaya2014 <- sertkaya2014 %>%
  # Only keep source and subject column and distinct rows
  select(src, subject) %>% unique %>%
  # Left join with size distribution
  merge(., sertkaya2014_market_size_dist, by.x='src', by.y='indication', all.x=TRUE) %>%
  # Sample market size distribution, then remove distribution columns
  mutate(market_size = runif(n(), min, max)) %>%
  select(-min, -max) %>%
  # Normalize sampled market size
  mutate(market_size = market_size * 10^6) %>% # Convert million usd to usd
  # Sample non-indication specific market parameters
  mutate(launch_success = rtriangle2(n(),
                                     sertkaya2014_launch_success_min,
                                     sertkaya2014_launch_success_max,
                                     sertkaya2014_launch_success_mid),
         generic_entry = rtriangle2(n(),
                                    sertkaya2014_generic_entry_min,
                                    sertkaya2014_generic_entry_max,
                                    sertkaya2014_generic_entry_mid),
         generic_reduction = rtriangle2(n(),
                                        sertkaya2014_market_reduction_min,
                                        sertkaya2014_market_reduction_max,
                                        sertkaya2014_market_reduction_mid)) %>%
  # Normalize non-indication specific market parameters
  mutate(launch_success = launch_success / 100, # Percentage to fraction
         generic_reduction = generic_reduction / 100, # Percentage to fraction
         generic_entry = round(generic_entry)) %>% # Round to full year
  # Cartesian product merge with market share dist
  merge(., sertkaya2014_market_share_dist) %>%
  # Normalize market share dist
  mutate(min = min / 100, max = max / 100) %>% # Percentages to fractions
  # Compute captured market share per year according to Sertkaya et. al (2014) Table 12
  mutate(share = (min * (1 - launch_success)) + (max * launch_success)) %>%
  # Compute reduction in market due to generic entry (Sertkaya et. al, 2014, 3.2.10)
  mutate(share = ifelse(year < generic_entry, share, (share - (share * generic_reduction)))) %>%
  # Compute yearly sales
  mutate(sales = share * market_size) %>%
  # Convert year to character factor matching MARKET_PHASES
  mutate(phase = sprintf('M%s', year)) %>%
  # Add hard-coded market data
  mutate(time = 1, prob = 1, cost = 0) %>%
  # Remove temporary columns
  select(subject, src, phase, time, cost, prob, sales) %>%
  # Add to initial dev sample
  rbind(sertkaya2014, .)
```

Finally, we sample the additional supply chain activity costs data,
distribute the samples across the development and market phases according to their corresponding fractions,
and sum up the additional costs per phase for every subject.
From this point onwards it is therefore impossible to distinguish additional supply chain costs (e.g. non-clinical work costs) from clinical phase costs.
These additional costs are then added into the full sample.

```{r, sample-sertkaya-additional}
sertkaya2014 <- sertkaya2014 %>%
  # Only keep source column and distinct rows
  select(subject) %>% unique %>%
  # Cartesian product with additional costs distribution
  merge(., sertkaya2014_additional_dist) %>%
  # Normalize additional costs
  mutate(min = min * 10^6,      # Convert million usd to usd
         mid = mid * 10^6,      # Convert million usd to usd
         max = max * 10^6) %>%  # Convert million usd to usd
  # Sample additional costs
  mutate(tot_add_cost = rtriangle2(n(), min, max, mid)) %>%
  # Remove temporary columns
  select(-min, -mid, -max, -activity) %>%
  # Wide to long: make phase names rows instead of columns
  gather(phase, fraction, -subject, -tot_add_cost) %>%
  # Compute actual additional costs in phase
  mutate(add_cost = tot_add_cost * fraction) %>%
  # Sum up costs per phase for every subject
  group_by(subject, phase) %>%
  summarize(add_cost = sum(add_cost)) %>%
  # Left join with full sample (NOTE: causes NAs where no additional costs)
  merge(sertkaya2014, ., by=c('subject', 'phase'), all.x = TRUE) %>%
  # Add additional cost to regular cost and remove temp column
  mutate(cost = ifelse(is.na(add_cost), cost, cost + add_cost)) %>%
  select(-add_cost)
# TODO: I could also do a merge earlier on subject and not phase, and then add
# if phase matches. That might look a bit messier but it should be more
# efficient and doesn't introduce NAs. Is this a better solution?
```


## DRIVE-AB sample

We first sample development data from the DRIVE-AB distribution.

```{r, sample-driveab-dev}
first_id <- max(sertkaya2014$subject) + 1
driveab2018 <- driveab2018_phase_dist %>%
  # Replicate every row N number of times
  uncount(N) %>%
  # Add subject ids but group by phase so identifiers are reused across phases
  group_by(phase) %>%
  mutate(subject = first_id:(first_id + n() - 1)) %>%
  ungroup() %>%
  # Sample phase data and add hard-coded data
  mutate(time = rtriangle2(n(), time_min, time_max, time_mid),
         cost = rtriangle2(n(), cost_min, cost_max, cost_mid),
         prob = rtriangle2(n(), prob_min, prob_max, prob_mid),
         sales = 0) %>%
  # Remove temp columns
  select(subject, phase, time, cost, prob, sales)
```

We then sample market data from the DRIVE-AB distribution and add it to the sampled development data.
Lastly, we sample discount rates and also add it into the full sample.

```{r, sample-driveab-market}
driveab2018 <- driveab2018 %>%
  # Select only subject column and unique rows
  select(subject) %>% unique %>%
  # Sample market size
  mutate(tot_sales = rtriangle2(n(),
                                driveab2018_tot_sales_min,
                                driveab2018_tot_sales_max,
                                driveab2018_tot_sales_mid)) %>%
  # Compute market slope
  mutate(slope = (tot_sales * 2 / (driveab2018_market_years + 1)) / driveab2018_market_years) %>%
  # Replicate as many times as we want market years
  uncount(driveab2018_market_years) %>%
  # Add market years to every subject
  group_by(subject) %>%
  mutate(market_year = 1:driveab2018_market_years) %>%
  ungroup %>%
  # Add hard-coded data
  mutate(time = 12, cost = 0, prob = 100) %>%
  # Compute (interpolate) yearly sales
  mutate(sales = market_year * slope) %>%
  # Convert market year to character factor
  mutate(phase = paste('M', market_year, sep='')) %>%
  # Drop temp columns
  select(subject, phase, time, cost, prob, sales) %>%
  # Add to initial dev sample
  rbind(driveab2018, .) %>%
  # Add data source name
  mutate(src = 'DRIVE-AB (2018)') %>%
  # Convert phase to ordered factor
  mutate(phase = factor(phase, levels=PHASES, ordered=TRUE)) %>%
  # Normalize sampled values
  mutate(time = time / 12,     # months -> years
         cost = cost * 10^6,   # million usd -> usd
         sales = sales * 10^6, # million usd -> usd
         prob = prob / 100,    # percentage -> fraction
         ) %>%
  # Arrange data nicely for printing purposes
  arrange(subject, phase)
```


## Additional parameters
First we sample one developer discount rate per subject (i.e. project) in the sertkaya dataset, using the discount rate distribution from the sertkaya source.

```{r, sample-sertkaya-discount-rate}
min <- sertkaya2014_private_discount_rate_min / 100 # Percentage to fraction
max <- sertkaya2014_private_discount_rate_max / 100 # Percentage to fraction
mid <- sertkaya2014_private_discount_rate_mid / 100 # Percentage to fraction
sertkaya2014_discount_rates <- sertkaya2014 %>%
  select(subject) %>%
  unique %>%
  mutate(priv_discount_rate = rtriangle2(n(), min, max, mid))
```

We then do the same for the DRIVE-AB dataset and source.

```{r, sample-driveab-discount-rate}
min <- driveab2018_priv_discount_rate_min / 100 # Percentage to fraction
max <- driveab2018_priv_discount_rate_max / 100 # Percentage to fraction
driveab2018_discount_rates <- driveab2018 %>%
  select(subject) %>%
  unique %>%
  mutate(priv_discount_rate = runif(n(), min, max))
```

We then amalgamate these two datasets so that we have a single dataset containing samples from both origins

```{r, combine-discount-rate-samples}
additional <- rbind(sertkaya2014_discount_rates, driveab2018_discount_rates)
```

We then sample a public discount rate and add it as a column to the amalgamated dataframe.

```{r, sample-public-discount-rate}
min <- public_discount_rate_min
max <- public_discount_rate_max
additional <- additional %>% mutate(publ_discount_rate = runif(n(), min, max))
```

And then do the same for public inefficiencies.

```{r, sample-public-inefficiency}
min <- public_inefficiency_min
max <- public_inefficiency_max
additional <- additional %>% mutate(publ_inefficiency = runif(n(), min, max))
```

Finally we sample and add intervention prize sizes.

```{r, sample-intervention-prize-size}
prize_min <- intervention_dist_prize_min
prize_max <- intervention_dist_prize_max
magn_min <- intervention_dist_magn_min
magn_max <- intervention_dist_magn_max
additional <- additional %>%
  mutate(prize_size = log10_sample(n(), prize_min, prize_max, magn_min, magn_max))
```

Below is an excerpt of the resulting dataframe.

```{r, agents-excerpt, echo=FALSE}
kable(head(additional, n=10))
```

And a histogram of the sampled intervention prize sizes.

```{r, prize-size-histogram, echo=FALSE}
additional %>%
  ggplot(aes(prize_size)) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = NULL,
                     labels = trans_format('log10', math_format(10^.x))) +
  geom_histogram()
```

Before moving further we also combine the Sertkaya and DRIVE-AB samples into a single dataframe.

```{r, combine}
phases <-
  # Merge data sources
  rbind(sertkaya2014, driveab2018) %>%
  # Convert phase to ordered factor
  mutate(phase = factor(phase, levels=PHASES, ordered=T))
```


# Valuation
As phase durations are long, the time value of money not only greatly reduces the attractiveness of revenues, but also dampen the pain of costs.
To take the time value of money into account we must make an assumption about how a real world evaluation of a project periodizes future costs, revenues and probabilities.
In other words, how a hypothetical evaluator chooses to transform a sequence of discrete phases into a sequence of discrete and uncertain cashflows.
We make the assumption that the evaluator converts every phase into a series of years and evenly (i.e. constantly) distribute all cashflows and probabilities of success over these years within a phase.

We use the function below to convert from phase-form to year-form.
Note however that we do not interpolate prizes but rather assume that they are introduced as lump sums.

```{r, year-based-conversion}
to_years <- function (phs) {

  # Add timestamps to every observation
  phs <- phs %>%
    group_by(subject) %>%
    arrange(phase) %>%
    mutate(time_to = cumsum(time) - time)

  # Compute cost steps
  cost_step <- (phs$cost / phs$time)
  cost_remainder <- phs$cost - cost_step * floor(phs$time)

  # Compute sales steps
  sales_step <- (phs$sales / phs$time)
  sales_remainder <- phs$sales - sales_step * floor(phs$time)

  # Compute prob steps
  prob_step <- phs$prob ^ (1 / phs$time)
  prob_remainder <- phs$prob / (prob_step ^ floor(phs$time))

  # Compute time steps
  time_step <- 1
  time_remainder <- phs$time - floor(phs$time)

  # Transform: to phase-local years
  phase_years <- tibble()
  for (x in 1:ceiling(max(phs$time) + 1)) { # Longest phase

    # Compute step-based properties
    has_decimals <- phs$time - floor(phs$time) != 0
    whole_step   <- x <= phs$time
    partial_step <- x <= phs$time + 1 & has_decimals

    cost   <- ifelse(whole_step, cost_step, cost_remainder)
    sales  <- ifelse(whole_step, sales_step, sales_remainder)
    prob   <- ifelse(whole_step, prob_step, prob_remainder)
    time   <- ifelse(whole_step, time_step, time_remainder)

    # Make tibble
    year <-
      tibble(src                = phs$src,
             subject            = phs$subject,
             phase_year         = x,
             phase              = phs$phase,
             time,
             cost,
             sales,
             prob
             ) %>% filter(whole_step | partial_step) # Only keep relevant data

    # Append
    phase_years <- bind_rows(phase_years, year)
  }
  phase_years
}
```

And then we convert.

```{r, phasely-to-yearly}
years <- to_years(phases)
```

We have now converted our phase-based data to year-based data.
It should be noted that the function does not distribute the phase-based data over a series of equidistant years.
Data points are only equidistant within a phase, but not necessarily across.
In other words, if P1 entry would occur after 5.3 years then we will distribute PC properties over the 6 first years.
If the duration of P1 is 2.5 years then P2 would start at year 5.3 and end at year 7.8.
While we would divide P1 into three equidistant (years) points (years), that start from year 5.3, 6.3, and 7.3 (respectively) they are not equidistant from the PC steps.


Below is an excerpt of the resulting dataset.

```{r, yearly-excerpt, echo=FALSE}
kable(head(years %>% arrange(subject, phase, phase_year), n=25))
```


## Valuation metrics
We employ the following financial metrics for cashflows:

- Non-capitalized value / Out-of-pocket value
- Risk-adjusted value (rV) / Expected value (EV)
- Capitalized value / Present value (PV)
- Risk-adjusted present value (rPV) / Expected present value (EPV)

When cumulating cashflows, the above financial metrics are respectively known as:

- Cumulative non-capitalized value / Cumulative out-of-pocket value
- Cumulative risk-adjusted value (Cumulative rV) / Cumulative expected value (Cumulative EV)
- Net present value (NPV)
- Risk-adjusted net present value (rNPV) / Expected net present value (ENPV)

We compute **Expected Value (EV)** as:

$$
EV_t = (R_t - C_t) * P_t
$$

where $R_t$ and $C_t$ are the revenues and costs (respectively) at time $t$, and $P_t$ the probability of reaching the cashflow from the point of evaluation.
The probability of reaching a given timestep $t_n$ from a point of evaluation $t_0$ is simply computed as: $P_t = \prod_{t_0}^{t^n}$.
Next, we compute **Present Value (PV)** as:

$$
\mathit{PV}_t = \frac{R_t - C_t}{(1 + i)^t}
$$

where $i$ is the discount rate of the evaluator, and $t$ is the time to the phase from the point of evaluation.
We compute **Expected Present Value (EPV)** as:

$$
\mathit{EPV}_t = \frac{R_t - C_t}{(1 + i)^t} * P_t
$$

Moving on to the cumulative valuations, we compute

**Net Expected Value (ENV)** as:

$$
\mathit{ENV_T} = \sum_{t\in T} EV_t\\
$$

**Net Present Value (NPV)** as:

$$
\mathit{NPV_T} = \sum_{t\in T} \mathit{PV}_t\\
$$

and **Expected Net Present Value (ENPV)** as:

$$
\mathit{ENPV_T} = \sum_{t\in T} \mathit{EPV}_t\\
$$

where $t$ is the time to the phase, and $T$ is the times to all timesteps of all phases for the project in evaluation.

<div class="alert alert-danger">
TODO: Verify that the probability portion of ENPV is actually computed accordingly!
</div>

## Valuation perspectives
To compare the cost of direct and indirect funding we need a way to compute the costs of each.
In order to determine the cost of indirect funding, we need a way to determine which projects will reach a go-decision under a given prize intervention since this will determine which projects the benefactor actually pays for.
Lastly, many argue that planning without respecting market logic inherently involves a certain level of inefficiency and for this reason we must have a way of computing the cost of indirect funding under different inefficiencies.

The above, leads us to the four different perspectives outlined in the table below.

| Perspective        | Summary                                                                                    |
| ------------------ | ------------------------------------------------------------------------------------------ |
| Private            | The private value of a project.                                                            |
| Intervened private | The private value of a project given the existance of a prize intervention.                |
| Indirect           | The negative value of issuing a prize intervention.                                        |
| Direct             | The negative value of paying for a project at-cost.                                        |
| Inefficient direct | The negative value of paying for a project at-cost under some operating inefficiency.      |

Any of the previously outlined valuation metrics can be computed from any of these perspectives.
Computing e.g. ENPV would yield the following valuations:

| Valuation metric          | Summary                                                                                        |
| ------------------------- | ---------------------------------------------------------------------------------------------- |
| Private ENPV              | The expected and capitalized private value of a project.                                                       |
| Intervened private ENPV   | The expected and capitalized private value of a project given the existance of a prize intervention.           |
| Indirect ENPV             | The expected and capitalized negative value of issuing a prize intervention.                                   |
| Direct ENPV               | The expected and capitalized negative value of paying for a project at-cost.                                   |
| Inefficient direct ENPV   | The expected and capitalized negative value of paying for a project at-cost under some operating inefficiency. |

Programatically, we use the function below to compute the valuation metrics for all perspectives at the start of pre-clinical.

```{r, valuation-function}
valuation <- function(df) {
  df %>%
    # Group by subject and intervention
    group_by(subject, prize_phase) %>%
    # Ensure order is correct before computing time to
    arrange(prize_phase, subject, phase, phase_year) %>%
    # Add absolute time (t) column
    mutate(time_to = cumsum(time) - time) %>%
    # Ensure order is based on time_to
    arrange(time_to) %>%
    # Valuation
    mutate(# Helpers
           inef_time = time + time * publ_inefficiency,
           inef_cost = cost + cost * publ_inefficiency,
           inef_time_to = cumsum(inef_time) - inef_time,
           prob_to = cumprod(prob) / prob,
           prize = ifelse(phase_year == 1 & phase == prize_phase, prize_size, 0),
           # Cashflow
           priv_cf       = sales - cost,
           int_priv_cf   = sales - cost + prize,
           ind_cf        = -prize,
           dir_cf        = -cost,
           inef_dir_cf   = -inef_cost,
           # Expected value
           priv_ev       = priv_cf     * prob_to,
           int_priv_ev   = int_priv_cf * prob_to,
           ind_ev        = ind_cf      * prob_to,
           dir_ev        = dir_cf      * prob_to,
           inef_dir_ev   = inef_dir_cf * prob_to,
           # Private value
           priv_pv       = priv_cf     / ((1 + priv_discount_rate) ^ time_to),
           int_priv_pv   = int_priv_cf / ((1 + priv_discount_rate) ^ time_to),
           ind_pv        = ind_cf      / ((1 + publ_discount_rate) ^ time_to),
           dir_pv        = dir_cf      / ((1 + publ_discount_rate) ^ time_to),
           inef_dir_pv   = inef_dir_cf / ((1 + publ_discount_rate) ^ time_to),
           # Expected present value
           priv_epv      = priv_pv * prob_to,
           int_priv_epv  = int_priv_pv * prob_to,
           ind_epv       = ind_pv * prob_to,
           dir_epv       = dir_pv * prob_to,
           inef_dir_epv  = inef_dir_pv * prob_to,
           # Cumulative cashflow
           priv_ccf      = cumsum(priv_cf),
           int_priv_ccf  = cumsum(int_priv_cf),
           ind_ccf       = cumsum(ind_cf),
           dir_ccf       = cumsum(dir_cf),
           inef_dir_ccf  = cumsum(inef_dir_cf),
           # Expected net value
           priv_env      = cumsum(priv_ev),
           int_priv_env  = cumsum(int_priv_ev),
           ind_env       = cumsum(ind_ev),
           dir_env       = cumsum(dir_ev),
           inef_dir_env  = cumsum(inef_dir_ev),
           # Net present value
           priv_npv      = cumsum(priv_pv),
           int_priv_npv  = cumsum(int_priv_pv),
           ind_npv       = cumsum(ind_pv),
           dir_npv       = cumsum(dir_pv),
           inef_dir_npv  = cumsum(inef_dir_pv),
           # Expected net present value
           priv_enpv     = cumsum(priv_epv),
           int_priv_enpv = cumsum(int_priv_epv),
           ind_enpv      = cumsum(ind_epv),
           dir_enpv      = cumsum(dir_epv),
           inef_dir_enpv = cumsum(inef_dir_epv),
           ) %>%
    # Ungroup to avoid surprises when using result
    ungroup
}
```


## Applying valuation
Before we can apply the valuation function we need to:

1. Merge the additional information dataset with the base dataset that we converted to a yearly format.
2. Replicate the full dataset once per treatment type, i.e. once per intervention target phase.

```{r, merge-yearly-and-additional}
valuable <- years %>%
  # Merge with additional information
  merge(., additional, by='subject') %>%
  # Cartesian merge with target phases distribution
  merge(., intervention_phase_dist)
```

We then apply our valuation function.

```{r, valuation}
valued <- valuation(valuable)
```

Since we are mostly interested in the value of bringing a project from start to finish, we will prepare a subset of the `valued` dataset that only contain the last "timestep" of every project.
When only looking at the final value we are only interested in the cumulative valuation metrics and therefore also filter out all CF, PV, EV, and EPV columns.
Further we also remove all timestep specific information.

```{r, finals}
finals <- valued %>%
  filter(src != 'DRIVE-AB (2018)') %>% # TODO: Remove the DRIVE-AB dataset completely instead.
  group_by(subject, prize_phase) %>%
  filter(time_to == max(time_to)) %>%
  select(-matches('_cf$|_pv$|_ev$|_epv$')) %>%
  select(-phase_year,
         -phase,
         -time,
         -inef_time,
         -cost,
         -inef_cost,
         -sales,
         -prob,
         -prize) %>%
  ungroup
```


# Analysis
TODO: Re-add summary statistics here.

## Private ENPV

Private ENPV clearly increase as prize sizes increase, regardless of intervention phase.

```{r, private-enpv-improvement, echo=FALSE}
finals %>%
  ggplot(aes(prize_size, int_priv_enpv - priv_enpv, col=prize_phase)) +
  geom_point(alpha=1, shape=1) +
  facet_wrap(prize_phase ~ src, ncol=6, scale='free') +
  xlab('Prize') +
  ylab('Private ENPV improvement') +
  theme(legend.position = 'top',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm')))
```

When plotting different phase prizes side by side we observe a "layering" where later phases are more costly than earlier ones.
Heteroscedasticity is fixed by plotting log-log.

```{r, layers, echo=FALSE}
finals %>%
  ggplot(aes(prize_size, int_priv_enpv - priv_enpv, color=prize_phase)) +
  geom_smooth(method='lm', se=FALSE) +
  geom_point(shape=1) +
  xlab('Prize') +
  ylab('Private ENPV improvement') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = c(1:9 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = c(1:9 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  facet_wrap(. ~ src) +
  theme(legend.position = 'top')
```



## Probability of go

By dividing the number of go-decisions under some given range of parameters with the number of projects under that same parameter range we reach the ratio of go/no-decisions.
We will refer to this as P(go) since it also describes the probability of a project reaching a go-decision.
We assume that go-decisions are reached when Private ENPV ≥ 0 or Intervened Private ENPV ≥ 0, before and after interventions respectively.

### Base probability
Without considering interventions.

```{r, distribution-of-go, echo=FALSE}
tmp <- finals %>%
  group_by(src) %>%
  summarize(go = sum(ifelse(priv_enpv >= 0, 1, 0)) / n(),
            no = sum(ifelse(priv_enpv >= 0, 0, 1)) / n()) %>%
  mutate(src = fct_reorder(src, -go)) %>%
  gather('go', 'percentage', -src) %>%
  ungroup

tmp %>%
  arrange(go) %>%
  mutate(go = go == 'go',
         percentage = percentage * 100) %>%
  ggplot(aes(src, percentage, fill=go)) +
  geom_col() +
  geom_text(aes(label=round(percentage, 0)),
                position=position_stack(vjust=0.5)) +
  ylab('%') +
  xlab(element_blank()) +
  labs(fill=element_blank()) +
  scale_y_continuous(breaks = pretty_breaks(n=10))

tmp %>%
  mutate(percentage = percentage * 100) %>%
  spread(go, percentage) %>%
  kable(digits=0, caption='Percentage go/no-go without interventions')
```


### Predict probability of go
Using logistic regression we can predict P(go) from intervention prize size.

Jitter plotting prize size and go-decisions suggests that logistic regression is a reasonable approach to the problem at hand.

```{r, prize-vs-no-to-go, echo=FALSE}
finals %>%
  filter(priv_enpv < 0) %>%
  ggplot(aes(prize_size, int_priv_enpv >= 0)) +
  geom_jitter(shape=1) +
  facet_grid(cols=vars(src), rows=vars(prize_phase)) +
  ylab('Go\n(Intervened Private ENPV ≥ 0)') +
  xlab('Prize size') +
  ggtitle('Private ENPV < 0') +
  theme(legend.position = 'none') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(seq(0,10,2))),
                     labels = trans_format('log10', math_format(10^.x)))
```

Build model.

```{r, hjlfksjdhf}
go_model <- function(df)
  glm(int_priv_enpv >= 0 ~ prize_size,
      data = df %>% filter(priv_enpv < 0),
      family = binomial(link='logit'))

predict_response <- function(fit, df)
  predict(fit, newdata=df, type='response')
```

We run the model once per combination of prize phase and source to avoid having to model interaction effects.

```{r, lshfjksdf}
go_models <- finals %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(go_fit = map(data, go_model),
         pred_go = map2(go_fit, data, predict_response),
         tidied = map(go_fit, tidy)) %>%
  ungroup

# NOTE: Using legacy unnest because:
# https://github.com/tidyverse/tidyr/issues/694

tidied <- go_models %>%
  unnest_legacy(tidied)
```

```{r, kjhlsdfsd, echo=FALSE}
go_models %>%
  unnest(data, pred_go) %>%
  ggplot(aes(prize_size, pred_go, color=prize_phase)) +
  geom_line() +
  facet_wrap(. ~ src) +
  ylab('P(go)') +
  xlab('Prize size') +
  theme(legend.position = 'top') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(seq(0,10,2))),
                     labels = trans_format('log10', math_format(10^.x)))
```

All p-values are significant.

```{r, skjldhfsdf, echo=FALSE}
digits = 4
tidied %>%
  select(src, prize_phase, term, p.value) %>%
  spread(prize_phase, p.value) %>%
  kable(digits=4,
        caption=sprintf('P-values (rounded to %s decimals, i.e. p < %s if 0)',
                        digits,
                        10^-digits))
```


### Predict prize size
To "reverse" the model and predict prize from some given probability we use the following function.

$$
f(y) =
\frac{
  ln(\frac{y}{1-y}) - \beta_0
}{
  \beta_1
}
$$

Which we encode as follows.

```{r, skhjflsdkf}
predict_prize_from_prob <- function(tidied, y) {
  b0 <- filter(tidied, term=='(Intercept)')$estimate
  b1 <- filter(tidied, term=='prize_size')$estimate
  (log(y / (1-y)) - b0) / b1
}
```

We can sanity check the output of the function by predicting prize size from predicted P(go) and check if it is correlated with the actual prize size that P(go) was predicted from.

```{r, lkhjgdfg, echo=FALSE}
tmp <- go_models %>%
  mutate(pred_prize_size = map2(tidied, pred_go, predict_prize_from_prob)) %>%
  unnest_legacy(pred_prize_size, pred_go, data)
```

```{r, ashjldfsdj, echo=FALSE}
max_prob <- 0.99999
tmp %>%
  filter(pred_go < max_prob) %>%
  ggplot(aes(prize_size, pred_prize_size)) +
  geom_line() +
  facet_wrap(prize_phase ~ src, scale='free') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  xlab('Prize size') +
  ylab('Predicted prize size') +
  theme(legend.position = 'none',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm'))) +
  ggtitle(sprintf('where P(go) < %s', max_prob))
```

Note that (not evident in the plot above is that) predicted prize size becomes a constant as y reaches 100%.
Therefore significant differences between predicted and actual prize appear and grow when approximating P(go) = 1 since we have sampled prizes much larger than prizes required to yield 100% go but when predicting required prize sizes to achieve 100% go will always yield the same prize size.

```{r, kjsdfhlksjdf, echo=FALSE}
tmp %>%
  filter(pred_go < max_prob) %>%
  mutate(diff = prize_size - pred_prize_size) %>%
  ggplot(aes(pred_go*100, diff)) +
  geom_point(shape=1) +
  facet_wrap(. ~ prize_phase, nrow=1) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(breaks=pretty_breaks(n=20)) +
  theme(axis.text.x = element_text(angle=90, hjust=1)) +
  ggtitle(sprintf('where P(go) < %s', max_prob))
```

```{r, sdlfhsdklfhsd, echo=FALSE}
tmp %>%
  ggplot(aes(prize_size, pred_go*100, group=prize_phase)) +
  geom_line(color='black', size=2) +
  geom_line(aes(x = pred_prize_size,
                y = pred_go*100),
            color = 'white',
            size = 0.7) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  xlab('Prize size') +
  ylab('P(go)') +
  facet_wrap(.~src) +
  coord_cartesian(xlim = c(10^6.7, 10^10)) +
  ggtitle(sprintf('Sanity checking of reverse prediction\n(white = predict x from predicted y)\nwhere P(go) < %s', max_prob)) +
  theme(legend.position = 'bottom')
```


### Effective prize sizes
Using our function that solves for prize size from a given probability of go we can predict what prize sizes are required to achive a target probability of go of `r target=0.95; target*100`% in each of the sources?

```{r, klhjsdfkdf}
effectives <- tidied %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(pred_prize_size = map2_dbl(data, target, predict_prize_from_prob)) %>%
  select(-data) %>%
  ungroup
```

```{r, alslflhjsdfkjhs, echo=FALSE}
effectives %>%
  ggplot(aes(src, pred_prize_size/10^6, fill=src)) +
  geom_col(position='dodge') +
  facet_wrap(. ~ prize_phase, nrow=1, scale='free_x') +
  scale_y_continuous(breaks = pretty_breaks(n = 20)) +
  ylab('Prize size\n(million usd)') +
  xlab(element_blank()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = 'none') +
  ggtitle(sprintf('P(go) = %s%%', target*100))
```

If prizes are not targeted by indication we must choose the highest prize to ensure a minimum of `r target*100`% go-decisions for all indications.
Hence the "max" row.

```{r, hjklfsdf, echo=FALSE}
effectives %>%
  group_by(prize_phase) %>%
  mutate(src = '(max)') %>%
  summarize(pred_prize_size = max(pred_prize_size),
            src = '(max)') %>%
  rbind(effectives, .) %>%
  mutate(pred_prize_size = round(pred_prize_size / 10^6)) %>%
  spread(prize_phase, pred_prize_size) %>%
  kable(caption=sprintf('Million usd (rounded). P(go) = %s%%.', target*100))
```



## Indirect ENPV
The cost of issuing an intervention depends on both prize size and prize phase.
When computing the cost of an intervention we only include projects that actually reach go-decisions after (even if go was reached before) the introduction of the intervention since the benefactor will only ever (possibly) pay for projects that are actually undertaken.

```{r, prize-vs-indirect-enpv-excluding-nos, echo=FALSE}
finals %>%
  filter(int_priv_enpv >= 0) %>%
  ggplot(aes(prize_size, -ind_enpv, color=prize_phase)) +
  geom_point(shape=1) +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ src) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = c(1:9 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = c(1:9 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  xlab('Prize size') +
  ylab('Indirect ENPV') +
  ggtitle('Intervened Private ENPV ≥ 0') +
  theme(legend.position = 'bottom')
```

We will only look at go-rates lower than `r go_lim_up=0.99; go_lim_up*100`%.
Since a go-rate higher than 100% is not achievable, any prize size larger than the lowest size that yields a go-rate of 100% will also merely yield 100%.

```{r, shkldflkjshsf, echo=FALSE}
for (curr in unique(go_models$prize_phase)) {
  p <- go_models %>%
    filter(prize_phase == curr) %>%
    unnest_legacy(data, pred_go) %>%
    filter(pred_go < go_lim_up) %>%
    filter(int_priv_enpv >= 0) %>%
    ggplot(aes(pred_go*100, -ind_enpv)) +
    geom_point(shape=1) +
    geom_smooth(method='loess', se=FALSE, color='red') +
    xlab('P(go)') +
    ylab('Indirect ENPV per entry\n(million usd)') +
    scale_y_continuous(label = unit_format(scale = 1e-6, unit = '')) +
    scale_x_continuous(breaks = pretty_breaks(n=10)) +
    facet_wrap(. ~ src, scale='free') +
    ggtitle(sprintf('Intervened Private ENPV ≥ 0    Prize phase = %s    P(go) < %s', curr, go_lim_up)) +
    theme(legend.position = 'bottom')
  print(p)
}
```

When smoothing the curves, phase wise "layering" again become obvious.

```{r, go-rate-vs-indirect-enpv-smooth, echo=FALSE}
go_models %>%
  unnest_legacy(data, pred_go) %>%
  filter(pred_go < go_lim_up) %>%
  filter(int_priv_enpv >= 0) %>%
  ggplot(aes(pred_go*100, -ind_enpv, color=prize_phase)) +
  geom_smooth(method='loess', se=FALSE) +
  scale_linetype_manual(values=c('dotted', 'solid')) +
  facet_wrap(. ~ src, scales='free') +
  xlab('P(go)') +
  ylab('Indirect ENPV\n(million usd)') +
  ggtitle('Intervened Private ENPV ≥ 0  (per go-decision)') +
  scale_y_continuous(label = unit_format(scale = 1e-6, unit = '')) +
  theme(legend.position = 'bottom', legend.title = element_blank())
```

The layering remains, even when dividing the indirect ENPV of each project by the probability that it reaches the market.

```{r, go-rate-vs-exit, echo=FALSE}
go_models %>%
  unnest_legacy(data, pred_go) %>%
  filter(pred_go < go_lim_up) %>%
  filter(int_priv_enpv >= 0) %>%
  ggplot(aes(pred_go*100, -ind_enpv/prob_to, color=prize_phase)) +
  geom_smooth(method='loess', se=FALSE) +
  facet_wrap(. ~ src, scales='free', nrow=2) +
  xlab('P(go)') +
  ylab('Indirect ENPV\n(million usd)') +
  ggtitle('Intervened Private ENPV ≥ 0    (per market entry)') +
  scale_y_continuous(label = unit_format(scale = 1e-6, unit = '')) +
  theme(legend.position = 'bottom')
```


## Direct ENPV

```{r, inefficiency-vs-direct-enpv-per-entry, echo=FALSE}
finals %>%
  ggplot(aes(publ_inefficiency*100, -inef_dir_enpv, color=src)) +
  geom_point(shape=1) +
  geom_smooth(method='lm', se=FALSE, color='black') +
  facet_wrap(. ~ src) +
  xlab('Public inefficiency (%)') +
  ylab('Direct ENPV\n(million usd)') +
  ggtitle('per go-decision') +
  scale_y_continuous(label = unit_format(scale = 1e-6, unit = '')) +
  theme(legend.position='none')
```

```{r, inefficiency-vs-direct-enpv-per-exit, echo=FALSE}
finals %>%
  ggplot(aes(publ_inefficiency*100, -inef_dir_enpv/prob_to, color=src)) +
  geom_point(shape=1) +
  geom_smooth(method='lm', se=FALSE, color='black') +
  facet_wrap(. ~ src) +
  xlab('Public inefficiency (%)') +
  ylab('Direct ENPV\n(million usd)') +
  ggtitle('per market entry') +
  scale_y_continuous(label = unit_format(scale = 1e-6, unit = '')) +
  theme(legend.position='none')
```


## Between-subject comparison

Note the truncation of the right plots.

```{r, cost-difference-comparison, echo=FALSE}
for (curr in unique(go_models$src)) {
  sub <- go_models %>%
    filter(src == curr) %>%
    unnest_legacy(data, pred_go) %>%
    filter(pred_go < go_lim_up) %>%
    filter(int_priv_enpv >= 0)

  p1 <- sub %>%
    ggplot(aes(pred_go*100, -ind_enpv/prob_to,
              group=prize_phase, color=prize_phase)) +
    geom_point(shape=20, alpha=0.5) +
    geom_smooth(method='loess', se=FALSE) +
    geom_line(stat='smooth', method='loess', color='black', alpha=0.2, size=1) +
    xlab('% no turned go') +
    ylab('Indirect ENPV per exit\n(million usd)') +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
    scale_y_continuous(label = unit_format(scale = 1e-6, unit = ''),
                       breaks = scales::pretty_breaks(n = 8)) +
    theme(legend.position = 'bottom') +
    ggtitle('Indirect funding')

  p2 <- sub %>%
    ggplot(aes(publ_inefficiency*100, -inef_dir_enpv/prob_to)) +
    geom_point(shape=20, color='darkgrey', alpha=0.6) +
    geom_line(stat='smooth', method='lm', size=1, color='red') +
    xlab('Public inefficiency (%)') +
    ylab('Direct ENPV per exit\n(million usd)') +
    scale_y_continuous(label = unit_format(scale = 1e-6, unit = ''),
                       breaks = scales::pretty_breaks(n = 8),
                       position='right'
                       ) +
    theme(legend.position = 'none') +
    ggtitle('Direct funding')

  # Sync y axis
  p1y <- ggplot_build(p1)$layout$panel_scales_y[[1]]$range$range
  p2y <- ggplot_build(p2)$layout$panel_scales_y[[1]]$range$range
  p1 <- p1 + coord_cartesian(ylim = range(p1y))
  p2 <- p2 + coord_cartesian(ylim = range(p1y))

  #print(plot_grid(p1, p2, ncol=2, align='h', axis='bt', rel_widths=c(1.8,1)))
  ggarrange(p1, p2, ncol=2, nrow=1, widths=c(1.8, 1), common.legend=TRUE, legend='top') %>%
    annotate_figure(top = curr) %>%
    print
}
```


## Binned difference

TODO: Doesn't seem like we have enough samples in these narrow spans.
Resample a specific prize and inefficiency and use that instead.

```{r, kljhsdkjlhlsdkhgk, echo=FALSE}
go_models %>%
  unnest_legacy(data, pred_go) %>%
  filter(int_priv_enpv >= 0) %>%
  filter(pred_go >= 0.94) %>%
  filter(pred_go <= 0.99) %>%
  mutate(favorable = ifelse(inef_dir_enpv > ind_enpv, 'Direct', 'Indirect')) %>%
  ggplot(aes(publ_inefficiency * 100,
             (inef_dir_enpv - ind_enpv)/10^6,
             color=prize_phase)) +
    ggtitle(sprintf('%s  p(go) = [0.94, 0.99]', '')) +
    ylab('Difference in favor of direct (million usd per go-decision)') +
    geom_smooth(method='lm', se=FALSE) +
    facet_wrap(. ~ src, nrow=1) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = 'bottom')
```

### Per phase
```{r, within-subject-high-go-rates-per-phase, echo=FALSE}
for (curr in unique(finals$prize_phase)) {
  p <- go_models %>%
    filter(prize_phase == curr) %>%
    unnest_legacy(data, pred_go) %>%
    filter(int_priv_enpv >= 0) %>%
    filter(pred_go >= 0.91) %>%
    filter(pred_go <= 0.99) %>%
    mutate(favorable = ifelse(inef_dir_enpv > ind_enpv, 'Direct', 'Indirect')) %>%
    ggplot(aes(publ_inefficiency * 100,
              (inef_dir_enpv - ind_enpv)/10^6)) +
    ggtitle(sprintf('%s    P(go) = [0.91, 0.99]', curr)) +
    ylab('Difference in favor of direct (million usd per go-decision)') +
    geom_point(aes(color=favorable)) +
    geom_smooth(method='lm', se=FALSE, color='black') +
    facet_wrap(. ~ src, nrow=1) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  print(p)
}
```

### Combined

```{r, hsdkjfhslhskdfgkc, echo=FALSE}
go_models %>%
  unnest_legacy(data, pred_go) %>%
  filter(int_priv_enpv >= 0) %>%
  filter(pred_go >= 0.91) %>%
  filter(pred_go <= 0.99) %>%
  mutate(favorable = ifelse(inef_dir_enpv > ind_enpv, 'Direct', 'Indirect')) %>%
  ggplot(aes(publ_inefficiency * 100,
             (inef_dir_enpv - ind_enpv)/10^6)) +
  ggtitle('all datasets  P(go) = [0.91, 0.99]') +
  ylab('Difference in favor of direct (million usd per go-decision)') +
  geom_point(aes(color=favorable)) +
  geom_smooth(method='lm', se=FALSE, color='black') +
  facet_wrap(. ~ prize_phase, nrow=1) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 12)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Point estimated inefficiency

```{r, sdfhsdflg, echo=FALSE}
tmp <- go_models %>%
  unnest_legacy(data, pred_go) %>%
  filter(int_priv_enpv >= 0) %>%
  filter(pred_go >= 0.91) %>%
  filter(pred_go <= 0.99) %>%
  filter(publ_inefficiency >= 0.3) %>%
  filter(publ_inefficiency <= 0.4)
```

#### Per market entry
```{r, aslkjdhfkjfd, echo=FALSE}
tmp %>%
  group_by(prize_phase, src) %>%
  summarize(mean_diff = mean(inef_dir_enpv/prob_to - ind_enpv/prob_to),
            sd_diff   = sd(inef_dir_enpv/prob_to - ind_enpv/prob_to),
            ymin      = mean_diff - sd_diff,
            ymin2     = mean_diff - sd_diff * 2,
            ymax      = mean_diff + sd_diff,
            ymax2     = mean_diff + sd_diff * 2,
            Favorable = ifelse(sum(inef_dir_enpv/prob_to) > sum(ind_enpv/prob_to),
                               'Direct',
                               'Indirect')) %>%
  ggplot(aes(src, mean_diff/10^6, fill=Favorable)) +
    geom_col() +
    geom_linerange(color='red',
                   aes(ymin=ifelse(ymin>=0, ymin/10^6, 0),
                       ymax=ifelse(ymax>=0, ymax/10^6, 0))) +
    geom_linerange(color='blue',
                   aes(ymin=ifelse(ymin<=0, ymin/10^6, 0),
                       ymax=ifelse(ymax<=0, ymax/10^6, 0))) +
    facet_wrap(. ~ prize_phase, ncol=1, strip.position='right') +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 12)) +
    ylab('Mean cost-savings of direct funding per market entry (million usd)') +
    xlab(element_blank()) +
    coord_flip() +
    ggtitle('P(go) = [0.91, 0.99]     Public inefficiency = [0.3, 0.4]     +/- 1sd') +
    theme(legend.position = 'right',
          panel.spacing = unit(1, 'mm'))
```

```{r, hlkjsdslkjdhfkjfd, echo=FALSE}
tmp %>%
  group_by(prize_phase) %>%
  summarize(mean_diff = mean(inef_dir_enpv/prob_to - ind_enpv/prob_to),
            Favorable = ifelse(sum(inef_dir_enpv/prob_to) > sum(ind_enpv/prob_to),
                               'Direct',
                               'Indirect')) %>%
  mutate(prize_phase = factor(prize_phase, levels=rev(PHASES))) %>%
  ggplot(aes(prize_phase, mean_diff/10^6, fill=Favorable)) +
    geom_col() +
    coord_flip() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 12)) +
    ylab('Mean cost-savings of direct funding\nper market entry (million usd)') +
    xlab(element_blank()) +
    ggtitle('P(go)=[0.91, 0.99]  public inefficiency=[0.3, 0.4]\nAssumes prize size tailored per indication to avoid overpaying')
```

#### Per go-decision
```{r, aslkjdhffd, echo=FALSE}
tmp %>%
  group_by(prize_phase, src) %>%
  summarize(mean_diff = mean(inef_dir_enpv - ind_enpv),
            sd_diff   = sd(inef_dir_enpv - ind_enpv),
            ymin      = mean_diff - sd_diff,
            ymin2     = mean_diff - sd_diff * 2,
            ymax      = mean_diff + sd_diff,
            ymax2     = mean_diff + sd_diff * 2,
            Favorable = ifelse(sum(inef_dir_enpv) > sum(ind_enpv),
                               'Direct',
                               'Indirect')) %>%
  ggplot(aes(src, mean_diff/10^6, fill=Favorable)) +
    geom_col() +
    geom_linerange(color='red',
                   aes(ymin=ifelse(ymin>=0, ymin/10^6, 0),
                       ymax=ifelse(ymax>=0, ymax/10^6, 0))) +
    geom_linerange(color='blue',
                   aes(ymin=ifelse(ymin<=0, ymin/10^6, 0),
                       ymax=ifelse(ymax<=0, ymax/10^6, 0))) +
    facet_wrap(. ~ prize_phase, ncol=1, strip.position='right') +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 12)) +
    ylab('Mean cost-savings of direct funding per go-decision (million usd)') +
    xlab(element_blank()) +
    coord_flip() +
    ggtitle('P(go) = [0.91, 0.99]     Public inefficiency = [0.3, 0.4]     +/- 1sd') +
    theme(legend.position = 'right',
          panel.spacing = unit(1, 'mm'))
```

```{r, asljkhdsskdf, echo=FALSE}
tmp %>%
  group_by(prize_phase) %>%
  summarize(mean_diff = mean(inef_dir_enpv - ind_enpv),
            Favorable = ifelse(sum(inef_dir_enpv) > sum(ind_enpv),
                               'Direct',
                               'Indirect')) %>%
  mutate(prize_phase = factor(prize_phase, levels=rev(PHASES))) %>%
  ggplot(aes(prize_phase, mean_diff/10^6, fill=Favorable)) +
    geom_col() +
    scale_y_continuous(breaks = scales::pretty_breaks(n = 12)) +
    ylab('Mean cost-savings of direct funding per go-decision (million usd)') +
    xlab(element_blank()) +
    coord_flip() +
    ggtitle('P(go)=[0.91, 0.99]  public inefficiency=[0.3, 0.4]\nAssumes prize size tailored per indication to avoid overpaying') +
    theme(legend.position = 'right')
```


## Predicting cost difference

We can predict the cost difference between indirect and direct funding by building a linear model where public inefficiency and P(go) are independent variables.
Note that since P(go) is predicted from prize size we could have used prize size as an independent variable but choose to use P(go) since it is a more meaningful model construct.
Choosing P(go) over prize size also makes it significantly easier to compare cost differences across prize phases without having to correct for the effect of the phase of intervention.

Note that it only makes sense to compare cost difference in projects that reach go-decisions (hence the filter below).
Also note that we only consider prize sizes with a predicted P(go) < `r go_lim_up` to avoid nonlinearity in the prediction model, caused by the fact that prize sizes larger than the one needed for P(go) = 1 will still yield P(go) = 1.

```{r, lkjkj1hghjk}
diff_model <- function(df)
  glm(inef_dir_enpv - ind_enpv ~ publ_inefficiency + pred_go, data = df)

diff_models <- go_models %>%
  unnest_legacy(data, pred_go) %>%
  # Only compare cost diff in projs with go-deciison after intervention
  filter(int_priv_enpv >= 0) %>%
  # Ignore prize sizes that cause P(go) ~ 100
  filter(pred_go < go_lim_up) %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(diff_fit = map(data, diff_model),
         pred_diff = map2(diff_fit, data, predict_response),
         tidied = map(diff_fit, tidy),
         glanced = map(diff_fit, glance)) %>%
  ungroup
```

All p-values are significant.

```{r, akhjlshlkdsf, dependson=-1, echo=FALSE}
decimals <- 5
diff_models %>%
  unnest_legacy(tidied) %>%
  select(src, prize_phase, term, p.value) %>%
  spread(prize_phase, p.value) %>%
  kable(digits=decimals,
        caption=sprintf('P-values rounded to %s decimals (i.e. p < %s if 0)',
                        decimals,
                        10^-decimals))
```

```{r, jlshdfk, dependson=-1, echo=FALSE}
diff_models %>%
  unnest_legacy(pred_diff, data) %>%
  ggplot(aes(pred_go, pred_diff)) +
  geom_point(shape=1) +
  facet_wrap(prize_phase ~ src, scale='free') +
  theme(legend.position = 'none',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm')))

diff_models %>%
  unnest_legacy(pred_diff, data) %>%
  ggplot(aes(publ_inefficiency, pred_diff)) +
  geom_point(shape=1) +
  facet_wrap(prize_phase ~ src, scale='free') +
  theme(legend.position = 'none',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm')))
```


### Difference heatmap
Using the above model we can generate a dataset of independent variables spanning the full range from 0 to 1 and then predict the cost difference for all points.
From this dataset and its predictions we can begin exploring these three-dimensional relationships.

```{r, sjhlfsdf}
steps <- 21
x <- data.frame(pred_go = seq(0, 1, length.out=steps))
y <- data.frame(publ_inefficiency = seq(public_inefficiency_min,
                                        public_inefficiency_max,
                                        length.out=steps))
# Build cartesian product of independent variables
variations <- merge(x, y)
```

Predict dependent variable, i.e. the cost difference between direct and indirect funding, from the broad range of independent variables generated above.

```{r, alhlkjsadhf, dependson=-1}
span <- diff_models %>%
  select(src, prize_phase, diff_fit) %>%
  group_by(src, prize_phase) %>%
  mutate(data = list(variations),
         pred_diff = map2(diff_fit, data, predict_response)) %>%
  unnest_legacy(data, pred_diff) %>%
  ungroup
```

The predictions yield the following heatmaps.

```{r, jkhflsf, dependson=-1, echo=FALSE}
for (curr in unique(tmp$prize_phase)) {
  p <- span %>%
    filter(prize_phase == curr) %>%
    ggplot(aes(pred_go*100, publ_inefficiency*100, z=pred_diff/10^6)) +
      facet_wrap(. ~ src) +
      geom_tile(aes(fill=pred_diff/10^6)) +
      #geom_contour(bins=steps, color='black', alpha=0.25) +
      scale_fill_gradient2(low='blue', high='red', mid='white', name='Diff', breaks=pretty_breaks(n=7)) +
      ylab('Public inefficiency') +
      xlab('P(go)') +
      ggtitle(curr) +
      theme(legend.position = 'right',
        panel.spacing = unit(1, 'mm'))
  print(p)
}
```

### Difference table

Tables with same information for P1 and M1.

```{r, sdfkjlhsdfg, dependson=-1, echo=FALSE, results='asis'}
for (curr_phase in c('P1', 'M1')) {
  tmp <- span %>%
    filter(prize_phase == curr_phase) %>%
    group_by(pred_go, publ_inefficiency) %>%
    summarize(mean_pred_diff = mean(pred_diff)) %>%
    ungroup %>%
    select(pred_go, publ_inefficiency, mean_pred_diff) %>%
    mutate(mean_pred_diff = round(mean_pred_diff/10^6, 1),
           pred_go = round(pred_go * 100, 0),
           publ_inefficiency = round(publ_inefficiency * 100, 0)) %>%
    spread(publ_inefficiency, mean_pred_diff) %>%
    kable(caption=sprintf('Mean (across indications) cost-savings of direct funding (per go-decision) of %s prize', curr_phase)) %>%
    print
}
```


## Resampled within-subject analysis
TODO: Resample with prizes that according to the go model yields a P(g) ~ 0.95. Run with both prizes that are suitable per indication but also with prizes that are suitable for all indications without controlling for indications. I.e. a broad vs a narrow PER.

