---
title: Time for public pharma?
subtitle: Supplementary material
author: Christopher Okhravi
date: 2019
output:
  html_document:
    toc: true
    #toc_float:
    #  collapsed: false
    theme: cosmo
  pdf_document:
    toc: true
    latex_engine: xelatex
---

<!--
# TODO: Something I haven't taken into consideration is that one can assume that a prize has a fixed number of max-recipients but still assume that all developers will believe that they will be the ones who get it. This should significantly reduce the cost per output antibiotic.
# TODO: I should make all Sertkaya data separate data sets. I.e. use all the different indications.
-->

```{r, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, collapse=TRUE)
```

```{r, libraries, include=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(triangle)
```

# Introduction
This document serves as supplementary material for the paper "Time for public pharma?".
This is a monte carlo simulation that explores direct versus indirect funding of antibiotics research and development (R&D).
Direct funding is here used to mean that a benefactor pays for antibiotics R&D at cost.
Indirect funding is here used to mean that a benefactor issues non-dilutive prizes to whoever completes a particular phase, with the intent of incentivizing private developers to undertake said and prior phases.
This analysis is attempting to estimate which of the two options would be cheaper, ceteris paribus, for the benefactor.

# Simulation and input data set

Let us begin by setting `N` to the number of samples that we want per data set.
This number represents the number of random samples that we'll draw from a stochastic representation of a hypothetical antibiotic R&D project.
The distribution of the stochastic project depends on the data set we're using, and in a simple attempt to avoid drawing data set specific conclusions, we'll look at multiple data sets.
Meaning that if one considers all the data sets at once then the number of random projects drawn is `N` multiplied with the number of data sets.

```{r}
N = 1000
```

## Sertkaya et. al (2014)

The following is an approximation of the data used by Sertkaya et. al (2014) as they make a few assumptions that are not reconcilable with the way we have chosen to model antibiotics R&D.
The points of differentiation are outlined as comments in the code chunk below.

<div class="alert alert-danger">
**Sertkaya uses triangular distributions, but we have used uniform distributions. This must be changed.**
</div>

```{r}
# Same across all phases
discount_rate <- runif(N, 0.09, 0.24)

# Samples phases
pc <- data.frame(subject=1:N, phase=factor('PC'))
pc$time             <- runif(N, min=4.3, max=6)
pc$cost             <- runif(N, min=19, max=23.2)
pc$sales            <- rep(0, N)
pc$prizes           <- rep(0, N)
pc$prob             <- runif(N, min=0.175, max=0.69)
pc$discount_rate    <- discount_rate
p1 <- data.frame(subject=1:N, phase=factor('P1'))
p1$time             <- runif(N, min=0.75, max=1.8)
p1$prob             <- runif(N, min=0.25, max=0.837)
p1$cost             <- runif(N, min=7.3, max=12)
p1$sales            <- rep(0, N)
p1$prizes           <- rep(0, N)
p1$discount_rate    <- discount_rate
p2 <- data.frame(subject=1:N, phase=factor('P2'))
p2$time             <- runif(N, min=0.75, max=2.5)
p2$sales            <- rep(0, N)
p2$prizes           <- rep(0, N)
p2$cost             <- runif(N, min=7.12, max=18.72)
p2$prob             <- runif(N, min=0.34, max=0.74)
p2$discount_rate    <- discount_rate
p3 <- data.frame(subject=1:N, phase=factor('P3'))
p3$cost             <- runif(N, min=26.88, max=121.68)
p3$prob             <- runif(N, min=0.314, max=0.786)
p3$time             <- runif(N, min=0.83, max=3.9)
p3$sales            <- rep(0, N)
p3$prizes           <- rep(0, N)
p3$discount_rate    <- discount_rate
p4 <- data.frame(subject=1:N, phase=factor('P4'))
p4$time             <- runif(N, min=0.5, max=1.04)
p4$prob             <- runif(N, min=0.83, max=0.99)
p4$cost             <- rep(98.297168, N)
p4$sales            <- rep(0, N)
p4$prizes           <- rep(0, N)
p4$discount_rate    <- discount_rate
mp <- data.frame(subject=1:N, phase=factor('MP'))
mp$prob          <- 1
mp$cost          <- 0
mp$sales         <- runif(N, min=218, max=2500)
mp$prizes        <- 0
mp$time          <- 10
mp$discount_rate <- discount_rate

# Combine all phases into single dataset
sertkaya2014 <- rbind(pc, p1, p2, p3, p4, mp)

# Set source name
sertkaya2014$src <- 'Sertkaya et. al (2014)'
```

## DRIVE-AB (2018)
This is an approximation of the data used in DRIVE-AB final report (2018).
Some deviations (reported as comments in the code chunk below) have been made as some of the DRIVE-AB assumptions are not compatible with our assumptions.

```{r}
discount_rate <- runif(N, 0.05, 0.30) # Same across phases
pc <- data.frame(subject=1:N, phase=factor('PC'))
pc$time          <- rtriangle(N, 4.33, 6, 5.5)
pc$cost          <- rtriangle(N, 14.25, 29, 21.1)
pc$sales         <- 0
pc$prizes        <- 0
pc$prob          <- rtriangle(N, 0.175, 0.69, 0.352)
pc$discount_rate <- discount_rate
p1 <- data.frame(subject=1:N, phase=factor('P1'))
p1$time          <- rtriangle(N, 0.75, 1.8, 0.875)
p1$prob          <- rtriangle(N, 0.25, 0.837, 0.33)
p1$cost          <- rtriangle(N, 13.1, 37.96, 24)
p1$sales         <- 0
p1$prizes        <- 0
p1$discount_rate <- discount_rate
p2 <- data.frame(subject=1:N, phase=factor('P2'))
p2$time          <- rtriangle(N, 0.75, 2.5, 1.08)
p2$sales         <- 0
p2$prizes        <- 0
# TODO: Below cost is incorrectly reported in DRIVE-AB report!
p2$cost          <- rtriangle(N, 4.55, 46.36, 12.95)
p2$sales         <- 0
p2$prizes        <- 0
p2$prob          <- rtriangle(N, 0.34, 0.74, 0.5)
p2$discount_rate <- discount_rate
p3 <- data.frame(subject=1:N, phase=factor('P3'))
p3$time          <- rtriangle(N, 0.83, 3.83, 1.82)
p3$cost          <- rtriangle(N, 10, 47, 21.8)
p3$prob          <- rtriangle(N, 0.314, 0.786, 0.67)
p3$sales         <- 0
p3$prizes        <- 0
p3$discount_rate <- discount_rate
p4 <- data.frame(subject=1:N, phase=factor('P4'))
p4$time          <- rtriangle(N, 0.5, 1.04, 0.75)
p4$prob          <- rtriangle(N, 0.83, 0.99, 0.85)
p4$cost          <- rtriangle(N, 55.5, 127.91, 88.35)
p4$sales         <- 0
p4$prizes        <- 0
p4$discount_rate <- discount_rate
mp <- data.frame(subject=1:N, phase=factor('MP'))
mp$prob          <- 1
mp$cost          <- 0
mp$sales         <- rtriangle(N, 0, 4336.00, 2559.5) # TODO: Does not model
# DRIVE-AB report correctly as the report does not assume that y1 has 0 sales.
mp$prizes        <- 0
mp$time          <- 10
mp$discount_rate <- discount_rate
driveab2018 <- rbind(pc, p1, p2, p3, p4, mp) # Combine phases
driveab2018$src <- 'DRIVE-AB (2018)' # Set source
```


## Combining all datasets
Before proceeding, we'll combine all datasets (sources) into a single dataset containing all sources.

```{r}
phases <- rbind(sertkaya2014, driveab2018)
```

```{r, include=FALSE}
# To avoid recomputing all the time we'll also store the names of the different sources:
sources <- unique(phases$src)
# To avoid misspellings of factors, lets' use levels:
metrics <- c('cashflow', 'ev', 'pv', 'epv')
cum_metrics <- c('cum', 'env', 'npv', 'enpv')
```

We must also convert `phase` into an ordered factor so that we can assume that `PC < P1 < P2 < P3 < P4 < MP`.

```{r}
phase_names <- c('PC','P1','P2','P3','P4','MP')
phase_levels <- factor(phase_names, levels=phase_names, ordered=TRUE)
phases$phase <- factor(phases$phase, levels=phase_names, ordered=TRUE)
```

Later we will apply interventions to this dataset and thereby create multiple permutations/versions of every observation.
This means that we must keep track of which intervention we're currently looking at, and as such we'll add that column immediately, and declare all the current data as suffering from "no intervention".
In terms of a randomized controlled trial, an intervention can be thought of as a "treatment".

```{r}
phases$intervention <- 'NONE'
```


## Summary statistics
Let us plot the phase parameters as frequency polygons (i.e. histograms) for each of the sources.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
p1 <- ggplot(phases, aes(cost, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab('Cost (million USD)')
p2 <- ggplot(phases, aes(time, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab('Duration (months)')
p3 <- ggplot(phases, aes(prob*100, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab('Probability (%)')
p4 <- ggplot(phases, aes(sales, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  xlab('Sales (million USD)')
ggarrange(p1, p2, p3, p4, ncol=2, nrow=2, common.legend=TRUE, legend='right')
```

Interestingly, some parameters are quite dispersly distributed between phases.
Consider e.g. how almost all of the cost is incurred in some phase(s) while all the time is spent in another.
The two following figures plot what percentage of the total of some property is spent in a given phase, per project.
The first plot is grouped by property, while the second by phase.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Transform: phase properties to long from wide
phase_props <- phases %>%
  filter(phase != 'MP') %>%
  select(-intervention, -discount_rate) %>%
  gather(key='prop', value='value', -src, -subject, -phase) %>%
  group_by(src, subject, prop) %>%
  mutate(total = sum(value),
         ratio = value / total) # NOTE: will cause NaN if 0/0

for (curr in sources) {
  p1 <- phase_props %>%
    filter(src == curr) %>%
    filter(!is.na(ratio)) %>%
    ggplot(aes(x=phase,y=ratio*100, fill=phase)) +
    geom_violin(draw_quantiles=c(0.25, 0.5, 0.75)) +
    facet_grid(~ prop) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          axis.ticks.x=element_blank()) +
      ylab('% of property in phase') + xlab('') +
      guides(fill = FALSE)

  p2 <- phase_props %>%
    filter(src == curr) %>%
    ggplot(aes(x=prop,y=ratio*100, fill=prop)) +
    geom_violin(draw_quantiles=c(0.25, 0.5, 0.75)) +
    facet_grid(~ phase) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          axis.ticks.x=element_blank()) +
        ylab('% of property in phase') + xlab('') +
        guides(fill = FALSE)

  grid.arrange(p1, p2, ncol=1, top=curr)
}
```

Computing and plotting the mean value perhaps tells the same story a bit more simply.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
for (curr in sources) {
  phase_props_summary <- phase_props %>%
    filter(src == curr) %>%
    group_by(phase, prop) %>%
    summarise(ratio.mean = mean(ratio))

  p1 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=prop, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=phase), position='dodge') +
    ylab('Mean % of property in phase') +
    guides(fill = FALSE) +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  p2 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=prop, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=phase), position='stack') +
    labs(fill='') +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  p3 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=phase, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=prop), position='dodge') +
    labs(fill='') +
    ylab('Mean % of property in phase') +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  grid.arrange(p2, p3, ncol=2, top=curr)
}
```



# Valuation metrics
We employ the following financial metrics for cashflows:

- Non-capitalized value / Out-of-pocket value
- Risk-adjusted value (rV) / Expected value (EV)
- Capitalized value / Present value (PV)
- Risk-adjusted present value (rPV) / Expected present value (EPV)

Symmetrically, when cumulating cashflows, the above financial metrics are known as:

- Cumulative non-capitalized value / Cumulative out-of-pocket value
- Cumulative risk-adjusted value (Cumulative rV) / Cumulative expected value (Cumulative EV)
- Net present value (NPV)
- Risk-adjusted net present value (rNPV) / Expected net present value (ENPV)

We compute **Expected Value (EV)** as:

$$
EV_t = (R_t - C_t) * P_t
$$

where $R_t$ and $C_t$ are the revenues and costs (respectively) at time $t$, and $P_t$ the probability of reaching the cashflow from the point of evaluation.
The probability of reaching a given timestep $t_n$ from a point of evaluation $t_0$ is simply computed as: $P_t = \prod_{t_0}^{t^n}$.
Next, we compute **Present Value (PV)** as:

$$
\mathit{PV}_t = \frac{R_t - C_t}{(1 + i)^t}
$$

where $i$ is the discount rate of the evaluator, and $t$ is the time to the phase from the point of evaluation.
We compute **Expected Present Value (EPV)** as:

$$
\mathit{EPV}_t = \frac{R_t - C_t}{(1 + i)^t} * P_t
$$

Moving on to the cumulative valuations, we compute **Net Expected Value (ENV)**, **Net Present Value (NPV)**, and **Expected Net Present Value (ENPV)** as:
$\sum_{t\in T} \mathit{EV}_t$

$$
\mathit{ENV_T} = \sum_{t\in T} EV_t\\
\mathit{NPV_T} = \sum_{t\in T} \mathit{PV}_t\\
\mathit{ENPV_T} = \sum_{t\in T} \mathit{EPV}_t\\
$$

respectively, where $t$ is the time to the phase, and $T$ is the times to all timesteps of all phases for the project in evaluation.

<div class="alert alert-danger">
TODO: Verify that the probability portion of ENPV is actually computed accordingly!
</div>


# Valuation methods
To compute valuation metrics for a given project we must make an assumption as to how a hypothetical evaluator chooses to transform a sequence of discrete phases into a sequence of discrete and uncertain cashflows.
We make the assumption that the evaluator converts every phase into a series of years in order to properly discount the cashflow of the given phase.
This method yields slightly different results when compared to simply applying financial valuation metrics to the sequence of phases under the assumption that all cashflows for a given phase occur immediately upon entering that phase.s when compared to simply applying financial valuation metrics to the sequence of phases under the assumption that all cashflows for a given phase occur immediately upon entering that phase.
To elucidate the consequence of these differences we will first apply financial metrics directly to the phase-based data, then transform the original data to a year-based form and apply the same metrics, and then finally compare the two.


## Phase-based method
As phase durations are long, the time value of money not only greatly reduces the attractiveness of revenues, but also dampen the pain of costs.
If we disregard the fact that some phases (such as e.g. pre-clinical) are lengthy, and assume that the cashflows related to a phase all happen immediately upon entry into the phase, we can trivially compute Expected Value (EV), Present Value (PV), and Expected Present Value (EPV) of all phases from the perspective of all phases.
We can then cumulate these metrics in order to, respectively, compute cumulative EV, Net Present Value (NPV), and Expected Net Present Value (ENPV) from any phase to any phase.
We compute the above like this:

```{r}
phase_based_valuation <- function(phases) {
  result <- tibble()
  for (from in phase_levels) {
    rows <- phases %>%
      filter(phase >= from) %>%
      group_by(src, intervention, subject) %>%
      arrange(src, intervention, subject, phase) %>%
      mutate(from = factor(from, levels=phase_names, ordered=TRUE),
             time_to = cumsum(time) - time,
             cum_prob = cumprod(prob),
             prob_to = cum_prob / prob,
             # cashflows
             cashflow = sales + prizes - cost,
             ev = cashflow * prob_to,
             pv = cashflow / ((1 + discount_rate) ^ time_to),
             epv = pv * prob_to,
             # cumulatives
             cum = cumsum(cashflow),
             env = cumsum(ev),
             npv = cumsum(pv),
             enpv = cumsum(epv),
      )
      result <- bind_rows(result, rows)
  }
  result
}
```

```{r}
phases_from_phases <- phase_based_valuation(phases)
```

Let us first plot the valuation of each individual phase from the perspective of pre-clinical.

<div class="alert alert-danger">
TODO: In the plots below we're ignoring the value at the market since it dwarfs all other values as a consequence of being an immediate lump-sum. Split the market years into M1-M10 for both approaches so that we can plot the market here as well. This is very important as it gives a wildly incorrect valuation.
</div>

```{r, echo=FALSE}
sub <- filter(phases_from_phases, from == 'PC' & phase != 'MP')
p1 <- ggplot(sub, aes(phase, cashflow, color=src)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  ggtitle('Cashflow') +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_boxplot()
p2 <- ggplot(sub, aes(phase, ev, color=src)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  ggtitle('EV') +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_boxplot()
p3 <- ggplot(sub, aes(phase, pv, color=src)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  ggtitle('PV') +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_boxplot()
p4 <- ggplot(sub, aes(phase, epv, color=src)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  ggtitle('EPV') +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_boxplot()
ggarrange(p1, p2, p3, p4, ncol=2, nrow=2, common.legend=TRUE, legend='right')
```

Let us now plot the cumulative value at each individual phase using pre-clinical as the starting point.

<div class="alert alert-danger">
TODO: In the plots below we're ignoring the value at the market since it dwarfs all other values as a consequence of being an immediate lump-sum. Split the market years into M1-M10 for both approaches so that we can plot the market here as well. This is very important as it gives a wildly incorrect valuation.
</div>

```{r, echo=FALSE}
sub <- filter(phases_from_phases, from == 'PC' & phase != 'MP')
p1 <- ggplot(sub, aes(phase, cum, color=src)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  ggtitle('Cumulative cashflow') +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_boxplot()
p2 <- ggplot(sub, aes(phase, env, color=src)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  ggtitle('ENV') +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_boxplot()
p3 <- ggplot(sub, aes(phase, npv, color=src)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  ggtitle('NPV') +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_boxplot()
p4 <- ggplot(sub, aes(phase, enpv, color=src)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  ggtitle('ENPV') +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_boxplot()
ggarrange(p1, p2, p3, p4, ncol=2, nrow=2, common.legend=TRUE, legend='right')
```

Another way to look at the data is to consider how starting at different phases alter the value of the project.
We simply compute it as follows:

```{r}
# TODO: Make sure this is a correct calculation!
final_value_phasewise <- phases_from_phases %>%
  group_by(src, subject, from) %>%
  filter(phase == max(phase)) %>%
  summarise(cum = tail(cum, n=1),
            env = tail(env, n=1),
            npv = tail(npv, n=1),
            enpv = tail(enpv, n=1))

final_value_phasewise_long <- final_value_phasewise %>%
  gather('valuation', 'value', cum, env, npv, enpv)

final_value_phasewise_long_summary <- final_value_phasewise_long %>%
  group_by(src, from, valuation) %>%
  summarize(median = median(value),
            mean = mean(value),
            sd   = sd(value),
            min  = min(value),
            max  = max(value))
```

We begin by plotting the mean value of bringing the project to completion from whatever starting phase we're currently considering.
Using the four metrics previously applied.
The vertical lines delimit +/- 1 standard deviation from the sample mean (i.e. ~68% of the data).

```{r, echo=FALSE}
pos <- position_dodge(0.2)
print(final_value_phasewise_long_summary %>%
      ggplot(aes(from, mean, color=valuation, group=valuation)) +
      geom_linerange(aes(ymin=mean-sd, ymax=mean+sd), position=pos, size=1) +
      geom_line(position=pos, size=1, linetype='dotted') +
      geom_point(position=pos, size=2) +
      facet_wrap(. ~ src, ncol=2) +
      scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
      xlab(element_blank()) + ylab(element_blank()))

# ALTERNATIVE:
# final_value_phasewise_long %>%
#   filter(from != 'MP' & src == curr) %>%
#   ggplot(aes(from, value, fill=valuation)) +
#   geom_boxplot()
```

The table below summarizes the valuation metrics from the start of pre-clinical.

```{r, echo=FALSE}
sub <- final_value_phasewise_long_summary %>%
  ungroup %>%
  filter(from=='PC') %>%
  select(-from)
knitr::kable(sub)
```

## Year-based method
To consider the fact that all cashflows of a phase do not occur immediately upon phase-entry we convert our phase-based data to year-based data and make assumptions as to how the values are interpolated over the years of a phase.

We assume that all properties are constantly distributed over the time of the phase except for revenues.
We assume that the revenues of year 0 is 0 and then linearly increase until the area under the curve is equal to the total revenues of the phase.
The first year will thus be non-zero if the total revenues of the project is non-zero.

We use the following function to convert from phase-form to year-form:

```{r, results='hide'}
#' Converts a data set in phase-form to year-form.
#'
#' @param phases Dataframe in phase-form.
#' @return Dataframe in year-form
to_years <- function (phases) {

  # Convert phases to ordered factor
  phases$phase <- factor(phases$phase, levels=phase_levels, ordered=TRUE)

  # Add timestamps to every observation
  phases <- phases %>%
    group_by(src, intervention, subject) %>%
    arrange(src, intervention, subject, phase) %>%
    mutate(t = cumsum(time) - time)

  # Compute cost steps
  phases$cost_step <- (phases$cost / phases$time)
  phases$cost_remainder <- phases$cost - phases$cost_step * floor(phases$time)

  # Compute prob steps
  phases$prob_step <- phases$prob ^ (1 / phases$time)
  phases$prob_remainder <- phases$prob / (phases$prob_step ^ floor(phases$time))

  # Compute sales slopes
  phases$sales_slope <- (phases$sales * 2 / (phases$time + 1)) / phases$time

  # Transform: To cashflows over time (phase yearly)
  phase_years <- tibble()
  for (x in 1:ceiling(max(phases$time) + 1)) {

    # Compute step-based properties
    whole_step <- x <= phases$time
    cost <- ifelse(whole_step, phases$cost_step, phases$cost_remainder)
    sales <- ifelse(whole_step, phases$sales_slope * x, 0)
    prob <- ifelse(whole_step, phases$prob_step, phases$prob_remainder)
    prizes <- if (x == 1) phases$prizes else 0 # Immediate lump-sum

    # Make tibble
    has_decimals <- phases$time - floor(phases$time) != 0
    part_of_phase <- x <= phases$time | (x <= phases$time + 1 & has_decimals)
    year <- tibble(part_of_phase,
                   src = phases$src,
                   intervention = phases$intervention,
                   subject = phases$subject,
                   phase_year = x,
                   t = phases$t + x - 1,
                   phase = phases$phase,
                   cost,
                   sales,
                   prizes,
                   prob,
                   time = phases$time,
                   discount_rate = phases$discount_rate
                   ) %>%
    filter(x <= time | (x <= time + 1 & has_decimals)) %>% # Only years in phase
    select(-c('part_of_phase')) # Remove temporary column

    # Append
    phase_years <- bind_rows(phase_years, year)
  }

  phase_years
}
```

Using this function we can now convert our phase-based data to year-based data.
It should be noted that the function does not distribute the phase-based data over a series of equidistant years.
Data points are only equidistant within a phase, but not necessarily across.
In other words, if P1 entry would occur after 5.3 years then we will distribute PC properties over the 6 first years.
If the duration of P1 is 2.5 years then P2 would start at year 5.3 and end at year 7.8.
While we would divide P1 into three equidistant (years) points (years), that start from year 5.3, 6.3, and 7.3 (respectively) they are not equidistant from the PC steps.

```{r}
years <- to_years(phases)
```

Before we start introducing interventions, let us first do some descriptive statistics of our year-based datasets.

Let us begin by looking at the cashflows using a few different valuation techniques.
From top-left to bottom-right: out-of-pocket cashflows, risk-adjusted/expected value (EV), capitalized/present value (PV), risk-adjusted/expected capitalized/present value (EPV).
The middle line in each ribbon tracks the mean value, while the edges capture all values within 2 standard deviations of the mean (meaning 95% of the data).

```{r, include=FALSE}
# TODO: Maybe this code should be included? At least explained!
# Transform: Phase years from different phases
years_from_phases <- tibble()
for (from in phase_levels) {
  pyfp <- years %>%
    filter(phase >= from) %>%
    group_by(src, subject, intervention) %>%
    arrange(t) %>%
    mutate(from          = factor(from, levels=phase_levels, ordered=TRUE),
           time_to       = t - min(t),
           cashflow      = sales + prizes - cost,
           revenue       = sales + prizes,
           cum_cost      = cumsum(cost),
           cum_revenue   = cumsum(revenue),
           cum  = cum_revenue - cum_cost,
           rem_prob      = prod(prob) / cumprod(prob) * prob,
           cum_prob      = cumprod(prob),
           prob_to       = cum_prob / prob,
           # cost
           cost_ev         = cost * prob_to, # TODO:ok?
           cost_env     = cumsum(cost_ev),
           cost_pv         = cost / ((1 + discount_rate) ^ time_to),
           cost_epv        = (cost / ((1 + discount_rate) ^ time_to)) * prob_to,
           cost_npv        = cumsum(cost_pv),
           cost_enpv       = cumsum(cost_epv), # TODO:ok?
           # revenue
           revenue_ev      = revenue * prob_to, # TODO:ok?
           revenue_env  = cumsum(revenue_ev),
           revenue_pv      = revenue / ((1 + discount_rate) ^ time_to),
           revenue_epv     = (revenue / ((1 + discount_rate) ^ time_to)) * prob_to,
           revenue_npv     = cumsum(revenue_pv),
           revenue_enpv    = cumsum(revenue_epv), # TODO:ok?
           # cashflow
           ev     = revenue_ev - cost_ev,
           env = revenue_env - cost_env,
           pv     = revenue_pv - cost_pv,
           epv    = revenue_epv - cost_epv,
           npv    = revenue_npv - cost_npv,
           enpv   = revenue_enpv - cost_enpv,
           # year
           year            = floor(time_to)
           )
  years_from_phases <- bind_rows(years_from_phases, pyfp)
}

# Long
years_from_phases_long <- years_from_phases %>%
  select(src, intervention, subject, from, year,
         cashflow, ev, pv, epv,
         cum, env, npv, enpv) %>%
  gather('valuation', 'value', -src, -intervention, -subject, -from, -year) %>%
  transform(valuation = factor(valuation, levels = c(metrics, cum_metrics)))

# Long summary
years_from_phases_long_sum <- years_from_phases_long %>%
  group_by(src, from, year, valuation) %>%
  mutate(mu  = mean(value),
         med = median(value),
         std = sd(value))

# Final long
final_year_from_phases_long <- years_from_phases_long %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, subject, from, valuation) %>%
  arrange(year) %>%
  summarize(value = tail(value, n=1))

# Final long summary
final_year_from_phases_long_sum <- years_from_phases_long_sum %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, from, valuation) %>%
  arrange(year) %>%
  summarize(mu  = tail(mu, n=1),
            med = tail(med, n=1),
            std = tail(std, n=1))
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
years_from_phases_long_sum %>%
  filter(from == 'PC') %>%
  filter(valuation %in% metrics) %>%
  ggplot(aes(year, med)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_ribbon(aes(x=year, ymin=(mu-std*2), ymax=(mu+std*2), fill=src), alpha=.4) +
  geom_line(aes(color=src)) +
  facet_wrap(. ~ valuation, scale='free_y', ncol=2)
```

If we cumulate these values over time, we get the cumulative versions of these metrics.
These can be thought of as the value of running the project for x years from the start.
From top-left to bottom right: cumulative out-of-pocket cashflows, cumulative risk-adjusted/expected value (EV), capitalized/net present value (NPV), and risk-adjusted/expected capitalized/net present value (ENPV).

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
years_from_phases_long_sum %>%
  filter(from == 'PC') %>%
  filter(valuation %in% cum_metrics) %>%
  ggplot(aes(year, med)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_ribbon(aes(x=year, ymin=(mu-std*2), ymax=(mu+std*2), fill=src), alpha=.4) +
  geom_line(aes(color=src)) +
  facet_wrap(. ~ valuation, scale='free_y', ncol=2)
```

Another way to look at the data is to consider how starting at different phases alter the value of the project.
We begin by plotting the mean value of bringing the project to completion from whatever starting phase we're currently considering.
Using the four metrics previously applied.
The vertical lines delimit +/- standard deviations from the sample mean (i.e. ~68% of the data).

```{r, echo=FALSE}
pos <- position_dodge(0.4)
final_year_from_phases_long_sum %>%
  ggplot(aes(from, mu, color=valuation, group=valuation)) +
  geom_linerange(aes(ymin=mu-std, ymax=mu+std), position=pos, size=1) +
  geom_line(position=pos, size=1, linetype='dotted') +
  geom_point(position=pos, size=2)  +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  facet_wrap(. ~ src, ncol=2) +
  xlab(element_blank()) + ylab(element_blank())
```

The table below summarizes the valuation metrics from the start of pre-clinical.

```{r, echo=FALSE}
sub <- final_year_from_phases_long_sum %>%
  ungroup %>%
  filter(from=='PC') %>%
  arrange(valuation)
knitr::kable(sub)
```

## Comparing methods
Let us compare the two methods by looking at the cumulative valuations from different starting points.
In the first two plots we observe that the simple valuation metrics cumulative cashflows and ENV, does not significantly differ between the two methods in any of the datasets.
In fact, the cumulative metric should yield exactly the same results in both methods as neither discounting nor probability is taken into consideration.
When computing ENV however, we take probability into account, and as a consequence, the outcome could in theory be different, but in practice, i.e. in the plot below, we observe that the difference is insignificant.

```{r, echo=FALSE, warning=FALSE}
pwise <- final_value_phasewise_long
ywise <- final_year_from_phases_long
methods <- c('phase', 'year')
pwise$method <- factor('phase')
ywise$method <- factor('year')
both <- bind_rows(pwise, ywise)
# TODO: Yields warning cuz coercing src to character vector (or similar)
# TODO: Use the phrases valuation metric and valuation method to differentiate
# whether we're talking about NPV/ENPV or phase-based/year-based.
```

```{r, echo=FALSE}
p <- both %>%
  filter(valuation %in% c('cum', 'env')) %>%
  ungroup %>%
  mutate(src = recode(src,
                      'DRIVE-AB (2018)' = 'drive-ab',
                      'Sertkaya et. al (2014)' = 'sertkaya')) %>%
  ggplot(aes(src, value, color=method, fill=method)) +
  geom_boxplot(alpha=0.3) +
  facet_wrap(valuation ~ from, scales='free', ncol=6) +
  ylab(element_blank()) +
  xlab(element_blank()) +
  theme(axis.text.x = element_text(angle=90))
print(p)
```

Taking discounting into consideration however, makes the two methods yield slightly different results.
The following two plots illustrate that difference for the metrics NPV and ENPV.
Again, in all datasets, from multiple starting phases.


```{r, echo=FALSE}
for (curr in c('npv', 'enpv')) {
  p <- both %>%
    filter(valuation == curr) %>%
    ungroup %>%
    mutate(src = recode(src,
                        'DRIVE-AB (2018)' = 'drive-ab',
                        'Sertkaya et. al (2014)' = 'sertkaya')) %>%
    ggplot(aes(src, value, color=method, fill=method)) +
    geom_boxplot(alpha=0.3) +
    facet_wrap(valuation ~ from, scales='free', ncol=6) +
    ylab(element_blank()) +
    xlab(element_blank()) +
    theme(axis.text.x = element_text(angle=90))
  print(p)
}
```


# Interventions
We will now introduce five interventions (treatments) to our base data sets and later analyze their effects on the valuation metrics.
We model these interventions as qualitatively (categorically) different, as they operate on different R&D phases, but theoretically they could be considered the same intervention that is quantitatively (numerically) different in terms of their actuation time.
The intervention can be described as a non-dilutive and unconditional prize.
This intervention can be considered a generalization of what is commonly referred to as either a market entry reward or a phase entry reward, where we've generalized the time of actuation.
We implement the intervention as a function `intervene` as follows:

```{r}
log10_sample <- function (min, max, magnitude_min, magnitude_max) {
  runif(N, min, max) * (10 ^ runif(N, magnitude_min, magnitude_max))
}

intervene <- function (phases, target_phase, intervention_name) {
  data.frame(phases) %>%
    mutate(intervention = intervention_name,
           prizes = ifelse(phase == target_phase,
                           log10_sample(1, 9, 1, 3),
                           prizes))
}
```

We can then treat copies of the original phase-based data set and merge it into one larger data set, like this:

```{r}
intervened <- rbind(phases,
  intervene(phases, 'P1', 'P1ER'),
  intervene(phases, 'P2', 'P2ER'),
  intervene(phases, 'P3', 'P3ER'),
  intervene(phases, 'P4', 'P4ER'),
  intervene(phases, 'MP', 'PDMER'))
```

Note that we are logarithmically sampling prize sizes.
This is because we assume that the absolute difference in effect will be much smaller when prize sizes are very small, as compared to when prize sizes are very large, and hence need more samples at the "bottom" to properly saturate the space.
In the analysis we at all times "control for" prizes which means that the chosen distribution does not affect any of the conclusions beyond sample saturation.
The sampled prizes are summarized in the histogram below.

```{r, echo=FALSE, message=FALSE}
print(intervened %>%
      filter(prizes > 0) %>%
      ggplot(aes(prizes, fill=phase)) +
      geom_histogram() +
      theme(axis.title.y=element_blank()) +
      facet_wrap(interaction(intervention, phase)~., ncol=3) +
      guides(fill = FALSE))
```


# Analysis

The question we want to explore is whether it is cheaper for the benefactor to directly or indirectly fund antibiotics R&D.
Directly here refers to the idea of simply paying for development "at cost".
Indirectly here refers to the idea of issuing prizes that encourage private developers to undertake a given activity with the hope of winning said prize.

We will compare these two different approaches by comparing their expected costs when facing a hypothetical project entering pre-clinical.
We assume that the valuation metric that private actors employ is ENPV and that the valuation method is yearly.
We will refer to this value as *private ENPV.*

```{r, echo=FALSE}
public_discount_rate_min = 0.035
public_discount_rate_max = 0.045
```

In order to reason about the cost for the benefactor of issuing a prize we will compute the necessarily negative ENPV of issuing said prize.
ENPV is an appropriate measure, since the prize is probabilistically issued in the future.
We assume that the benefactor is the public sector and use a uniformly distributed discount rate between `r public_discount_rate_min * 100`% and `r public_discount_rate_max * 100`%.
We refer to the expected cost of a prize (i.e. an intervention) as *indirect ENPV*.
This can be thought of as the expected cost for the benefactor.

If indirect ENPV is the cost of the intervention for the public, then we must also compute the expected cost of simply paying for the project in question at cost.
To take both cost of capital and project risk into consideration, we will again compute the negative ENPV.
The probabilistic cashflows used in the ENPV calculation in this case is the project's cost without any expectation of revenues.
We assume that the public payer is the same as the benefactor and hence employ discount rates from the same distribution
(`r public_discount_rate_min*100`% - `r public_discount_rate_max*100`%).
We refer to the expected cost of paying for the project at cost as *direct ENPV*.


```{r, echo=FALSE}
valuations <-
  to_years(intervened) %>%
  rename(discount_rate_priv = discount_rate) %>%

  # Add public discount rate
  group_by(subject) %>%
  mutate(discount_rate_publ = runif(1, min=0.035, max=0.045)) %>%
  ungroup %>%

  arrange(t) %>%
  group_by(src, subject, intervention) %>%
  mutate(
         intervention_size = sum(prizes),
         cashflow  = sales + prizes - cost,
         prob_to   = cumprod(prob) / prob, # TODO: Is this correct?
         enpv_priv = cumsum((cashflow / ((1 + discount_rate_priv) ^ t)) * prob_to),
         enpv_publ = cumsum((-cost / ((1 + discount_rate_publ) ^ t)) * prob_to),
         enpv_benf = cumsum((-prizes / ((1 + discount_rate_publ) ^ t)) * prob_to),
         ) %>%
  select(-prob_to)

finals <- valuations %>%
  arrange(t) %>%
  group_by(src, subject, intervention) %>%
  summarise(
         prizes    = sum(prizes),
         enpv_priv = tail(enpv_priv, n=1),
         enpv_publ = tail(enpv_publ, n=1),
         enpv_benf = tail(enpv_benf, n=1)
         )
```

```{r, include=FALSE}
benf_higher <- filter(finals, enpv_benf > enpv_publ)
```


## Prizes and private ENPV
Before we proceed further, let us first ensure that the issued prizes (i.e. the interventions) actually do have an effect on private ENPV.
As such there should be a correlation between prize size and private ENPV in all data sets.

```{r, echo=FALSE}
print(finals %>%
  ggplot(aes(prizes, enpv_priv, color=src)) +
  geom_point(alpha=1, shape=1) +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ intervention, ncol=3, scale='free') +
  xlab('Prize') +
  ylab('Private ENPV') +
  theme(legend.position = 'top')
)
```

<div class="alert alert-danger">
TODO: Compute ENPV improvement since it will always be >= 0 and then plot that on log-y-log-x. Actually, it should even be > 0 and if it is not I should explain why.
</div>

Intervention prize and private ENPV, expectedly, seem correlated.
As other studies have shown, the later a prize is awarded, the higher size needs to be to achieve a comparative increase in ENPV.
This phenomena is clearly visible in our model when plotting the correlation between prize size and private ENPV for all interventions on the same scale.

```{r, echo=FALSE}
print(finals %>%
  filter(intervention != 'NONE') %>%
  ggplot(aes(prizes, enpv_priv, color=intervention)) +
  geom_smooth(method='lm', se=FALSE) +
  geom_point(alpha=0.5, shape=1) +
  facet_wrap(. ~ src, ncol=3) +
  xlab('Prize') +
  theme(legend.position = 'top') +
  ylab('Private ENPV')
)
```

<div class="alert alert-danger">
TODO: Following the last todo-note. The above can then be exchanged for an improvement plot.
</div>


## Prizes and indirect ENPV
In order to gauge the actual cost of how much a benefactor will have to pay when issuing a particular prize, we choose to compute ENPV.
This metric takes both the opportunity cost (cost of capital) and risk of a project into consideration.
Taking opportunity cost into consideration is important as the benefactor looses the opportunity to activate their money elsewhere when making a payment to a beneficiary.
Taking risk into consideration is important as the benefactor will not necessarily pay the promised prize size, due to projects being highly risky.
We will now explore the correlation between prize size and the negative ENPV for the benefactor.
Note that ENPV for the benefactor necessarily will be negative as the prize payout is the only cashflow considered.

```{r, echo=FALSE}
finals %>%
  ggplot(aes(prizes, -enpv_benf, color=intervention)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ src, ncol=3) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:3)),
                     minor_breaks = c(1:9 %o% 10^(0:3))) +
  xlab('Prize (log)') + ylab('-Indirect ENPV')
```

<div class="alert alert-danger">
TODO: Change above to improvement in ENPV.
</div>

## Indirect ENPV and Private ENPV
How much it costs the benefactor compared to how much it benefits the beneficiary.

```{r, echo=FALSE}
ggplot(finals, aes(y=enpv_priv, x=-enpv_benf, color=intervention)) +
  geom_point() +
  facet_wrap(. ~ src, ncol=3) +
  geom_smooth(method='lm', se=FALSE) +
  coord_cartesian(xlim = c(0, 500), ylim = c(-25, 200)) +
  ylab('Private ENPV') + xlab('-Indirect ENPV')
```

## Prizes and go-decisions
Assuming that a go-decision is reached in pre-clinical whenever private ENPV is greater than or equal to 0, we can use logistic regression to predict whether a particular prize size will yield a go or no-go decision.
We first compute go-decisions:
```{r}
finals$go <- factor(finals$enpv_priv >= 0, levels=c(TRUE, FALSE))
```

then construct the model and use it to predict values as follows:
```{r}
prize_model <- glm(data = finals,
             go ~ prizes * interaction(src, intervention),
             family = binomial(link='logit'))
finals$pred_go_prize <- predict(prize_model, type='response')
```

Finally, we can plot the predicted decision against different prize sizes.
```{r, echo=FALSE}
finals %>%
  ggplot(aes(prizes, (1-pred_go_prize)*100, color=intervention)) +
  geom_line() +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
      xlab('Prize (log)') + ylab('Probability of go-decision') +
      facet_wrap(. ~ src, ncol=3)
```


## Indirect ENPV and go-decisions
In the same fashion, we can construct a model that predicts go-decisions from the benefactor's ENPV.

```{r}
benf_model <- glm(data=finals,
             go ~ enpv_benf * interaction(src, intervention),
             family=binomial(link='logit'))
finals$pred_go_benf <- predict(benf_model, type='response')
```

This can help us understand how much the benefactor will have to spend in order to achieve a particular go-decision probability.
```{r, echo=FALSE}
finals %>%
  ggplot(aes(-enpv_benf, (1-pred_go_benf)*100, color=intervention)) +
  geom_line() +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
      xlab('-Indirect ENPV (log)') + ylab('Probability of go-decision') +
      facet_wrap(. ~ src, ncol=3)
```

# Conclusion
