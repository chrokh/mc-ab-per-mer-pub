---
title: Time for public pharma?
subtitle: Supplementary material
author: Christopher Okhravi
date: 2019
output:
  html_document:
    toc: true
    #toc_float:
    #  collapsed: false
    theme: cosmo
  pdf_document:
    toc: true
    latex_engine: xelatex
---

<!--
# TODO: Something I haven't taken into consideration is that one can assume that a prize has a fixed number of max-recipients but still assume that all developers will believe that they will be the ones who get it. This should significantly reduce the cost per output antibiotic.
# TODO: I should make all Sertkaya data separate data sets. I.e. use all the different indications.
-->

```{r, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, collapse=TRUE)
set.seed(1)
```

```{r, libraries, include=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(triangle)
```

# Introduction
This document serves as supplementary material for the paper "Time for public pharma?".
This is a monte carlo simulation that explores direct versus indirect funding of antibiotics research and development (R&D).
Direct funding is here used to mean that a benefactor pays for antibiotics R&D at cost.
Indirect funding is here used to mean that a benefactor issues non-dilutive prizes to whoever completes a particular phase, with the intent of incentivizing private developers to undertake said and prior phases.
This analysis is attempting to estimate which of the two options would be cheaper, ceteris paribus, for the benefactor.

# Simulation and input data set

Let us begin by setting `N` to the number of samples that we want per data set.
This number represents the number of random samples that we'll draw from a stochastic representation of a hypothetical antibiotic R&D project.
The distribution of the stochastic project depends on the data set we're using, and in a simple attempt to avoid drawing data set specific conclusions, we'll look at multiple data sets.
Meaning that if one considers all the data sets at once then the number of random projects drawn is `N` multiplied with the number of data sets.

```{r}
N = 5000
```

## Sertkaya et. al (2014)

The following is an approximation of the data used by Sertkaya et. al (2014) as they make a few assumptions that are not reconcilable with the way we have chosen to model antibiotics R&D.
The points of differentiation are outlined as comments in the code chunk below.

<div class="alert alert-danger">
**Sertkaya uses triangular distributions, but we have used uniform distributions. This must be changed.**
</div>

```{r}
pc <- data.frame(
  phase = 'PC',
  time  = runif(N, min=4.3, max=6),
  cost  = runif(N, min=19, max=23.2),
  prob  = runif(N, min=0.175, max=0.69),
  sales = 0)
p1 <- data.frame(
  phase = 'P1',
  time  = runif(N, min=0.75, max=1.8),
  prob  = runif(N, min=0.25, max=0.837),
  cost  = runif(N, min=7.3, max=12),
  sales = 0)
p2 <- data.frame(
  phase = 'P2',
  time  = runif(N, min=0.75, max=2.5),
  cost  = runif(N, min=7.12, max=18.72),
  prob  = runif(N, min=0.34, max=0.74),
  sales = 0)
p3 <- data.frame(
  phase = 'P3',
  cost  = runif(N, min=26.88, max=121.68),
  prob  = runif(N, min=0.314, max=0.786),
  time  = runif(N, min=0.83, max=3.9),
  sales = 0)
p4 <- data.frame(
  phase = 'P4',
  time  = runif(N, min=0.5, max=1.04),
  prob  = runif(N, min=0.83, max=0.99),
  cost  = 98.297168,
  sales = 0)
mp <- data.frame(
  phase = 'MP',
  time  = 10,
  prob  = 1,
  cost  = 0,
  sales = runif(N, min=218, max=2500))

# Combine all phases into single dataset
sertkaya2014 <- rbind(pc, p1, p2, p3, p4, mp)
sertkaya2014$subject <- 1:N

# Discount rate is the same across all phases
sertkaya2014$discount_rate_priv <- runif(N, 0.09, 0.24)

# Set source name
sertkaya2014$src <- 'Sertkaya et. al (2014)'
```

## DRIVE-AB (2018)
This is an approximation of the data used in DRIVE-AB final report (2018).
Some deviations (reported as comments in the code chunk below) have been made as some of the DRIVE-AB assumptions are not compatible with our assumptions.

```{r}
pc <- data.frame(
  phase = 'PC',
  time  = rtriangle(N, 4.33, 6, 5.5),
  cost  = rtriangle(N, 14.25, 29, 21.1),
  prob  = rtriangle(N, 0.175, 0.69, 0.352),
  sales = 0)
p1 <- data.frame(
  phase = 'P1',
  time  = rtriangle(N, 0.75, 1.8, 0.875),
  prob  = rtriangle(N, 0.25, 0.837, 0.33),
  cost  = rtriangle(N, 13.1, 37.96, 24),
  sales = 0)
p2 <- data.frame(
  phase = 'P2',
  time  = rtriangle(N, 0.75, 2.5, 1.08),
  prob  = rtriangle(N, 0.34, 0.74, 0.5),
  cost  = rtriangle(N, 4.55, 46.36, 12.95), # TODO: Cost is incorrectly reported in DRIVE-AB report!
  sales = 0)
p3 <- data.frame(
  phase = 'P3',
  time  = rtriangle(N, 0.83, 3.83, 1.82),
  prob  = rtriangle(N, 0.314, 0.786, 0.67),
  cost  = rtriangle(N, 10, 47, 21.8),
  sales = 0)
p4 <- data.frame(
  phase = 'P4',
  time  = rtriangle(N, 0.5, 1.04, 0.75),
  prob  = rtriangle(N, 0.83, 0.99, 0.85),
  cost  = rtriangle(N, 55.5, 127.91, 88.35),
  sales = 0)
mp <- data.frame(
  phase = 'MP',
  time  = 10,
  prob  = 1,
  cost  = 0,
  sales = rtriangle(N, 0, 4336.00, 2559.5)) # TODO: Does not model DRIVE-AB report correctly as the report does not assume that y1 has 0 sales.

# Combine all phases into single dataset
driveab2018 <- rbind(pc, p1, p2, p3, p4, mp)
driveab2018$subject <- 1:N

# Discount rate is the same across all phases
driveab2018$discount_rate_priv <- runif(N, 0.05, 0.30)

# Set source name
driveab2018$src <- 'DRIVE-AB (2018)'
```


## Combining all datasets
Before proceeding, we'll combine all datasets (sources) into a single dataset containing all sources.

```{r}
phases <- rbind(sertkaya2014, driveab2018)
```

```{r, include=FALSE}
# To avoid recomputing all the time we'll also store the names of the different sources:
sources <- unique(phases$src)
# To avoid misspellings of factors, lets' use levels:
metrics <- c('cashflow', 'ev', 'pv', 'epv')
cum_metrics <- c('cum', 'env', 'npv', 'enpv')
```

We must also convert `phase` into an ordered factor so that we can assume that `PC < P1 < P2 < P3 < P4 < MP`.

```{r}
phase_names <- c('PC','P1','P2','P3','P4','MP')
phase_levels <- factor(phase_names, levels=phase_names, ordered=TRUE)
phases$phase <- factor(phases$phase, levels=phase_names, ordered=TRUE)
```

Later we will apply interventions to this dataset and thereby create multiple permutations/versions of every observation.
This means that we must keep track of which intervention we're currently looking at, and as such we'll add that column immediately, and declare all the current data as suffering from "no intervention".
In terms of a randomized controlled trial, this is the "control group".
When we apply different interventions to the control group we will thus create different "treatment groups".
As the interventions are applied to the control group rather than to completely new samples, we are able to perform "within subject analysis" rather than having to resort to "across subject analysis".

```{r}
phases$prizes <- 0
phases$intervention <- 'NONE'
```

Finally, let us add a "public discount rate".
Meaning the cost of capital for the benefactor, i.e. for the body that pays the intervention with no expectation of monetary return.
This parameter is used in the analysis, where its raison d'etre is also further explained.

```{r}
publ_dr_min = 0.035
publ_dr_max = 0.045
```

We assume that the benefactor is the public sector and use a uniformly distributed discount rate between `r publ_dr_min * 100`% and `r publ_dr_max * 100`%.
Public discount rate varies across subjects but not within.
The same subject has the same public discount rate across all datasets (i.e. sources).

```{r}
phases$discount_rate_publ <- runif(N, publ_dr_min, publ_dr_max)
```

## Summary statistics
Let us plot the phase parameters as frequency polygons (i.e. histograms) for each of the sources.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
p1 <- ggplot(phases, aes(cost, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab('Cost (million USD)')
p2 <- ggplot(phases, aes(time, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab('Duration (months)')
p3 <- ggplot(phases, aes(prob*100, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab('Probability (%)')
p4 <- ggplot(phases, aes(sales, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  xlab('Sales (million USD)')
p5 <- ggplot(phases, aes(discount_rate_priv*100, color=src, fill=src)) +
  geom_freqpoly() +
  facet_grid(phase ~ ., scale='free_y') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position='none') +
  xlab('Discount rate (%)')
ggarrange(p1, p2, p3, p4, p5, ncol=3, nrow=2, common.legend=TRUE, legend='bottom')
```

Interestingly, some parameters are quite dispersly distributed between phases.
Consider e.g. how almost all of the cost is incurred in some phase(s) while all the time is spent in another.
The two following figures plot what percentage of the total of some property is spent in a given phase, per project.
The first plot is grouped by property, while the second by phase.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Transform: phase properties to long from wide
phase_props <- phases %>%
  filter(phase != 'MP') %>%
  select(src, subject, phase, cost, time, prob) %>%
  gather(key='prop', value='value', -src, -subject, -phase) %>%
  group_by(src, subject, prop) %>%
  mutate(total = sum(value),
         ratio = value / total) # NOTE: will cause NaN if 0/0

for (curr in sources) {
  p1 <- phase_props %>%
    filter(src == curr) %>%
    filter(!is.na(ratio)) %>%
    ggplot(aes(x=phase,y=ratio*100, fill=phase)) +
    geom_violin(draw_quantiles=c(0.25, 0.5, 0.75)) +
    facet_grid(~ prop) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          axis.ticks.x=element_blank()) +
      ylab('% of property in phase') + xlab('') +
      guides(fill = FALSE)

  p2 <- phase_props %>%
    filter(src == curr) %>%
    ggplot(aes(x=prop,y=ratio*100, fill=prop)) +
    geom_violin(draw_quantiles=c(0.25, 0.5, 0.75)) +
    facet_grid(~ phase) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          axis.ticks.x=element_blank()) +
        ylab('% of property in phase') + xlab('') +
        guides(fill = FALSE)

  grid.arrange(p1, p2, ncol=1, top=curr)
}
```

Computing and plotting the mean value perhaps tells the same story a bit more simply.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
for (curr in sources) {
  phase_props_summary <- phase_props %>%
    filter(src == curr) %>%
    group_by(phase, prop) %>%
    summarise(ratio.mean = mean(ratio))

  p1 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=prop, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=phase), position='dodge') +
    ylab('Mean % of property in phase') +
    guides(fill = FALSE) +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  p2 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=prop, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=phase), position='stack') +
    labs(fill='') +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  p3 <- ggplot(filter(phase_props_summary, is.finite(ratio.mean)), aes(x=phase, y=ratio.mean * 100)) +
    geom_bar(stat='identity', aes(fill=prop), position='dodge') +
    labs(fill='') +
    ylab('Mean % of property in phase') +
    theme(axis.title.y=element_blank(), axis.title.x=element_blank())

  grid.arrange(p2, p3, ncol=2, top=curr)
}
```



# Valuation metrics
We employ the following financial metrics for cashflows:

- Non-capitalized value / Out-of-pocket value
- Risk-adjusted value (rV) / Expected value (EV)
- Capitalized value / Present value (PV)
- Risk-adjusted present value (rPV) / Expected present value (EPV)

Symmetrically, when cumulating cashflows, the above financial metrics are known as:

- Cumulative non-capitalized value / Cumulative out-of-pocket value
- Cumulative risk-adjusted value (Cumulative rV) / Cumulative expected value (Cumulative EV)
- Net present value (NPV)
- Risk-adjusted net present value (rNPV) / Expected net present value (ENPV)

We compute **Expected Value (EV)** as:

$$
EV_t = (R_t - C_t) * P_t
$$

where $R_t$ and $C_t$ are the revenues and costs (respectively) at time $t$, and $P_t$ the probability of reaching the cashflow from the point of evaluation.
The probability of reaching a given timestep $t_n$ from a point of evaluation $t_0$ is simply computed as: $P_t = \prod_{t_0}^{t^n}$.
Next, we compute **Present Value (PV)** as:

$$
\mathit{PV}_t = \frac{R_t - C_t}{(1 + i)^t}
$$

where $i$ is the discount rate of the evaluator, and $t$ is the time to the phase from the point of evaluation.
We compute **Expected Present Value (EPV)** as:

$$
\mathit{EPV}_t = \frac{R_t - C_t}{(1 + i)^t} * P_t
$$

Moving on to the cumulative valuations, we compute **Net Expected Value (ENV)**, **Net Present Value (NPV)**, and **Expected Net Present Value (ENPV)** as:
$\sum_{t\in T} \mathit{EV}_t$

$$
\mathit{ENV_T} = \sum_{t\in T} EV_t\\
$$

$$
\mathit{NPV_T} = \sum_{t\in T} \mathit{PV}_t\\
$$

$$
\mathit{ENPV_T} = \sum_{t\in T} \mathit{EPV}_t\\
$$

respectively, where $t$ is the time to the phase, and $T$ is the times to all timesteps of all phases for the project in evaluation.

<div class="alert alert-danger">
TODO: Verify that the probability portion of ENPV is actually computed accordingly!
</div>


# Valuation methods
To compute valuation metrics for a given project we must make an assumption as to how a hypothetical evaluator chooses to transform a sequence of discrete phases into a sequence of discrete and uncertain cashflows.
We make the assumption that the evaluator converts every phase into a series of years in order to properly discount the cashflow of the given phase.
This method yields slightly different results when compared to simply applying financial valuation metrics to the sequence of phases under the assumption that all cashflows for a given phase occur immediately upon entering that phase.s when compared to simply applying financial valuation metrics to the sequence of phases under the assumption that all cashflows for a given phase occur immediately upon entering that phase.
To elucidate the consequence of these differences we will first apply financial metrics directly to the phase-based data, then transform the original data to a year-based form and apply the same metrics, and then finally compare the two.


## Phase-based method
As phase durations are long, the time value of money not only greatly reduces the attractiveness of revenues, but also dampen the pain of costs.
If we disregard the fact that some phases (such as e.g. pre-clinical) are lengthy, and assume that the cashflows related to a phase all happen immediately upon phase entry, then we can trivially compute Expected Value (EV), Present Value (PV), and Expected Present Value (EPV) of all phases from the perspective of all phases.
We can then cumulate these metrics in order to, respectively, compute cumulative EV, Net Present Value (NPV), and Expected Net Present Value (ENPV) from any phase to any phase.

However, before performing this computation we must make an assumption as to how the free market revenues are distributed over the time spent in market.
Assuming that all revenues are secured directly upon market entry is a wildly inadequate assumption that will significantly skew the results due to discounting (i.e. the time value of money).

Instead, we will assume that revenues linearly increase over time for a period of 10 years.
Specifically, we assume that the revenues of year 0 is 0 and then linearly increase until the area under the curve is equal to the total global net sales.
The first year will thus be non-zero if the market revenues of the project is non-zero.
We compute the yearly revenues by adding yearly observations to our datasets and then removing the old market observations, as follows:

```{r}
phasely <- data.frame(phases)
market_years <- phases %>% filter(phase == 'MP')
sales_slope <- (market_years$sales * 2 / (market_years$time + 1)) / market_years$time
for (yr in 0:max(market_years$time)) {
  myr <- data.frame(market_years)
  myr$phase <- paste('Y', yr, sep = '')
  myr$time  <- 1
  myr$prob  <- myr$prob ^ (1 / myr$time)
  myr$cost  <- myr$cost / myr$time
  myr$sales <- ifelse(myr$time <= yr, yr * sales_slope, 0)
  phasely <- rbind(phasely, myr)
}
# Remove market phases since we've now added market years:
phasely <- filter(phasely, phase != 'MP')
# Re-code to ordered factor:
myr_names <- c('Y0','Y1','Y2','Y3','Y4','Y5','Y6','Y7','Y8','Y9','Y10')
phasely$phase <- factor(phasely$phase, levels=c(phase_names, myr_names), ordered=TRUE)
```

We can now compute the valuation metrics discussed above as follows:

```{r}
# TODO: Use the same function for all valuations.
# Should not matter whether it's phasely or yearly.
phase_based_valuation <- function(phs) {
  result <- tibble()
  for (from in phase_levels) {
    rows <- phs %>%
      filter(phase >= from) %>%
      group_by(src, intervention, subject) %>%
      arrange(src, intervention, subject, phase) %>%
      mutate(from = factor(from, levels=phase_names, ordered=TRUE),
             time_to = cumsum(time) - time,
             cum_prob = cumprod(prob),
             prob_to = cum_prob / prob,
             # cashflows
             cashflow = sales + prizes - cost,
             ev = cashflow * prob_to,
             pv = cashflow / ((1 + discount_rate_priv) ^ time_to),
             epv = pv * prob_to,
             # cumulatives
             cum = cumsum(cashflow),
             env = cumsum(ev),
             npv = cumsum(pv),
             enpv = cumsum(epv),
      )
      result <- bind_rows(result, rows)
  }
  result
}
```

Before plotting we must do a bit of data wrangling.

```{r}
phasely_phases_from_phases <- phase_based_valuation(phasely)

# Long
phasely_phases_from_phases_long <- phasely_phases_from_phases %>%
  select(src, intervention, subject, from, phase,
         cashflow, ev, pv, epv,
         cum, env, npv, enpv) %>%
  gather('valuation', 'value', -src, -intervention, -subject, -from, -phase) %>%
  transform(valuation = factor(valuation, levels = c(metrics, cum_metrics)))

# Long summary
phasely_phases_from_phases_long_sum <- phasely_phases_from_phases_long %>%
  group_by(src, from, phase, valuation) %>%
  mutate(mu  = mean(value),
         med = median(value),
         std = sd(value))

# Final long
phasely_final_from_phases_long <- phasely_phases_from_phases_long %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, subject, from, valuation) %>%
  arrange(phase) %>%
  summarize(value = tail(value, n=1))

# Final long summary
phasely_final_from_phases_long_sum <- phasely_phases_from_phases_long_sum %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, from, valuation) %>%
  arrange(phase) %>%
  summarize(mu  = tail(mu, n=1),
            med = tail(med, n=1),
            std = tail(std, n=1))
```

Let us then plot the value of taking the project from beginning (i.e. pre-clinical) to end (i.e. the final market year), using our cumulative valuation metrics.
Note the usage of different scales across the different metrics due the vastly different values.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
phasely_final_from_phases_long %>%
  filter(from == 'PC' & valuation %in% cum_metrics) %>%
  ggplot(aes(valuation, value, fill=src)) +
  geom_boxplot() +
  xlab(element_blank()) +
  ylab(element_blank()) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  facet_wrap(. ~ valuation, scale='free', nrow=1)
```

Another way to look at the data is to consider how starting at different phases alter the value of the project.
Below we plot the mean value of bringing the project to completion from various starting phases, using the four cumulative metrics.
The vertical lines delimit +/- 1 standard deviation from the sample mean (i.e. ~68% of the data).

```{r, echo=FALSE}
pos <- position_dodge(0.4)
phasely_final_from_phases_long_sum %>%
  ggplot(aes(from, mu, color=valuation, group=valuation)) +
  geom_linerange(aes(ymin=mu-std, ymax=mu+std), position=pos, size=1) +
  geom_line(position=pos, size=1, linetype='dotted') +
  geom_point(position=pos, size=2) +
  facet_wrap(. ~ src, ncol=2) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab(element_blank()) + ylab(element_blank())
```

## Year-based method
To consider the fact that all cashflows of a phase do not occur immediately upon phase-entry we convert our phase-based data to year-based data and make assumptions as to how the values are interpolated over the years of a phase.

We assume that all properties are constantly distributed over the time of the phase except for revenues.
We assume that the revenues of year 0 is 0 and then linearly increase until the area under the curve is equal to the total revenues of the phase.
The first year will thus be non-zero if the total revenues of the project is non-zero.

We use the following function to convert from phase-form to year-form:

```{r, results='hide'}
#' Converts a data set in phase-form to year-form.
#'
#' @param phases Dataframe in phase-form.
#' @return Dataframe in year-form
to_years <- function (phases) {

  # Convert phases to ordered factor
  phases$phase <- factor(phases$phase, levels=phase_levels, ordered=TRUE)

  # Add timestamps to every observation
  phases <- phases %>%
    group_by(src, intervention, subject) %>%
    arrange(src, intervention, subject, phase) %>%
    mutate(t = cumsum(time) - time)

  # Compute cost steps
  phases$cost_step <- (phases$cost / phases$time)
  phases$cost_remainder <- phases$cost - phases$cost_step * floor(phases$time)

  # Compute prob steps
  phases$prob_step <- phases$prob ^ (1 / phases$time)
  phases$prob_remainder <- phases$prob / (phases$prob_step ^ floor(phases$time))

  # Compute sales slopes
  phases$sales_slope <- (phases$sales * 2 / (phases$time + 1)) / phases$time

  # Transform: To cashflows over time (phase yearly)
  phase_years <- tibble()
  for (x in 1:ceiling(max(phases$time) + 1)) {

    # Compute step-based properties
    whole_step <- x <= phases$time
    cost <- ifelse(whole_step, phases$cost_step, phases$cost_remainder)
    sales <- ifelse(whole_step, phases$sales_slope * x, 0)
    prob <- ifelse(whole_step, phases$prob_step, phases$prob_remainder)
    prizes <- if (x == 1) phases$prizes else 0 # Immediate lump-sum

    # Make tibble
    has_decimals <- phases$time - floor(phases$time) != 0
    part_of_phase <- x <= phases$time | (x <= phases$time + 1 & has_decimals)
    year <- tibble(part_of_phase,
                   src = phases$src,
                   intervention = phases$intervention,
                   subject = phases$subject,
                   phase_year = x,
                   t = phases$t + x - 1,
                   phase = phases$phase,
                   discount_rate_publ = phases$discount_rate_publ,
                   cost,
                   sales,
                   prizes,
                   prob,
                   time = phases$time,
                   discount_rate_priv = phases$discount_rate_priv
                   ) %>%
    filter(x <= time | (x <= time + 1 & has_decimals)) %>% # Only years in phase
    select(-c('part_of_phase')) # Remove temporary column

    # Append
    phase_years <- bind_rows(phase_years, year)
  }

  phase_years
}
```

Using this function we can now convert our phase-based data to year-based data.
It should be noted that the function does not distribute the phase-based data over a series of equidistant years.
Data points are only equidistant within a phase, but not necessarily across.
In other words, if P1 entry would occur after 5.3 years then we will distribute PC properties over the 6 first years.
If the duration of P1 is 2.5 years then P2 would start at year 5.3 and end at year 7.8.
While we would divide P1 into three equidistant (years) points (years), that start from year 5.3, 6.3, and 7.3 (respectively) they are not equidistant from the PC steps.

```{r}
years <- to_years(phases)
```

We can now compute the same valuation metrics we computed for the phase-based method but this time for the year-based method:

```{r}
years_from_phases <- tibble()
for (from in phase_levels) {
  pyfp <- years %>%
    filter(phase >= from) %>%
    group_by(src, subject, intervention) %>%
    arrange(t) %>%
    mutate(from     = factor(from, levels=phase_levels, ordered=TRUE),
           time_to  = t - min(t),
           year     = floor(time_to),
           cum_prob = cumprod(prob),
           prob_to  = cum_prob / prob,
           # cashflows
           cashflow = sales + prizes - cost,
           ev       = cashflow * prob_to,
           pv       = cashflow / ((1 + discount_rate_priv) ^ time_to),
           epv      = pv * prob_to,
           # cumulatives
           cum      = cumsum(cashflow),
           env      = cumsum(ev),
           npv      = cumsum(pv),
           enpv     = cumsum(epv))
    years_from_phases <- bind_rows(years_from_phases, pyfp)
}
```

Again, we must follow this up with some data wrangling to prepare for analysis.

```{r}
# Long
years_from_phases_long <- years_from_phases %>%
  select(src, intervention, subject, from, year,
         cashflow, ev, pv, epv,
         cum, env, npv, enpv) %>%
  gather('valuation', 'value', -src, -intervention, -subject, -from, -year) %>%
  transform(valuation = factor(valuation, levels = c(metrics, cum_metrics)))

# Long summary
years_from_phases_long_sum <- years_from_phases_long %>%
  group_by(src, from, year, valuation) %>%
  summarize(mu  = mean(value),
            med = median(value),
            std = sd(value))

# Final long
final_year_from_phases_long <- years_from_phases_long %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, subject, from, valuation) %>%
  arrange(year) %>%
  summarize(value = tail(value, n=1))

# Final long summary
final_year_from_phases_long_sum <- years_from_phases_long_sum %>%
  filter(valuation %in% cum_metrics) %>%
  group_by(src, from, valuation) %>%
  arrange(year) %>%
  summarize(mu  = tail(mu, n=1),
            med = tail(med, n=1),
            std = tail(std, n=1))
```


As with the phase-based method, let us also do some descriptive statistics.
We begin with the value of taking the project from beginning (i.e. pre-clinical) to end (i.e. the final market year), using our cumulative valuation metrics.
Again, note the usage of different scales across the different metrics due the vastly different values.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
final_year_from_phases_long %>%
  filter(from == 'PC' & valuation %in% cum_metrics) %>%
  ggplot(aes(valuation, value, fill=src)) +
  geom_boxplot() +
  xlab(element_blank()) +
  ylab(element_blank()) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  facet_wrap(. ~ valuation, scale='free', nrow=1)
```

When we've transformed the data to year-based we've moved from an ordinal to a ratio scale.
As such it makes sense to explore how these valuations emerge as a consequence of the yearly cashflows of a project.
From top-left to bottom-right: out-of-pocket cashflows, risk-adjusted/expected value (EV), capitalized/present value (PV), risk-adjusted/expected capitalized/present value (EPV).
The middle line in each ribbon tracks the mean value, while the edges capture all values within 2 standard deviations of the mean (meaning 95% of the data).

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
years_from_phases_long_sum %>%
  filter(from == 'PC') %>%
  filter(valuation %in% metrics) %>%
  ggplot(aes(year, med)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_ribbon(aes(x=year, ymin=(mu-std*2), ymax=(mu+std*2), fill=src), alpha=.4) +
  geom_line(aes(color=src)) +
  facet_wrap(. ~ valuation, scale='free_y', ncol=2)
```

If we cumulate these values over time, we get the cumulative versions of these metrics.
These can be thought of as the value of running the project for x years from the start.
From top-left to bottom right: cumulative out-of-pocket cashflows, cumulative risk-adjusted/expected value (EV), capitalized/net present value (NPV), and risk-adjusted/expected capitalized/net present value (ENPV).

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
years_from_phases_long_sum %>%
  filter(from == 'PC') %>%
  filter(valuation %in% cum_metrics) %>%
  ggplot(aes(year, med)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
  xlab(element_blank()) + ylab(element_blank()) +
  geom_ribbon(aes(x=year, ymin=(mu-std*2), ymax=(mu+std*2), fill=src), alpha=.4) +
  geom_line(aes(color=src)) +
  facet_wrap(. ~ valuation, scale='free_y', ncol=2)
```

Another way to look at the data is to consider how starting at different phases alter the value of the project.
We begin by plotting the mean value of bringing the project to completion from whatever starting phase we're currently considering.
Using the four metrics previously applied.
The vertical lines delimit +/- 1 standard deviations from the sample mean (i.e. ~68% of the data).

<div class="alert alert-danger">
TODO: Why is cumulative decreasing in P1? That should be impossible? This must be an error?
</div>

```{r}
pos <- position_dodge(0.4)
final_year_from_phases_long_sum %>%
  ggplot(aes(from, mu, color=valuation, group=valuation)) +
  geom_linerange(aes(ymin=mu-std, ymax=mu+std), position=pos, size=1) +
  geom_line(position=pos, size=1, linetype='dotted') +
  geom_point(position=pos, size=2)  +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  facet_wrap(. ~ src, ncol=2) +
  xlab(element_blank()) + ylab(element_blank())
```

The table below summarizes the valuation metrics from the start of pre-clinical.

```{r, echo=FALSE}
sub <- final_year_from_phases_long_sum %>%
  ungroup %>%
  filter(from=='PC') %>%
  arrange(valuation)
knitr::kable(sub)
```

## Comparing methods
Let us compare the two methods by looking at the cumulative valuations from different starting points.
In the first two plots we observe that the simple valuation metrics cumulative cashflows and ENV, does not significantly differ between the two methods in any of the datasets.
In fact, the cumulative metric should yield exactly the same results in both methods as neither discounting nor probability is taken into consideration.
When computing ENV however, we take probability into account, and as a consequence, the outcome could in theory be different, but in practice, i.e. in the plot below, we observe that the difference is insignificant.

```{r, echo=FALSE, warning=FALSE}
pwise <- phasely_final_from_phases_long
ywise <- final_year_from_phases_long
methods <- c('phase', 'year')
pwise$method <- factor('phase')
ywise$method <- factor('year')
both <- bind_rows(pwise, ywise)
# TODO: Yields warning cuz coercing src to character vector (or similar)
# TODO: Use the phrases valuation metric and valuation method to differentiate
# whether we're talking about NPV/ENPV or phase-based/year-based.
```

```{r, echo=FALSE}
p <- both %>%
  filter(valuation %in% c('cum', 'env')) %>%
  ungroup %>%
  mutate(src = recode(src,
                      'DRIVE-AB (2018)' = 'drive-ab',
                      'Sertkaya et. al (2014)' = 'sertkaya')) %>%
  ggplot(aes(src, value, color=method, fill=method)) +
  geom_boxplot(alpha=0.3) +
  facet_wrap(valuation ~ from, scales='free', ncol=6) +
  ylab(element_blank()) +
  xlab(element_blank()) +
  theme(axis.text.x = element_text(angle=90))
print(p)
```

Taking discounting into consideration however, makes the two methods yield slightly different results.
The following two plots illustrate that difference for the metrics NPV and ENPV.
Again, in all datasets, from multiple starting phases.


```{r, echo=FALSE}
for (curr in c('npv', 'enpv')) {
  p <- both %>%
    filter(valuation == curr) %>%
    ungroup %>%
    mutate(src = recode(src,
                        'DRIVE-AB (2018)' = 'drive-ab',
                        'Sertkaya et. al (2014)' = 'sertkaya')) %>%
    ggplot(aes(src, value, color=method, fill=method)) +
    geom_boxplot(alpha=0.3) +
    facet_wrap(valuation ~ from, scales='free', ncol=6) +
    ylab(element_blank()) +
    xlab(element_blank()) +
    theme(axis.text.x = element_text(angle=90))
  print(p)
}
```


# Interventions
We will now introduce five interventions (treatments) to our base data sets and later analyze their effects on the valuation metrics.
We model these interventions as qualitatively (categorically) different, as they operate on different R&D phases, but theoretically they could be considered the same intervention that is quantitatively (numerically) different in terms of their actuation time.
The intervention can be described as a non-dilutive and unconditional prize.
This intervention can be considered a generalization of what is commonly referred to as either a market entry reward or a phase entry reward, where we've generalized the time of actuation.
We implement the intervention as a function `intervene` as follows:

```{r}
log10_sample <- function (n, min, max, magnitude_min, magnitude_max) {
  runif(n, min, max) * (10 ^ runif(n, magnitude_min, magnitude_max))
}

prize_samples <- log10_sample(N, 1, 9.999, -1, 3) # Reuse intervention samples across phases
intervene <- function (phases, target_phase, intervention_name) {
  data.frame(phases) %>%
    mutate(intervention = intervention_name,
           prizes = ifelse(phase == target_phase, prize_samples, 0))
}
```

We can then treat copies of the original phase-based data set and merge it into one larger data set, like this:

```{r}
intervened <- rbind(phases,
  intervene(phases, 'P1', 'P1ER'),
  intervene(phases, 'P2', 'P2ER'),
  intervene(phases, 'P3', 'P3ER'),
  intervene(phases, 'P4', 'P4ER'),
  intervene(phases, 'MP', 'PDMER'))
```

Note that we are logarithmically sampling prize sizes.
This is because we assume that the absolute difference in effect will be much smaller when prize sizes are very small, as compared to when prize sizes are very large, and hence need more samples at the "bottom" to properly saturate the space.
In the analysis we at all times "control for" prizes which means that the chosen distribution does not affect any of the conclusions beyond sample saturation.
The sampled prizes are summarized in the histogram below.

```{r, echo=FALSE, message=FALSE}
print(intervened %>%
      filter(prizes > 0) %>%
      ggplot(aes(prizes, fill=phase)) +
      geom_histogram() +
      theme(axis.title.y=element_blank()) +
      facet_wrap(interaction(intervention, phase)~., ncol=3) +
      guides(fill = FALSE))
```

We then convert the data to yearly, and restructure it so that every row with an intervenetion is matched with the corresponding row without an intervention.

```{r}
intervened_years <- to_years(intervened)
control <- filter(intervened_years, intervention == 'NONE')
treated <- filter(intervened_years, intervention != 'NONE')
comparable_years <-
  semi_join(treated, control, by=c('src', 'subject', 'phase', 't'))
```


# Analysis

The question we want to explore is whether it is cheaper for the benefactor to directly or indirectly fund antibiotics R&D.
Directly here refers to the idea of simply paying for development "at cost".
Indirectly here refers to the idea of issuing prizes that encourage private developers to undertake a given activity with the hope of winning said prize.

We will compare these two different approaches by comparing their expected costs when facing a hypothetical project entering pre-clinical.
We assume that the valuation metric that private actors employ is ENPV and that the valuation method is yearly.
We will refer to this value as *private ENPV.*

```{r, echo=FALSE}
public_discount_rate_min = 0.035
public_discount_rate_max = 0.045
```

In order to reason about the cost for the benefactor of issuing a prize we will compute the necessarily negative ENPV of issuing said prize.
ENPV is an appropriate measure, since the prize is probabilistically issued in the future.
As previously described, we assume that the benefactor is the public sector and employ a uniformly distributed discount rate between `r publ_dr_min * 100`% and `r publ_dr_max * 100`%.
We refer to the expected cost of a prize (i.e. an intervention) as *indirect ENPV*.
This can be thought of as the expected cost for the benefactor.

If indirect ENPV is the cost of the intervention for the public, then we must also compute the expected cost of simply paying for the project in question at cost.
To take both cost of capital and project risk into consideration, we will again compute the negative ENPV.
The probabilistic cashflows used in the ENPV calculation in this case is the project's cost without any expectation of revenues.
We assume that the public payer is the same as the benefactor and hence employ discount rates from the same distribution
(`r publ_dr_min * 100`% - `r publ_dr_max * 100`%).
We refer to the expected cost of paying for the project at cost as *direct ENPV*.

We compute private, direct, and indirect ENPV like this:

```{r}
comparable_years <- comparable_years %>%
  arrange(t) %>%
  group_by(src, subject, intervention) %>%
  mutate(
         prob_to = cumprod(prob) / prob, # TODO: Is this correct?
         intervention_size = sum(prizes),
         cashflow_bef = sales - cost,
         cashflow_aft = sales - cost + prizes,
         enpv_prv_bef = cumsum((cashflow_bef / ((1 + discount_rate_priv) ^ t)) * prob_to),
         enpv_prv_aft = cumsum((cashflow_aft / ((1 + discount_rate_priv) ^ t)) * prob_to),
         enpv_dir = cumsum((-cost / ((1 + discount_rate_publ) ^ t)) * prob_to),
         enpv_ind = cumsum((-prizes / ((1 + discount_rate_publ) ^ t)) * prob_to),
         ) %>%
  select(-prob_to)
```

We extract the different ENPV values from the perspective of pre-clinical, like this:

```{r}
finals <- comparable_years %>%
  arrange(t) %>%
  group_by(src, subject, intervention) %>%
  summarise(
         prizes = sum(prizes),
         enpv_prv_bef = tail(enpv_prv_bef, n=1),
         enpv_prv_aft = tail(enpv_prv_aft, n=1),
         enpv_dir = tail(enpv_dir, n=1),
         enpv_ind = tail(enpv_ind, n=1))
```


## Intervention effectiveness
Determining the efficiency of an intervention means that we must take both its effectivess and its cost into consideration.
We will measure effectiveness as the likelihood that a given intervention turns a pre-clinical no-decision into a go-decision, and we will measure cost as the previously computed indirect ENPV.

Before we proceed further however, let us first ensure that the issued prizes (i.e. the interventions) actually do have an effect on private ENPV.
As such there should be a correlation between prize size (of every intervention) and private ENPV in all datasets.

```{r, echo=FALSE}
print(finals %>%
  ggplot(aes(prizes, enpv_prv_aft, color=src)) +
  geom_point(alpha=1, shape=1) +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ intervention, ncol=3, scale='free') +
  xlab('Prize') +
  ylab('Private ENPV') +
  theme(legend.position = 'top')
)
```

While clearly correlated, the data seems more suitable to plot on a logarithmic scale.
However, since private ENPV sometimes is negative it is unsuitable for plotting on a log scale.
By subtracting the private ENPV value before the application of the intervention from the private ENPV after the application of the intervention we get the improvement in private ENPV, i.e. the delta.
This can be thought of as the absolute effect of the treatment.
Since we're modeling non-dilutive prizes with no strings attached, the intervention will always improve private ENPV which means that the delta as a consequence always will be greater than zero.
Intervention prize and private ENPV improvement also, expectedly, seem correlated.

```{r, echo=FALSE}
print(finals %>%
  ggplot(aes(prizes, enpv_prv_aft-enpv_prv_bef, color = src)) +
  geom_point(alpha=0.8, shape=1) +
  facet_wrap(. ~ intervention, ncol=3, scale='free') +
  xlab('Prize') +
  ylab('Private ENPV improvement') +
  scale_x_continuous(trans='log10',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  scale_y_continuous(trans='log10',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  theme(legend.position = 'top')
)
```

Other studies have shown, that the later a prize is awarded, the higher the size of the prize must be to achieve a comparative increase in ENPV.
This phenomena is clearly visible in our model when plotting the correlation between prize size and private ENPV improvement for all interventions on the same scale.

```{r, echo=FALSE, warning=FALSE}
print(finals %>%
  ggplot(aes(prizes, enpv_prv_aft - enpv_prv_bef, color=intervention)) +
  geom_smooth(method='lm', se=FALSE) +
  geom_point(shape=1) +
  xlab('Prize') +
  ylab('Private ENPV improvement') +
  scale_y_continuous(trans='log10',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  scale_x_continuous(trans='log10',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  facet_wrap(. ~ src, ncol=2, 'free') +
  theme(legend.position = 'top')
)
# TODO: This plot yields warnings
```

Let us now move to estimating the interventions' improvement on pre-clinical go-decisions.
We assume that a pre-clinical go-decision will be reached if ENPV upon entering pre-clinical is at or above zero, and a no-decision is reached if not.

```{r}
finals$go <- factor(finals$enpv_prv_aft >= 0)
```

If we jitter plot prize size on the x-axis and the binary go-decision on the y-axis, we (expectedly) observe that as prizes increase, initial no-decisions are eventually turned into go-decisions.
If we look closely we even see that the range of prizes that turns no's into go's seems to be higher in later phases and lower in earlier.
This is of course also expected since we've already established that late-phase prizes must be higher to achieve an ENPV improvement comparable to early-phase prizes.

```{r, echo=FALSE}
finals %>%
  ggplot(aes(prizes, go, color=intervention)) +
  geom_jitter(shape=1) +
  facet_grid(cols=vars(src), rows=vars(intervention)) +
  theme(legend.position = 'none') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4)))
```

The analysis above is however also diffused by the fact that we are not differentiating between turning a no-decision into a go-decision and simply paying money to a project that already faced a go-decision.
Let us focus in on projects that faced an initial no-decision.

```{r}
nos <- filter(finals, enpv_prv_bef < 0)
```

Running the same plot on the new subset we get a clearer picture of when no's actually are turned into go's.

```{r, echo=FALSE}
nos %>%
  ggplot(aes(prizes, go, color=intervention)) +
  geom_jitter(shape=1) +
  facet_grid(cols=vars(src), rows=vars(intervention)) +
  theme(legend.position = 'none') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4)))
```

To more properly understand at what ranges the different interventions have their effects we can build a logistic regression model where prize size predicts no/go:

```{r}
go_model <- glm(
  go ~ prizes * interaction(src, intervention),
  data = nos,
  family = binomial(link='logit'))
```

and then use that model to predict the probability of a particular prize yielding a go- as opposed to a no-decision.
We will also refer to this as the go-rate of an intervention.

```{r}
nos$go_pred_prize <- predict(go_model, type='response')
```

With this we can more clearly reason about the prize sizes at which the interventions actually have their effects.
In other words we can now reason about the probability of a particular prize size turning a no-decision into a go-decision.
We now clearly see how late-phase prizes must be substantially higher to have the same probability of yielding go-decisions as early-phase prizes.

```{r, echo=FALSE}
nos %>%
  ggplot(aes(prizes, go_pred_prize*100, color=intervention)) +
  geom_line() +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  xlab('Prize (log)') + ylab('Probability of no -> go') +
  facet_wrap(. ~ src)
```

When filtering out projects that already faced go-decisions we notice that a plethora of projects already faced go-decisions even without intervention.
Many (the author included) have suggested that prizes should be coupled with profitability analyses in order to avoid wasteful spending of public resources.
However, such a filtering process is likely to be prohibitively difficult in reality.
And even if conducted, the incentive for developers to "rig" the numbers and bend the truth is substantial.
Granting prizes to developers with projects that would have been pursued regardless of the existance of the prize is consequently assumed to be an unfortunate but possibly unavoidable reality.

Under this assumption we cannot employ the "cleaner" no-to-go model but must use all the data and use prize size to predict go-decisions in the existance of the noise caused by projects facing go-decisions irrespectively.

```{r}
mod <- glm(
  go ~ prizes * interaction(src, intervention),
  data = finals,
  family = binomial(link='logit'))
finals$go_pred_prize <- predict(mod, type='response')
```

```{r, echo=FALSE}
finals %>%
  ggplot(aes(prizes, go_pred_prize*100, color=intervention)) +
  geom_line() +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  xlab('Prize (log)') + ylab('Probability of go-decision') +
  facet_wrap(. ~ src)
```

While this works, the "starting point" is now closer to the baseline probability of a go-decision without an intervention rather than closer to 0.
Presumably this second model also explains less of the variance.

<div class="alert alert-danger">
TODO: Report variance explained (maybe r squared?) for the two models.
</div>



## Intervention cost
While we now have a way of determining the effectiveness of a given prize, we must combine that information with its cost in order to determine its efficiency.
In order to gauge the actual cost of how much a benefactor will have to pay when issuing a particular prize, we choose to compute ENPV from the perspective of the payer.
We call this "indirect ENPV", and it can be thought of as the expected, capitalized cost of issuing the intervention publicly and then paying for every successful candidate without receiving any financial return whatsoever.

Specifically, the metric takes both the opportunity cost (cost of capital) and risk of a project into consideration.
Taking opportunity cost into consideration is important as the benefactor looses the opportunity to activate their money elsewhere when making a payment to a beneficiary.
Taking risk into consideration is important as the benefactor will not necessarily pay the promised prize size, due to projects being highly risky.
We will now explore the correlation between prize size and the negative ENPV for the benefactor.
Note that ENPV for the benefactor necessarily will be negative as the prize payout is the only cashflow considered.

```{r, echo=FALSE}
finals %>%
  ggplot(aes(prizes, -enpv_ind, color=intervention)) +
  geom_point(shape=1) +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ src, ncol=3) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:3)),
                     minor_breaks = c(1:9 %o% 10^(0:3))) +
  xlab('Prize (log)') + ylab('-Indirect ENPV')
```

However, since not all projects will reach a go-decision, we cannot compute the indirect cost as simply the expected cost of the intervention given the project's probabilities, but must instead compute the expected cost of the intervention given the project's probabilities and the owner's decision of whether to pursuit it or not.
In other words, an intervention will not have to fund projects ran by owners that choose to terminate their projects, and consequently never reach the prize point.

Again, assuming that a pre-clinical go-decision is reached if ENPV after the introduction of the intervention is greater than or equal to 0, we can filter and keep only observations where we've reached a go-decision and plot again.
While paying for substantially fewer projects when issuing lower prizes, we still observe the same "stacking" of the interventions as we did before removing non-go projects.

```{r, echo=FALSE}
finals %>%
  filter(enpv_prv_aft > 0) %>%
  ggplot(aes(prizes, -enpv_ind, color=intervention)) +
  geom_point(shape=1) +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ src, ncol=3) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:4)),
                     minor_breaks = c(1:9 %o% 10^(0:4))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:3)),
                     minor_breaks = c(1:9 %o% 10^(0:3))) +
  xlab('Prize (log)') + ylab('-Indirect ENPV')
```

In summary, later interventions must be larger to achieve yield a private ENPV improvements comparable to earlier interventions, but if we disregard this difference in ENPV improvement then later interventions are cheaper for the benefactor than earlier ones of the same size.
Such a simplification is unfortunately unacceptable as the purpose of an indirect intervention is to improve private ENPV to a point where no-decisions are turned into go-decisions.


## Intervention efficiency
To estimate an intervention's efficiency we must consider both its effectiveness and its cost.
More specifically we must compute its cost per unit of effect.
As argued above, we will use the full dataset that includes projects that would have reached go-decisions irrespective of the existance of an intervention as ignore them would systematically skew the data.

We will however only look at go-rates lower than `r go_lim_up = 0.99; go_lim_up*100`%.
Since a go-rate higher than 100% is not achievable, any prize size larger than the lowest size that yields a go-rate of 100% will also merely yield 100%.

```{r, echo=FALSE}
finals %>%
  filter(go_pred_prize < go_lim_up) %>%
  ggplot(aes(go_pred_prize*100, -enpv_ind, color=intervention)) +
  geom_point(shape=1) +
  #geom_smooth() +
  #scale_x_continuous(trans = 'log',
  #                   breaks = c(1 %o% 10^(0:4)),
  #                   minor_breaks = c(1:9 %o% 10^(0:4))) +
  #scale_y_continuous(trans = 'log',
  #                   breaks = c(1 %o% 10^(0:4)),
  #                   minor_breaks = c(1:9 %o% 10^(0:4))) +
  xlab('Probability of go-decision (%)') +
  ylab('Inverse indirect ENPV') +
  theme(legend.position='none') +
  facet_grid(cols=vars(intervention), rows=vars(src))
```

Using local regression (loess) we can see that the interventions again seem to "stack".

```{r, echo=FALSE}
finals %>%
  filter(go_pred_prize < go_lim_up) %>%
  ggplot(aes(go_pred_prize*100, -enpv_ind, color=intervention)) +
  geom_smooth(method='loess', se=FALSE) +
  #scale_x_continuous(trans = 'log',
  #                   breaks = c(1:9 %o% 10^(0:4)),
  #                   minor_breaks = c(1:9 %o% 10^(0:4))) +
  #scale_y_continuous(trans = 'log',
  #                   breaks = c(1 %o% 10^(0:4)),
  #                   minor_breaks = c(1:9 %o% 10^(0:4))) +
  facet_wrap(. ~ src) +
  xlab('Probability of go-decision (%)') +
  ylab('Inverse indirect ENPV') +
  theme(legend.position = 'top')
```

# Conclusion
