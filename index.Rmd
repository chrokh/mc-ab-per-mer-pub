---
title: Time for public pharma?
subtitle: Supplementary material
author: Christopher Okhravi
date: 2019
output:
  bookdown::gitbook:
    css: style.css
    split_by: section
    config:
      toc:
        collapse: subsection
---


# Preface {-}
This document contains the full analysis that underpins the paper paper "Time for public pharma?".
This is a monte carlo simulation that explores direct versus indirect funding of antibiotics research and development (R&D).
Direct funding is here used to mean that a benefactor pays for antibiotics R&D at cost.
Indirect funding is here used to mean that a benefactor issues non-dilutive prizes to whoever completes a particular phase, with the intent of incentivizing private developers to undertake said and prior phases.
This analysis estimates which of the two, ceteris paribus, is cheaper for the benefactor.

## How to compile this document {-}
If you're reading this in an html format then the document is already compiled.
You can compile the Rmd file(s) like this:

```bash
Rscript -e "bookdown::render_book('index.html')"
```

Note: While some of the code that is used to perform the analysis is included in the rendered (i.e. output) document, some is not.
If you need further details, please consult the source code used to generate the output document.


# Input data
We begin by setting `N` which will dictate the number of project samples that we'll draw from each dataset.
Meaning that if one considers all the data sets at once then the number of random projects drawn in total is `N` times the number of data sets considered.
In other words, `N` is the number of random samples that we'll draw from a stochastic representation of a hypothetical antibiotic R&D project.
The distribution of the stochastic project depends on the data set we're using, and in a simple attempt to avoid drawing data set specific conclusions, we'll look at multiple data sets.

```{r, set-n}
N = 2000
```

```{r, basics, include=FALSE}
# Dependencies
library(knitr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(triangle)
library(scales)
library(cowplot)
library(broom)
library(purrr)
library(forcats)
library(mgcv) # Non-parametric models (GAM)
library(splines) # Non-parametric models (splines::bs)

# Cache code chunks
knitr::opts_chunk$set(cache=TRUE, collapse=TRUE)

# Set seed for reproducibility
set.seed(3)

# NOTE: I can't figure out how to sample from multiple triangular distributions
# at the same time using rtriangle, so here's my sloppy quickfix implementation.
# Usage is the same as rtriangle, but you can pass vectors in mins, maxs and mids.
rtriangle2 <- function(n, mins, maxs, mids) {
  xs <- c()
  while (length(xs) < n)
    for (x in 1:length(mins))
      xs <- c(xs, rtriangle(1, mins[x], maxs[x], mids[x]))
  xs
}

# Pass in a p-value to get a result like -4 meaning p < 10^-4.
p_lower_than <- function(p)
  for (x in -500:100)
    if (p < 10^x)
      return(x)

# The phases we'll be working with
DEVELOPMENT_PHASES <- c('PC','P1','P2','P3','P4')
MARKET_PHASES <- paste('M', 1:20, sep='')
PHASES <- c(DEVELOPMENT_PHASES, MARKET_PHASES)
```

The data in this study stem from Sertkaya et. al (2014), who presents data for hypothetical antibiotics targeting six different indications.
The following is an approximation of the data used by Sertkaya et. al (2014).
We say approximation as a few of their assumptions are not reconcilable with our modeling choices.
These compromises are described when encountered.

Sertkaya et. al (2014) differentiates between antibiotics on the basis of target indication.
In our analysis we sometimes use the term dataset source (or src for short) to refer to indication since we treat each indication as a separate dataset.

The table below summarize the distribution of development data as per 3.2.2 -- 3.2.5 in Sertkaya et. al (2014).
Time is expressed in months, cost and revenues in million USD, and probabilities and market shares in percentage.

We use the term phase 4 (abbreviated P4) while Sertkaya et. al (2014) use the term NDA/BLA.

```{r, dist-sertkaya, echo=FALSE}
INDICATIONS <- c('ABOM', 'ABSSSI', 'CABP', 'CIAI', 'CUTI', 'HABP/VABP')

sertkaya2014_phase_dist <-
  rbind(data.frame(indication = INDICATIONS,
                   phase = 'PC',
                   time_min = 52,
                   time_mid = 66,
                   time_max = 72,
                   cost_min = 19,
                   cost_mid = 21.1,
                   cost_max = 23.2,
                   prob_min = 17.5,
                   prob_mid = 35.2,
                   prob_max = 69
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P1',
                   time_min = 9,
                   time_mid = 10.5,
                   time_max = 21.6,
                   cost_min = 7.3,
                   cost_mid = 9.7,
                   cost_max = 12,
                   prob_min = 25,
                   prob_mid = 33,
                   prob_max = 83.7
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P2',
                   time_min = c(12, 9, 12, 10, 10, 16),
                   time_mid = c(15, 10, 15, 11, 11, 18),
                   time_max = 30,
                   cost_min = c(7.4, 7.12, 7.28, 7.68, 7.28, 12.48),
                   cost_mid = c(9.2, 8.9, 9.1, 9.6, 9.1, 15.6),
                   cost_max = c(11, 10.68, 10.92, 11.52, 10.92, 18.72),
                   prob_min = 34,
                   prob_mid = 50,
                   prob_max = 74
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P3',
                   time_min = c(20, 10, 10, 17, 17, 35),
                   time_mid = c(24, 12.5, 12.5, 21.5, 21.5, 39),
                   time_max = 47,
                   cost_min = c(33.36, 26.88, 31.04, 40.48, 35.04, 81.12),
                   cost_mid = c(41.7, 33.6, 38.8, 50.6, 43.8, 101.4),
                   cost_max = c(50.04, 40.32, 46.56, 60.72, 52.56, 121.68),
                   prob_min = 31.4,
                   prob_mid = 67,
                   prob_max = 78.6
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P4',
                   time_min = 6,
                   time_mid = 9,
                   time_max = 12.5,
                   cost_min = 1.9588, # NDA/BLA submission cost
                   cost_mid = 1.9588, # NDA/BLA submission cost
                   cost_max = 1.9588, # NDA/BLA submission cost,
                   prob_min = 83,
                   prob_mid = 85,
                   prob_max = 99
                   )
        ) %>% arrange(phase, indication)
kable(sertkaya2014_phase_dist, caption='sertkaya2014_phase_dist')
```

Same table, transposed:

```{r, olhskfjhsdkj, echo=FALSE, results='asis'}
sertkaya2014_phase_dist %>%
  pivot_longer(time_min:prob_max, names_to='param', values_to='value') %>%
  spread(indication, value) %>%
  separate(param, c('param', 'point'), '_') %>%
  mutate(point = factor(point, levels=c('min','mid','max'), ordered=T)) %>%
  select(param, phase, point, everything()) %>%
  arrange(param, phase, point) %>%
  kable(caption='Sertkaya et. al (2014)') %>%
  print
```

Sertkaya et. al (2014) seems to not directly sample the market share distribution, but rather to sample a product launch success probability distribution, and then apply that sample to reach an estimate for every year's market share.
This ensures that the market share does not vary widely between the lower and upper bound on a yearly basis.
Instead, the point between the lower and upper bound remains constant, while the lower and upper bounds themselves vary.
Also, this ensures that no year has a lower market share than the previous year before peak year sales.

```{r, sertkaya2014-market-dist, echo=FALSE}
sertkaya2014_market_size_dist <-
  data.frame(indication = INDICATIONS,
             min = c(2720, 3070, 2290, 2530, 5760, 1780),
             max = 9230)

sertkaya2014_market_share_dist <-
  data.frame(year = 1:20,
             min = c(0.05, 0.87, 1.57, 2.57, 3.92, 5.79, 7.52, 8.52, 10.10, rep(12.27, 11)),
             max = c(0.11, 1.91, 3.47, 5.68, 8.64, 12.77, 16.59, 18.80, 22.30, rep(27.08, 11)))

kable(sertkaya2014_market_size_dist, caption='sertkaya2014_market_size_dist')
kable(sertkaya2014_market_share_dist, caption='sertkaya2014_market_share_dist')
```

In line with 3.2.11 of Sertkaya et. al (2014) we assume a total product life (i.e. market life) of 20 years.
In line with 3.2.9-3.2.10 of Sertkaya et. al (2014) we assume that patent expiry leads to a reduction in revenues due to generic entry.
The time (year) of generic entry (i.e. patent expiry) and the corresponding reduction of revenues are distributed as per the table below.
In summary this means that the captured market share will increase from year 1 to year 10, and then remain constant until generic entry (i.e. patent expiry), upon which it will be reduced to a lower constant until year 20.

The table below also reports the discount rates which stem from 3.2.1 of Sertkaya et. al (2014) who use the term real opportunity cost of capital.

```{r, echo=FALSE}
sertkaya2014_market_reduction_min <- 25 # percent
sertkaya2014_market_reduction_mid <- 50 # percent
sertkaya2014_market_reduction_max <- 75 # percent
sertkaya2014_generic_entry_min <- 10 # years
sertkaya2014_generic_entry_mid <- 12 # years
sertkaya2014_generic_entry_max <- 14 # years
sertkaya2014_launch_success_min <- 40 # percent
sertkaya2014_launch_success_mid <- 60 # percent
sertkaya2014_launch_success_max <- 80 # percent
sertkaya2014_private_discount_rate_min <- 9 # percent
sertkaya2014_private_discount_rate_mid <- 11 # percent
sertkaya2014_private_discount_rate_max <- 24 # percent

tmp <- data.frame(parameter = c('Product launch success probability (%)',
                                'Market reduction due to generic entry (%)',
                                'Generic entry (year)',
                                'Private developer discount rate (%)'),
                  min = c(sertkaya2014_launch_success_min,
                          sertkaya2014_market_reduction_min,
                          sertkaya2014_generic_entry_min,
                          sertkaya2014_private_discount_rate_min),
                  mid = c(sertkaya2014_launch_success_mid,
                          sertkaya2014_market_reduction_mid,
                          sertkaya2014_generic_entry_mid,
                          sertkaya2014_private_discount_rate_mid),
                  max = c(sertkaya2014_launch_success_max,
                          sertkaya2014_market_reduction_max,
                          sertkaya2014_generic_entry_max,
                          sertkaya2014_private_discount_rate_max))
kable(tmp)
```

Sertkaya et. al (2014) also consider costs for a few additional (1) supply chain activities, (2) non-clinical work, and (3) post-approval studies as listed below.
The costs are spread across various phases as indicated by the percentages under the corresponding phase columns.
These figures and percentages stem from Table 9, section 3.2.7, and section 3.2.8 of Sertkaya et. al (2014) respectively.

Sertkaya et. al (2014) report that the cost of post-approval studies may last up to three years following market entry.
As such we assume that the cost is evenly distributed over three years.

```{r, sertkaya-additional-dist, echo=FALSE}
activity <-
  c('Sample preparation for animal/human studies',
    'Process research/development/design',
    'Plant design',
    'Plant build',
    'Non-clinical work',
    'Post-approval studies')
sertkaya2014_additional_dist <-
  data.frame(activity,
             min = c(2.4, 18.7, 10.7, 69.6, 3.4, 8),
             mid = c(2.7, 26.8, 13.4, 83,   3.7, 10),
             max = c(2.9, 34.8, 16.1, 96.3, 4,   12),
             PC = 0,
             P1 = c(1/3, 0.5, 0,    0, 0,   0),
             P2 = c(1/3, 0.5, 0,    0, 1/3, 0),
             P3 = c(1/3, 0,   0.75, 0, 1/3, 0),
             P4 = c(0,   0,   0.25, 1, 1/3, 0),
             M1 = c(0,   0,   0,    0, 0,   1/3),
             M2 = c(0,   0,   0,    0, 0,   1/3),
             M3 = c(0,   0,   0,    0, 0,   1/3)
  )

sertkaya2014_additional_dist %>%
  mutate(P1 = round(P1, 2) * 100,
         P2 = round(P2, 2) * 100,
         P3 = round(P3, 2) * 100,
         P4 = round(P4, 2) * 100,
         M1 = round(M1, 2) * 100,
         M2 = round(M2, 2) * 100,
         M3 = round(M3, 2) * 100) %>%
  kable(caption='sertkaya2014_additional_dist (million usd and rounded percentages)')
```


## Benefactor parameters
To perform our analysis we require two additional parameters.
First, a "public" or "social" discount rate that is representative of the cost of capital of the benefactor, i.e. the body that pays for the intervention with no expectation of monetary return.
Second, an inefficiency parameter that will be used to measure the fact that many argue that agents with lower expectation of monetary returns (e.g. states) are often assumed to be less efficient.
We uniformly distribute these parameters as per the table below.

```{r, additional-dist, echo=FALSE}
public_discount_rate_min <- 0.035
public_discount_rate_max <- 0.045

public_inefficiency_min <- 0
public_inefficiency_max <- 1

data.frame(parameter = c('Public discount rate (%)',
                         'Public inefficiency (%)'),
           min = c(public_discount_rate_min * 100,
                   public_inefficiency_min * 100),
           max = c(public_discount_rate_max * 100,
                   public_inefficiency_max * 100)) %>% kable
```

## Interventions distribution
We model interventions as qualitatively (categorically) different, as they operate on different R&D phases, but theoretically they could be considered the same intervention that is quantitatively (numerically) different in terms of its actuation time.
The intervention can be described as a unconditional, non-dilutive prize.
This intervention can be considered a generalization of what is commonly referred to as either a market entry reward or a phase entry reward, where we've generalized the time of actuation.
Prize size is quantitatively varied stochastically.

We are logarithmically sampling prize sizes.
This is because we assume that the absolute difference in effect will be much smaller when prize sizes are very small, as compared to when prize sizes are very large, and hence need more samples at the "bottom" to properly saturate the space.
In the analysis we at all times "control for" prizes which means that the chosen distribution does not affect any of the conclusions beyond sample saturation.
The table below show the intervention distribution that we are sampling from.

```{r, intervention-dist, echo=FALSE}
intervention_dist_magn_min <- 5
intervention_dist_magn_max <- 11

# Print as table
# TODO: More descriptive table headers
data.frame(parameter = 'Exponent',
           min = intervention_dist_magn_min,
           max = intervention_dist_magn_max) %>%
  kable(caption='Interventions')
```

While the intervention prize size can be thought of as the treatment dosage, the intervention phase can be thought of as the treatment type.
The table below report which phases we apply interventions to.

```{r, prize-phase-distribution, echo=FALSE}
intervention_phase_dist <-
  data.frame(prize_phase = factor(c('P1', 'P2', 'P3', 'P4', 'M1'),
                                  levels=PHASES, ordered=TRUE))
kable(intervention_phase_dist, caption='Intervention target phases')
```

## Sampling
We first sample development data from the Sertkaya distributions.

```{r, sample-sertkaya-dev}
phases <- sertkaya2014_phase_dist %>%
  # Convert phase to ordered factor
  mutate(phase = factor(phase, levels=PHASES, ordered=TRUE)) %>%
  # Replicate every row N number of times
  uncount(N) %>%
  # Add subject ids but group by phase so identifiers are reused across phases
  group_by(phase) %>% mutate(subject = 1:n()) %>% ungroup() %>%
  # Sample distributions
  mutate(time = rtriangle2(n(), time_min, time_max, time_mid),
         cost = rtriangle2(n(), cost_min, cost_max, cost_mid),
         prob = rtriangle2(n(), prob_min, prob_max, prob_mid)) %>%
  # Normalize sampled data
  mutate(cost = cost * 10^6,    # Convert million usd to usd
         time = time / 12,      # Convert months to years
         prob = prob / 100) %>% # Convert percentage to fraction
  # Add hard-coded development data
  mutate(sales = 0) %>%
  # Rename indication column and remove temp cols
  rename(src = indication) %>%
  select(subject, src, phase, time, cost, prob, sales)
```

We then sample market data from the Sertkaya distributions and merge it with our development phases sample.
<!-- First, we pair up every subject with the market size distribution of its indication.... -->

```{r, sample-sertkaya-market}
phases <- phases %>%
  # Only keep source and subject column and distinct rows
  select(src, subject) %>% unique %>%
  # Left join with size distribution
  merge(., sertkaya2014_market_size_dist, by.x='src', by.y='indication', all.x=TRUE) %>%
  # Sample market size distribution, then remove distribution columns
  mutate(market_size = runif(n(), min, max)) %>%
  select(-min, -max) %>%
  # Normalize sampled market size
  mutate(market_size = market_size * 10^6) %>% # Convert million usd to usd
  # Sample non-indication specific market parameters
  mutate(launch_success = rtriangle2(n(),
                                     sertkaya2014_launch_success_min,
                                     sertkaya2014_launch_success_max,
                                     sertkaya2014_launch_success_mid),
         generic_entry = rtriangle2(n(),
                                    sertkaya2014_generic_entry_min,
                                    sertkaya2014_generic_entry_max,
                                    sertkaya2014_generic_entry_mid),
         generic_reduction = rtriangle2(n(),
                                        sertkaya2014_market_reduction_min,
                                        sertkaya2014_market_reduction_max,
                                        sertkaya2014_market_reduction_mid)) %>%
  # Normalize non-indication specific market parameters
  mutate(launch_success = launch_success / 100, # Percentage to fraction
         generic_reduction = generic_reduction / 100, # Percentage to fraction
         generic_entry = round(generic_entry)) %>% # Round to full year
  # Cartesian product merge with market share dist
  merge(., sertkaya2014_market_share_dist) %>%
  # Normalize market share dist
  mutate(min = min / 100, max = max / 100) %>% # Percentages to fractions
  # Compute captured market share per year according to Sertkaya et. al (2014) Table 12
  mutate(share = (min * (1 - launch_success)) + (max * launch_success)) %>%
  # Compute reduction in market due to generic entry (Sertkaya et. al, 2014, 3.2.10)
  mutate(share = ifelse(year < generic_entry, share, (share - (share * generic_reduction)))) %>%
  # Compute yearly sales
  mutate(sales = share * market_size) %>%
  # Convert year to character factor matching MARKET_PHASES
  mutate(phase = sprintf('M%s', year)) %>%
  # Add hard-coded market data
  mutate(time = 1, prob = 1, cost = 0) %>%
  # Remove temporary columns
  select(subject, src, phase, time, cost, prob, sales) %>%
  # Add to initial dev sample
  rbind(phases, .)
```

Finally, we sample the additional supply chain activity costs data,
distribute the samples across the development and market phases according to their corresponding fractions,
and sum up the additional costs per phase for every subject.
From this point onwards it is therefore impossible to distinguish additional supply chain costs (e.g. non-clinical work costs) from clinical phase costs.
These additional costs are then added into the full sample.

```{r, sample-sertkaya-additional}
phases <- phases %>%
  # Only keep source column and distinct rows
  select(subject) %>% unique %>%
  # Cartesian product with additional costs distribution
  merge(., sertkaya2014_additional_dist) %>%
  # Normalize additional costs
  mutate(min = min * 10^6,      # Convert million usd to usd
         mid = mid * 10^6,      # Convert million usd to usd
         max = max * 10^6) %>%  # Convert million usd to usd
  # Sample additional costs
  mutate(tot_add_cost = rtriangle2(n(), min, max, mid)) %>%
  # Remove temporary columns
  select(-min, -mid, -max, -activity) %>%
  # Wide to long: make phase names rows instead of columns
  gather(phase, fraction, -subject, -tot_add_cost) %>%
  # Compute actual additional costs in phase
  mutate(add_cost = tot_add_cost * fraction) %>%
  # Sum up costs per phase for every subject
  group_by(subject, phase) %>%
  summarize(add_cost = sum(add_cost)) %>%
  # Left join with full sample (NOTE: causes NAs where no additional costs)
  merge(phases, ., by=c('subject', 'phase'), all.x = TRUE) %>%
  # Add additional cost to regular cost and remove temp column
  mutate(cost = ifelse(is.na(add_cost), cost, cost + add_cost)) %>%
  select(-add_cost)
# TODO: I could also do a merge earlier on subject and not phase, and then add
# if phase matches. That might look a bit messier but it should be more
# efficient and doesn't introduce NAs. Is this a better solution?
```

Moving to additional parameters we first sample one developer discount rate per subject (i.e. project) in the sertkaya dataset, using the discount rate distribution from the sertkaya source.

```{r, sample-sertkaya-discount-rate}
min <- sertkaya2014_private_discount_rate_min / 100 # Percentage to fraction
max <- sertkaya2014_private_discount_rate_max / 100 # Percentage to fraction
mid <- sertkaya2014_private_discount_rate_mid / 100 # Percentage to fraction
additional <- phases %>%
  select(subject) %>%
  unique %>%
  mutate(priv_discount_rate = rtriangle2(n(), min, max, mid))
```

We then sample a public discount rate and add it as a column to the dataframe.

```{r, sample-public-discount-rate}
min <- public_discount_rate_min
max <- public_discount_rate_max
additional <- additional %>% mutate(publ_discount_rate = runif(n(), min, max))
```

And then do the same for public inefficiencies.

```{r, sample-public-inefficiency}
min <- public_inefficiency_min
max <- public_inefficiency_max
additional <- additional %>% mutate(publ_inefficiency = runif(n(), min, max))
```

Finally we sample and add intervention prize sizes.

```{r, sample-intervention-prize-size}
magn_min <- intervention_dist_magn_min
magn_max <- intervention_dist_magn_max
additional <- additional %>%
  mutate(prize_size = 10 ^ runif(n(), magn_min, magn_max))
```

Below is an excerpt of the resulting dataframe.

```{r, agents-excerpt, echo=FALSE}
kable(head(additional, n=10))
```

And a histogram of the sampled intervention prize sizes.

```{r, prize-size-histogram, echo=FALSE}
additional %>%
  ggplot(aes(prize_size)) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = NULL,
                     labels = trans_format('log10', math_format(10^.x))) +
  geom_histogram()
```



# Valuation
As phase durations are long, the time value of money not only greatly reduces the attractiveness of revenues, but also dampen the pain of costs.
To take the time value of money into account we must make an assumption about how a real world evaluation of a project periodizes future costs, revenues and probabilities.
In other words, how a hypothetical evaluator chooses to transform a sequence of discrete phases into a sequence of discrete and uncertain cashflows.
We make the assumption that the evaluator converts every phase into a series of years and evenly (i.e. constantly) distribute all cashflows and probabilities of success over these years within a phase.

We use the function below to convert from phase-form to year-form.
Note however that we do not interpolate prizes but rather assume that they are introduced as lump sums.

```{r, year-based-conversion}
to_years <- function (phs) {

  # Add timestamps to every observation
  phs <- phs %>%
    group_by(subject) %>%
    arrange(phase) %>%
    mutate(time_to = cumsum(time) - time)

  # Compute cost steps
  cost_step <- (phs$cost / phs$time)
  cost_remainder <- phs$cost - cost_step * floor(phs$time)

  # Compute sales steps
  sales_step <- (phs$sales / phs$time)
  sales_remainder <- phs$sales - sales_step * floor(phs$time)

  # Compute prob steps
  prob_step <- phs$prob ^ (1 / phs$time)
  prob_remainder <- phs$prob / (prob_step ^ floor(phs$time))

  # Compute time steps
  time_step <- 1
  time_remainder <- phs$time - floor(phs$time)

  # Transform: to phase-local years
  phase_years <- tibble()
  for (x in 1:ceiling(max(phs$time) + 1)) { # Longest phase

    # Compute step-based properties
    has_decimals <- phs$time - floor(phs$time) != 0
    whole_step   <- x <= phs$time
    partial_step <- x <= phs$time + 1 & has_decimals

    cost   <- ifelse(whole_step, cost_step, cost_remainder)
    sales  <- ifelse(whole_step, sales_step, sales_remainder)
    prob   <- ifelse(whole_step, prob_step, prob_remainder)
    time   <- ifelse(whole_step, time_step, time_remainder)

    # Make tibble
    year <-
      tibble(src                = phs$src,
             subject            = phs$subject,
             phase_year         = x,
             phase              = phs$phase,
             time,
             cost,
             sales,
             prob
             ) %>% filter(whole_step | partial_step) # Only keep relevant data

    # Append
    phase_years <- bind_rows(phase_years, year)
  }
  phase_years
}
```

And then we convert.

```{r, phasely-to-yearly}
years <- to_years(phases)
```

We have now converted our phase-based data to year-based data.
It should be noted that the function does not distribute the phase-based data over a series of equidistant years.
Data points are only equidistant within a phase, but not necessarily across.
In other words, if P1 entry would occur after 5.3 years then we will distribute PC properties over the 6 first years.
If the duration of P1 is 2.5 years then P2 would start at year 5.3 and end at year 7.8.
While we would divide P1 into three equidistant (years) points (years), that start from year 5.3, 6.3, and 7.3 (respectively) they are not equidistant from the PC steps.


Below is an excerpt of the resulting dataset.

```{r, yearly-excerpt, echo=FALSE}
kable(head(years %>% arrange(subject, phase, phase_year), n=25))
```


## Valuation metrics
We employ the following financial metrics for cashflows:

- Non-capitalized value / Out-of-pocket value
- Risk-adjusted value (rV) / Expected value (EV)
- Capitalized value / Present value (PV)
- Risk-adjusted present value (rPV) / Expected present value (EPV)

When cumulating cashflows, the above financial metrics are respectively known as:

- Cumulative non-capitalized value / Cumulative out-of-pocket value
- Cumulative risk-adjusted value (Cumulative rV) / Cumulative expected value (Cumulative EV)
- Net present value (NPV)
- Risk-adjusted net present value (rNPV) / Expected net present value (ENPV)

We compute **Expected Value (EV)** as:

$$
EV_t = (R_t - C_t) * P_t
$$

where $R_t$ and $C_t$ are the revenues and costs (respectively) at time $t$, and $P_t$ the probability of reaching the cashflow from the point of evaluation.
The probability of reaching a given timestep $t_n$ from a point of evaluation $t_0$ is simply computed as: $P_t = \prod_{t_0}^{t^n}$.
Next, we compute **Present Value (PV)** as:

$$
\mathit{PV}_t = \frac{R_t - C_t}{(1 + i)^t}
$$

where $i$ is the discount rate of the evaluator, and $t$ is the time to the phase from the point of evaluation.
We compute **Expected Present Value (EPV)** as:

$$
\mathit{EPV}_t = \frac{R_t - C_t}{(1 + i)^t} * P_t
$$

Moving on to the cumulative valuations, we compute

**Net Expected Value (ENV)** as:

$$
\mathit{ENV_T} = \sum_{t\in T} EV_t\\
$$

**Net Present Value (NPV)** as:

$$
\mathit{NPV_T} = \sum_{t\in T} \mathit{PV}_t\\
$$

and **Expected Net Present Value (ENPV)** as:

$$
\mathit{ENPV_T} = \sum_{t\in T} \mathit{EPV}_t\\
$$

where $t$ is the time to the phase, and $T$ is the times to all timesteps of all phases for the project in evaluation.

<div class="alert alert-danger">
TODO: Verify that the probability portion of ENPV is actually computed accordingly!
</div>

## Valuation perspectives
To compare the cost of direct and indirect funding we need a way to compute the costs of each.
In order to determine the cost of indirect funding, we need a way to determine which projects will reach a go-decision under a given prize intervention since this will determine which projects the benefactor actually pays for.
Lastly, many argue that planning without respecting market logic inherently involves a certain level of inefficiency and for this reason we must have a way of computing the cost of indirect funding under different inefficiencies.

The above, leads us to the four different perspectives outlined in the table below.

| Perspective        | Summary                                                                                    |
| ------------------ | ------------------------------------------------------------------------------------------ |
| Private            | The private value of a project.                                                            |
| Intervened private | The private value of a project given the existance of a prize intervention.                |
| Indirect           | The negative value of issuing a prize intervention.                                        |
| Direct             | The negative value of paying for a project at-cost.                                        |
| Inefficient direct | The negative value of paying for a project at-cost under some operating inefficiency.      |

Any of the previously outlined valuation metrics can be computed from any of these perspectives.
Computing e.g. ENPV would yield the following valuations:

| Valuation metric          | Summary                                                                                        |
| ------------------------- | ---------------------------------------------------------------------------------------------- |
| Private ENPV              | The expected and capitalized private value of a project.                                                       |
| Intervened private ENPV   | The expected and capitalized private value of a project given the existance of a prize intervention.           |
| Indirect ENPV             | The expected and capitalized negative value of issuing a prize intervention.                                   |
| Direct ENPV               | The expected and capitalized negative value of paying for a project at-cost.                                   |
| Inefficient direct ENPV   | The expected and capitalized negative value of paying for a project at-cost under some operating inefficiency. |

Programatically, we use the function below to compute the valuation metrics for all perspectives at the start of pre-clinical.

```{r, valuation-function}
valuation <- function(df) {
  df %>%
    # Group by subject and intervention
    group_by(subject, prize_phase) %>%
    # Ensure order is correct before computing time to
    arrange(prize_phase, subject, phase, phase_year) %>%
    # Add absolute time (t) column
    mutate(time_to = cumsum(time) - time) %>%
    # Ensure order is based on time_to
    arrange(time_to) %>%
    # Valuation
    mutate(# Helpers
           inef_cost = cost + cost * publ_inefficiency,
           prob_to = cumprod(prob) / prob,
           prize = ifelse(phase_year == 1 & phase == prize_phase, prize_size, 0),
           # Cashflow
           priv_cf       = sales - cost,
           int_priv_cf   = sales - cost + prize,
           ind_cf        = -prize,
           dir_cf        = -cost,
           inef_dir_cf   = -inef_cost,
           # Expected value
           priv_ev       = priv_cf     * prob_to,
           int_priv_ev   = int_priv_cf * prob_to,
           ind_ev        = ind_cf      * prob_to,
           dir_ev        = dir_cf      * prob_to,
           inef_dir_ev   = inef_dir_cf * prob_to,
           # Private value
           priv_pv       = priv_cf     / ((1 + priv_discount_rate) ^ time_to),
           int_priv_pv   = int_priv_cf / ((1 + priv_discount_rate) ^ time_to),
           ind_pv        = ind_cf      / ((1 + publ_discount_rate) ^ time_to),
           dir_pv        = dir_cf      / ((1 + publ_discount_rate) ^ time_to),
           inef_dir_pv   = inef_dir_cf / ((1 + publ_discount_rate) ^ time_to),
           # Expected present value
           priv_epv      = priv_pv * prob_to,
           int_priv_epv  = int_priv_pv * prob_to,
           ind_epv       = ind_pv * prob_to,
           dir_epv       = dir_pv * prob_to,
           inef_dir_epv  = inef_dir_pv * prob_to,
           # Cumulative cashflow
           priv_ccf      = cumsum(priv_cf),
           int_priv_ccf  = cumsum(int_priv_cf),
           ind_ccf       = cumsum(ind_cf),
           dir_ccf       = cumsum(dir_cf),
           inef_dir_ccf  = cumsum(inef_dir_cf),
           # Expected net value
           priv_env      = cumsum(priv_ev),
           int_priv_env  = cumsum(int_priv_ev),
           ind_env       = cumsum(ind_ev),
           dir_env       = cumsum(dir_ev),
           inef_dir_env  = cumsum(inef_dir_ev),
           # Net present value
           priv_npv      = cumsum(priv_pv),
           int_priv_npv  = cumsum(int_priv_pv),
           ind_npv       = cumsum(ind_pv),
           dir_npv       = cumsum(dir_pv),
           inef_dir_npv  = cumsum(inef_dir_pv),
           # Expected net present value
           priv_enpv     = cumsum(priv_epv),
           int_priv_enpv = cumsum(int_priv_epv),
           ind_enpv      = cumsum(ind_epv),
           dir_enpv      = cumsum(dir_epv),
           inef_dir_enpv = cumsum(inef_dir_epv),
           ) %>%
    # Ungroup to avoid surprises when using result
    ungroup
}
```


## Applying valuation
Before we can apply the valuation function we need to:

1. Merge the additional information dataset with the base dataset that we converted to a yearly format.
2. Replicate the full dataset once per treatment type, i.e. once per intervention target phase.

```{r, merge-yearly-and-additional}
valuable <- years %>%
  # Merge with additional information
  merge(., additional, by='subject') %>%
  # Cartesian merge with target phases distribution
  merge(., intervention_phase_dist)
```

We then apply our valuation function.

```{r, valuation}
valued <- valuation(valuable)
```

Since we are mostly interested in the value of bringing a project from start to finish, we will prepare a subset of the `valued` dataset that only contain the last "timestep" of every project.
When only looking at the final value we are only interested in the cumulative valuation metrics and therefore also filter out all CF, PV, EV, and EPV columns.
Further we also remove all timestep specific information.

```{r, finals}
final_valuation <- function(df) {
  df %>%
    group_by(subject, prize_phase) %>%
    filter(time_to == max(time_to)) %>%
    select(-matches('_cf$|_pv$|_ev$|_epv$')) %>%
    select(-phase_year,
           -phase,
           -time,
           -cost,
           -inef_cost,
           -sales,
           -prob,
           -prize) %>%
    ungroup
}

finals <- final_valuation(valued)
```


# Analysis
TODO: Re-add summary statistics here.

## Private ENPV

Private ENPV clearly increase as prize sizes increase, regardless of intervention phase.

```{r, private-enpv-improvement, echo=FALSE}
finals %>%
  ggplot(aes(prize_size, int_priv_enpv - priv_enpv, col=prize_phase)) +
  geom_point(alpha=1, shape=1) +
  facet_wrap(prize_phase ~ src, ncol=6, scale='free') +
  xlab('Prize') +
  ylab('Private ENPV improvement') +
  theme(legend.position = 'top',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm')))
```

When plotting different phase prizes side by side we observe a "layering" where later phases are more costly than earlier ones.
Heteroscedasticity is fixed by plotting log-log.

```{r, layers, echo=FALSE}
finals %>%
  ggplot(aes(prize_size, int_priv_enpv - priv_enpv, color=prize_phase)) +
  geom_smooth(method='lm', se=FALSE) +
  geom_point(shape=1) +
  xlab('Prize') +
  ylab('Private ENPV improvement') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = c(1:9 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = c(1:9 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  facet_wrap(. ~ src) +
  theme(legend.position = 'top')
```

### Predict private ENPV improvement

To confirm this we can build a model that predicts difference in private ENPV from prize size.

```{r, uiyuihvx}
improvement_model <- function(df)
  glm(log10(int_priv_enpv - priv_enpv) ~ log10(prize_size), data = df)

predict_response <- function(fit, df)
  predict(fit, newdata=df, type='response')
```

We run the model once per combination of prize phase and source to avoid having to model interaction effects.
Resulting in the following p-values.

```{r, llsdkfshfjksdf, echo=FALSE, dependson=-1}
improvement_models <- finals %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(improvement_fit = map(data, improvement_model),
         pred_improvement = map2(improvement_fit, data, predict_response),
         tidied = map(improvement_fit, tidy)) %>%
  ungroup

improvement_models %>%
  unnest_legacy(tidied) %>%
  select(src, prize_phase, term, p.value) %>%
  mutate(p.value = format(p.value, scientific=TRUE)) %>%
  spread(prize_phase, p.value) %>%
  kable(caption='P-values')

ends <- improvement_models %>%
  unnest_legacy(tidied) %>%
  mutate(p.value = sapply(p.value, p_lower_than)) %>%
  summarize(max = max(p.value),
            min = min(p.value))
```

Highest p-value is lower than 10^`r ends$max` and lowest lower than 10^`r ends$min`.

```{r, klshj, echo=FALSE, dependson=-1}
improvement_models %>%
  unnest_legacy(data, pred_improvement) %>%
  ggplot(aes(prize_size, 10^pred_improvement, color=prize_phase)) +
  geom_point(aes(y = int_priv_enpv - priv_enpv), alpha=0.5, shape=1) +
  geom_line() +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(seq(0,10,2))),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(seq(0,10,2))),
                     labels = trans_format('log10', math_format(10^.x))) +
  facet_wrap(. ~ src) +
  theme(legend.position = 'top')
```


## Probability of go

By dividing the number of go-decisions under some given range of parameters with the number of projects under that same parameter range we reach the ratio of go/no-decisions.
We will refer to this as P(go) since it also describes the probability of a project reaching a go-decision.
We assume that go-decisions are reached when Private ENPV ≥ 0 or Intervened Private ENPV ≥ 0, before and after interventions respectively.

### Base probability
Without considering interventions.

```{r, distribution-of-go, echo=FALSE}
tmp <- finals %>%
  group_by(src) %>%
  summarize(go = sum(ifelse(priv_enpv >= 0, 1, 0)) / n(),
            no = sum(ifelse(priv_enpv >= 0, 0, 1)) / n()) %>%
  mutate(src = fct_reorder(src, -go)) %>%
  gather('go', 'percentage', -src) %>%
  ungroup

tmp %>%
  arrange(go) %>%
  mutate(go = go == 'go',
         percentage = percentage * 100) %>%
  ggplot(aes(src, percentage, fill=go)) +
  geom_col() +
  geom_text(aes(label=round(percentage, 0)),
                position=position_stack(vjust=0.5)) +
  ylab('%') +
  xlab(element_blank()) +
  labs(fill=element_blank()) +
  scale_y_continuous(breaks = pretty_breaks(n=10))

tmp %>%
  mutate(percentage = percentage * 100) %>%
  spread(go, percentage) %>%
  kable(digits=0, caption='Percentage go/no-go without interventions')
```


### Predict probability of go
Using logistic regression we can predict P(go) from intervention prize size.

Jitter plotting prize size and go-decisions suggests that logistic regression is a reasonable approach to the problem at hand.

```{r, prize-vs-no-to-go, echo=FALSE}
finals %>%
  filter(priv_enpv < 0) %>%
  ggplot(aes(prize_size, int_priv_enpv >= 0, color=src)) +
  geom_jitter(shape=1) +
  facet_grid(cols=vars(src), rows=vars(prize_phase)) +
  ylab('Go\n(Intervened Private ENPV ≥ 0)') +
  xlab('Prize size') +
  ggtitle('Private ENPV < 0') +
  theme(legend.position = 'none') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(seq(0,10,2))),
                     labels = trans_format('log10', math_format(10^.x)))
```

Build model.

```{r, hjlfksjdhf}
go_model <- function(df)
  glm(int_priv_enpv >= 0 ~ log10(prize_size),
      data = df %>% filter(priv_enpv < 0),
      family = binomial(link='logit'))

predict_response <- function(fit, df)
  predict(fit, newdata=df, type='response')
```

We run the model once per combination of prize phase and source to avoid having to model interaction effects.

```{r, lshfjksdf}
go_models <- finals %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(go_fit = map(data, go_model),
         pred_go = map2(go_fit, data, predict_response),
         tidied = map(go_fit, tidy)) %>%
  ungroup

# NOTE: Using legacy unnest because:
# https://github.com/tidyverse/tidyr/issues/694

tidied <- go_models %>%
  unnest_legacy(tidied)
```

```{r, kjhlsdfsd, echo=FALSE}
go_models %>%
  unnest(data, pred_go) %>%
  ggplot(aes(prize_size, pred_go, color=prize_phase)) +
  geom_line() +
  facet_wrap(. ~ src) +
  ylab('P(go)') +
  xlab('Prize size') +
  theme_minimal() +
  theme(legend.position = 'top') +
  labs(color = 'Prize phase') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(seq(0,10,2))),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(breaks = pretty_breaks(n = 11))

ggsave(file = './_figures/predict-go.eps', plot = last_plot())
```

All p-values are significant.

```{r, skjldhfsdf, echo=FALSE}
tidied %>%
  select(src, prize_phase, term, p.value) %>%
  mutate(p.value = format(p.value, scientific=TRUE)) %>%
  spread(prize_phase, p.value)
```

```{r, klhsdfksdkf, echo=FALSE}
tidied %>%
  select(src, prize_phase, term, p.value) %>%
  mutate(p.value = sapply(p.value, p_lower_than)) %>%
  spread(prize_phase, p.value) %>%
  kable(caption='P-values lower than 10^x')

ends <- tidied %>%
  mutate(p.value = sapply(p.value, p_lower_than)) %>%
  summarize(max = max(p.value),
            min = min(p.value))
```

Highest p-value is lower than 10^`r ends$max` and lowest lower than 10^`r ends$min`.


### Predict prize size
To "reverse" the model and predict prize from some given probability we use the following function.

$$
f(y) =
\frac{
  ln(\frac{y}{1-y}) - \beta_0
}{
  \beta_1
}
$$

Which we encode as follows.

```{r, skhjflsdkf}
predict_prize_from_prob <- function(tidied, y) {
  b0 <- filter(tidied, term=='(Intercept)')$estimate
  b1 <- filter(tidied, term=='log10(prize_size)')$estimate
  10^((log(y / (1-y)) - b0) / b1)
}
```

We can sanity check the output of the function by predicting prize size from predicted P(go) and check if it is correlated with the actual prize size that P(go) was predicted from.


```{r, lkhjgdfg, echo=FALSE}
tmp <- go_models %>%
  mutate(pred_prize_size = map2(tidied, pred_go, predict_prize_from_prob)) %>%
  unnest_legacy(pred_prize_size, pred_go, data)
```

```{r, ashjldfsdj, echo=FALSE}
max_prob <- 0.99999
tmp %>%
  filter(pred_go < max_prob) %>%
  ggplot(aes(prize_size, pred_prize_size)) +
  geom_line() +
  facet_wrap(prize_phase ~ src, scale='free') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  xlab('Prize size') +
  ylab('Predicted prize size') +
  theme(legend.position = 'none',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm'))) +
  ggtitle(sprintf('where P(go) < %s', max_prob))
```

Note that (not evident in the plot above is that) predicted prize size becomes a constant as y reaches 100%.
Therefore significant differences between predicted and actual prize appear and grow when approximating P(go) = 1 since we have sampled prizes much larger than prizes required to yield 100% go but when predicting required prize sizes to achieve 100% go will always yield the same prize size.

```{r, kjsdfhlksjdf, echo=FALSE}
tmp %>%
  filter(pred_go < max_prob) %>%
  mutate(diff = prize_size - pred_prize_size) %>%
  ggplot(aes(pred_go*100, diff)) +
  geom_point(shape=1) +
  facet_wrap(. ~ prize_phase, nrow=1) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(breaks=pretty_breaks(n=20)) +
  theme(axis.text.x = element_text(angle=90, hjust=1)) +
  ggtitle(sprintf('where P(go) < %s', max_prob))
```

```{r, sdlfhsdklfhsd, echo=FALSE}
tmp %>%
  ggplot(aes(prize_size, pred_go*100, group=prize_phase)) +
  geom_line(color='black', size=2) +
  geom_line(aes(x = pred_prize_size,
                y = pred_go*100),
            color = 'white',
            size = 0.7) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  xlab('Prize size') +
  ylab('P(go)') +
  facet_wrap(.~src) +
  coord_cartesian(xlim = c(10^6.7, 10^10)) +
  ggtitle(sprintf('Sanity checking of reverse prediction\n(white = predict x from predicted y)\nwhere P(go) < %s', max_prob)) +
  theme(legend.position = 'bottom')
```


### Effective prize sizes{#effective-prize-sizes}
Using our function that solves for prize size from a given probability of go we can predict what prize sizes are required to achive a target probability of go of `r target=0.95; target*100`% in each of the sources?

```{r, klhjsdfkdf}
effectives <- tidied %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(pred_prize_size = map2_dbl(data, target, predict_prize_from_prob)) %>%
  select(-data) %>%
  ungroup
```

```{r, alslflhjsdfkjhs, echo=FALSE}
effectives %>%
  ggplot(aes(src, pred_prize_size/10^6, fill=src)) +
  geom_col(position='dodge') +
  facet_wrap(. ~ prize_phase, nrow=1, scale='free_x') +
  scale_y_continuous(breaks = pretty_breaks(n = 20)) +
  ylab('Prize size\n(million usd)') +
  xlab(element_blank()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = 'none') +
  ggtitle(sprintf('P(go) = %s%%', target*100))
```

If prizes are not targeted by indication we must choose the highest prize to ensure a minimum of `r target*100`% go-decisions for all indications.
Hence the "max" row.

```{r, hjklfsdf, echo=FALSE}
effectives %>%
  group_by(prize_phase) %>%
  mutate(src = '(max)') %>%
  summarize(pred_prize_size = max(pred_prize_size),
            src = '(max)') %>%
  rbind(effectives, .) %>%
  mutate(pred_prize_size = round(pred_prize_size / 10^6)) %>%
  spread(prize_phase, pred_prize_size) %>%
  kable(caption=sprintf('Million usd (rounded). P(go) = %s%%.', target*100))
```



## Indirect ENPV
The cost of issuing an intervention depends on both prize size and prize phase.
When computing the cost of an intervention we only include projects that actually reach go-decisions after (even if go was reached before) the introduction of the intervention since the benefactor will only ever (possibly) pay for projects that are actually undertaken.

```{r, prize-vs-indirect-enpv-excluding-nos, echo=FALSE}
finals %>%
  filter(int_priv_enpv >= 0) %>%
  ggplot(aes(prize_size, -ind_enpv, color=prize_phase)) +
  geom_point(shape=1) +
  geom_smooth(method='lm', se=FALSE) +
  facet_wrap(. ~ src) +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = c(1:9 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = c(1:9 %o% 10^(0:10)),
                     labels = trans_format('log10', math_format(10^.x))) +
  xlab('Prize size') +
  ylab('Indirect ENPV') +
  ggtitle('Intervened Private ENPV ≥ 0') +
  theme(legend.position = 'bottom')
```

We will only look at go-rates lower than `r go_lim_up=0.99; go_lim_up*100`%.
Since a go-rate higher than 100% is not achievable, any prize size larger than the lowest size that yields a go-rate of 100% will also merely yield 100%.

```{r, sxhkldflkjshsf, echo=FALSE}
for (curr in unique(go_models$prize_phase)) {
  p <- go_models %>%
    filter(prize_phase == curr) %>%
    unnest_legacy(data, pred_go) %>%
    filter(pred_go < go_lim_up) %>%
    filter(int_priv_enpv >= 0) %>%
    ggplot(aes(pred_go*100, -ind_enpv)) +
    geom_point(shape=1) +
    geom_smooth(method=lm, formula=y ~ splines::bs(x, 8), se=FALSE, color='red') +
    xlab('P(go)') +
    ylab('Indirect ENPV per entry\n(million usd)') +
    facet_wrap(. ~ src, scale='free') +
    ggtitle(sprintf('Prize phase = %s    Intervened Private ENPV ≥ 0     P(go) < %s', curr, go_lim_up)) +
    theme(legend.position = 'bottom')
  print(p)
}
```

Smoothing the curves and plotting log-log we can more easily compare the expected costs of achieving a target P(go) between phases.

```{r, go-rate-vs-indirect-enpv-smooth, echo=FALSE}
go_models %>%
  unnest_legacy(data, pred_go) %>%
  filter(pred_go < go_lim_up) %>%
  filter(int_priv_enpv >= 0) %>%
  ggplot(aes(pred_go*100, -ind_enpv, color=prize_phase)) +
  geom_smooth(method=lm, formula=y ~ splines::bs(x, 8), se=FALSE) +
  scale_linetype_manual(values=c('dotted', 'solid')) +
  facet_wrap(. ~ src, scales='free') +
  xlab('P(go)') +
  ylab('Indirect ENPV\n(million usd)') +
  ggtitle('Intervened Private ENPV ≥ 0  (per go-decision)') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = NULL,
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = NULL,
                     labels = trans_format('log10', math_format(10^.x))) +
  theme(legend.position = 'bottom', legend.title = element_blank())
```

```{r, go-rate-vs-exit, echo=FALSE}
go_models %>%
  unnest_legacy(data, pred_go) %>%
  filter(pred_go < go_lim_up) %>%
  filter(int_priv_enpv >= 0) %>%
  ggplot(aes(pred_go*100, -ind_enpv/prob_to, color=prize_phase)) +
  geom_smooth(method=lm, formula=y ~ splines::bs(x, 8), se=FALSE) +
  facet_wrap(. ~ src, scales='free', nrow=2) +
  xlab('P(go)') +
  ylab('Indirect ENPV\n(million usd)') +
  ggtitle('Intervened Private ENPV ≥ 0    (per market entry)') +
  scale_x_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = NULL,
                     labels = trans_format('log10', math_format(10^.x))) +
  scale_y_continuous(trans = 'log',
                     breaks = c(1 %o% 10^(0:10)),
                     minor_breaks = NULL,
                     labels = trans_format('log10', math_format(10^.x))) +
  theme(legend.position = 'bottom')
```


## Direct ENPV

```{r, inefficiency-vs-direct-enpv-per-entry, echo=FALSE}
finals %>%
  ggplot(aes(publ_inefficiency*100, -inef_dir_enpv, color=src)) +
  geom_point(shape=1) +
  geom_smooth(method='lm', se=FALSE, color='black') +
  facet_wrap(. ~ src) +
  xlab('Public inefficiency (%)') +
  ylab('Direct ENPV\n(million usd)') +
  ggtitle('per go-decision') +
  scale_y_continuous(label = unit_format(scale = 1e-6, unit = '')) +
  theme(legend.position='none')
```

```{r, inefficiency-vs-direct-enpv-per-exit, echo=FALSE}
finals %>%
  ggplot(aes(publ_inefficiency*100, -inef_dir_enpv/prob_to, color=src)) +
  geom_point(shape=1) +
  geom_smooth(method='lm', se=FALSE, color='black') +
  facet_wrap(. ~ src) +
  xlab('Public inefficiency (%)') +
  ylab('Direct ENPV\n(million usd)') +
  ggtitle('per market entry') +
  scale_y_continuous(label = unit_format(scale = 1e-6, unit = '')) +
  theme(legend.position='none')
```


## Probability of preferable direct
Predicting whether direct is cheaper than indirect, let's call it P(direct), at no public inefficiency.

```{r, direct_preferble_jitter, echo=FALSE}
go_models %>%
  unnest_legacy(data, pred_go) %>%
  mutate(dir = (dir_enpv - ind_enpv) > 0) %>%
  ggplot(aes(pred_go, dir, color=src)) +
    facet_grid(cols=vars(src), rows=vars(prize_phase)) +
    geom_jitter(shape=1) +
    xlab('P(go)') +
    ylab('P(direct)') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = 'none')
```

```{r, dir_models}
dir_model_logit <- function (df)
  glm((dir_enpv - ind_enpv) > 0 ~ pred_go, data = df, family = binomial(link='logit'))

dir_models <- go_models %>%
  unnest_legacy(data, pred_go) %>%
  # Only compare cost diff in projs with go-deciison after intervention
  filter(int_priv_enpv >= 0) %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(dir_fit = map(data, dir_model_logit),
         pred_dir = map2(dir_fit, data, predict_response),
         tidied_dir = map(dir_fit, tidy)) %>%
  ungroup
```

```{r, plot_dir_models, echo=FALSE, dependson=-1}
dir_models %>%
  unnest_legacy(data, pred_dir) %>%
  ggplot(aes(pred_go, pred_dir, color=prize_phase)) +
    geom_line() +
    facet_wrap(. ~ src) +
    xlab('P(go)') +
    ylab('P(direct)') +
    labs(color = 'Prize phase') +
    scale_x_continuous(breaks = pretty_breaks(n=8)) +
    scale_y_continuous(breaks = pretty_breaks(n=11)) +
    coord_cartesian(xlim = c(0.3, 1)) +
    theme_minimal() +
    theme(legend.position = 'top')
```

P-values:
```{r, dir_models_p, echo=FALSE, dependson=-1}
dir_models %>%
  unnest(tidied_dir) %>%
  select(src, prize_phase, term, p.value) %>%
  spread(prize_phase, p.value)
```

Coefficients:
```{r, dir_models_coef, echo=FALSE, dependson=-1}
dir_models %>%
  unnest(tidied_dir) %>%
  select(src, prize_phase, term, estimate) %>%
  spread(prize_phase, estimate) %>% kable
```

```{r, dir_models_ends, echo=FALSE, dependson=-1}
ps <- dir_models %>% unnest(tidied_dir) %>% select(p.value)
preds <- dir_models %>% unnest(pred_dir) %>% select(pred_dir)
```

Highest and lowest p-values are `r max(ps)` and `r min(ps)` respectively.

Lowest and highest predicted values are `r min(preds)` and `r max(preds)` respectively.


### Including public inefficiency
```{r, dir_models_with_inef}
dir_model_logit <- function (df)
  glm((inef_dir_enpv - ind_enpv) > 0 ~ pred_go + publ_inefficiency, data = df, family = binomial(link='logit'))

dir_models <- go_models %>%
  unnest_legacy(data, pred_go) %>%
  # Only compare cost diff in projs with go-deciison after intervention
  filter(int_priv_enpv >= 0) %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(dir_fit = map(data, dir_model_logit),
         pred_dir = map2(dir_fit, data, predict_response),
         tidied_dir = map(dir_fit, tidy)) %>%
  ungroup
```

```{r, plot_dir_models_with_inef, echo=FALSE, dependson=-1}
dir_models %>%
  unnest_legacy(data, pred_dir) %>%
  ggplot(aes(pred_go, pred_dir, color=prize_phase)) +
    geom_point(shape=1) +
    facet_wrap(. ~ src) +
    xlab('P(go)') +
    ylab('P(direct)') +
    labs(color = 'Prize phase') +
    scale_x_continuous(breaks = pretty_breaks(n=8)) +
    scale_y_continuous(breaks = pretty_breaks(n=11)) +
    coord_cartesian(xlim = c(0.3, 1)) +
    theme_minimal() +
    theme(legend.position = 'top')
```

P-values:
```{r, dir_models_with_inef_p, echo=FALSE, dependson=-1}
dir_models %>%
  unnest(tidied_dir) %>%
  select(src, prize_phase, term, p.value) %>%
  spread(prize_phase, p.value)
```

Coefficients:
```{r, dir_models_with_inef_coef, echo=FALSE, dependson=-1}
dir_models %>%
  unnest(tidied_dir) %>%
  select(src, prize_phase, term, estimate) %>%
  spread(prize_phase, estimate) %>% kable
```

```{r, dir_models_with_inef_ends, echo=FALSE, dependson=-1}
ps <- dir_models %>% unnest(tidied_dir) %>% select(p.value)
preds <- dir_models %>% unnest(pred_dir) %>% select(pred_dir)
```

Highest and lowest p-values are `r max(ps)` and `r min(ps)` respectively.

Lowest and highest predicted values are `r min(preds)` and `r max(preds)` respectively.

Prepare function for predicting P(go) from inefficiency and target P(direct):
```{r, dir_models_backwards_pred, dependson=-1}
tidied_dir_models <- dir_models %>%
  select(src, prize_phase, tidied_dir) %>%
  unnest(tidied_dir)

predict_p_go_from_prob_and_inef <- function(s, p, x2, y) {
  sub <- filter(tidied_dir_models, src==s & prize_phase==p)
  b0 <- filter(sub, term=='(Intercept)')$estimate
  b1 <- filter(sub, term=='pred_go')$estimate
  b2 <- filter(sub, term=='publ_inefficiency')$estimate
  (log(y / (1-y)) - b0 - b2 * x2) / b1
}
```

### Heatmaps
Generate independent variables.
```{r, dir_models_new_df, dependson=-1}
steps <- 100
x <- data.frame(pred_go = seq(0, 1, length.out=steps))
y <- data.frame(publ_inefficiency = seq(public_inefficiency_min,
                                        public_inefficiency_max,
                                        length.out=steps))
variations <- merge(x, y) # Cartesian product
```

Predict dependent variable.
```{r, dir_models_new_df_pred, dependson=-1}
span <- dir_models %>%
  select(src, prize_phase, dir_fit) %>%
  unique %>%
  mutate(data = list(variations),
         pred_dir = map2(dir_fit, data, predict_response)) %>%
  unnest_legacy(data, pred_dir)
```

Find what P(go) yields P(direct) = `r target_dir=0.5; target_dir` under various inefficiencies.
```{r, backwards_predict_when_ind_and_dir_is_even, dependson=-1}
steps <- 10
evens <-
  merge(intervention_phase_dist, data.frame(src = INDICATIONS)) %>%
  merge(., data.frame(publ_inefficiency = seq(0, 1, length.out=steps))) %>%
  mutate(pred_dir = target_dir) %>%
  mutate(pred_pred_go = mapply(predict_p_go_from_prob_and_inef, src, prize_phase, publ_inefficiency, pred_dir))
```

```{r, dir_models_heatmaps, dependson=-1, echo=FALSE}
for (curr in unique(span$prize_phase)) {
  p <- span %>%
    filter(prize_phase == curr) %>%
    ggplot(aes(pred_go, publ_inefficiency, z=pred_dir)) +
      facet_wrap(. ~ src) +
      geom_tile(aes(fill=pred_dir)) +
      scale_fill_gradient2(low='coral3', high='dodgerblue3', mid='white', midpoint=0.5,
                           name='P(direct)', breaks=pretty_breaks(n=7)) +
      geom_line(data = filter(evens, prize_phase==curr),
                aes(pred_pred_go, publ_inefficiency), linetype='dotted', color='black') +
      ylab('Public inefficiency') +
      xlab('P(go)') +
      ggtitle(curr) +
      scale_x_continuous(breaks = pretty_breaks(n=5)) +
      coord_cartesian(xlim = c(0.5, 1)) +
      theme(legend.position = 'right',
        panel.spacing = unit(1, 'mm'))
  print(p)
}
```

### Breakpoints
At what combination of P(go) and inefficiency is there no difference between direct and indirect?

Caption:
Given some public inefficiency (X), what is the maximum P(go) (Y) that a prize can target before direct funding is equally likely to be cheaper, i.e. before P(direct) = 50\%.
At P(go) below this point indirect funding is likely to be cheaper, and above this point direct funding is likely to be cheaper.

```{r, dklsdhfsdf, echo=FALSE, dependson=-1}
evens %>%
  ggplot(aes(publ_inefficiency, pred_pred_go, color=src)) +
  geom_line() +
  facet_wrap(. ~ prize_phase, nrow=1) +
  labs(color = '') +
  ylab('P(go)') +
  xlab('Public inefficiency') +
  scale_x_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1)) +
  scale_y_continuous(breaks = pretty_breaks(n = 10)) +
  theme_minimal() +
  theme(legend.position = 'top',
        axis.text.x = element_text(angle=90, vjust=0.5, hjust=1),
        panel.border = element_rect(color = 'grey', fill = NA, size = 0.5),
        panel.spacing = unit(0.9, 'lines')
        )

ggsave(file = './_figures/predict-dir-1.eps', plot = last_plot())

evens %>%
  ggplot(aes(publ_inefficiency, pred_pred_go, color=prize_phase)) +
  geom_line() +
  facet_wrap(. ~ src) +
  labs(color = '') +
  ylab('P(go)') +
  xlab('Public inefficiency') +
  scale_x_continuous(breaks = pretty_breaks(n = 11)) +
  scale_y_continuous(breaks = pretty_breaks(n = 10)) +
  theme_minimal() +
  theme(legend.position = 'top',
        axis.text.x = element_text(angle=90, vjust=0.5, hjust=1),
        panel.border = element_rect(color = 'grey', fill = NA, size = 0.5),
        panel.spacing = unit(0.9, 'lines')
        )

ggsave(file = './_figures/predict-dir-2.eps', plot = last_plot())
```



## Cost difference (parametric)
Flip P(go) to P(no) and build power law model.

TODO: Also try logit in glm with `log(diff_norm) ~ pred_go`.

TODO: Would it be better for the non-linear models to actually include very high P(go) and not filter them out as I now do?

Build models:
```{r, diff_models_parametric}
# Normalization
nmin <- function (xs)
  min(xs) + 1
normalize <- function(x, xs)
  (x + abs(nmin(xs))) / (max(xs) - nmin(xs))
denormalize <- function(x, xs)
  (x * (max(xs) - nmin(xs)) - abs(nmin(xs)))

# Model: power law
pwr_model <- function (df)
  nls(diff_norm ~ a*pred_no^b, data=df,
      start=list(a=1, b=-1),
      control=nls.control(maxiter=1000))

# Model: Linear in log-log
lm_model <- function (df)
  glm(log(diff_norm) ~ log(pred_no), data=df)
```

Apply models:
```{r, diff_models_parametric_apply, dependson=-1}
diff_models_parametric <- go_models %>%
  unnest_legacy(pred_go, data) %>%
  # Remove high P(go)
  filter(pred_go <= go_lim_up) %>%
  # Only compare cost diff in projs with go-decison after intervention
  filter(int_priv_enpv >= 0) %>%
  # Prepare independent variables
  mutate(pred_no = 1 - pred_go, # P(no) = 1 - P(go)
         diff = dir_enpv - ind_enpv,
         diff_norm = normalize(diff, diff)) %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(# Power law
         diff_pwr_fit = map(data, pwr_model),
         pred_diff_pwr = map2(diff_pwr_fit, data, predict_response),
         tidied_diff_pwr = map(diff_pwr_fit, tidy),
         # Linear in log-log
         diff_lm_fit = map(data, lm_model),
         pred_diff_lm = map2(diff_lm_fit, data, predict_response),
         tidied_diff_lm = map(diff_lm_fit, tidy)) %>%
  ungroup
```

### Model output
```{r, diff_models_parametric_tables, echo=FALSE, dependson=-1}
diff_models_parametric %>%
  select(src, prize_phase, tidied_diff_pwr) %>%
  unnest(tidied_diff_pwr) %>%
  mutate(p.value = sprintf('< 10^%s', sapply(p.value, p_lower_than))) %>%
  kable(caption = 'Power law')

diff_models_parametric %>%
  select(src, prize_phase, tidied_diff_lm) %>%
  unnest(tidied_diff_lm) %>%
  mutate(p.value = sprintf('< 10^%s', sapply(p.value, p_lower_than))) %>%
  kable(caption = 'Linear in log-log')
```

### Plots
```{r, diff_models_parametric_plots, echo=FALSE, dependson=-1}
for (curr_src in unique(diff_models_parametric$src)) {
  for (curr_ph in unique(diff_models_parametric$prize_phase)) {
    sub <- diff_models_parametric %>%
      filter(src == curr_src & prize_phase == curr_ph) %>%
      unnest_legacy(data, pred_diff_lm, pred_diff_pwr)

    p1 <- sub %>%
      ggplot(aes(pred_no, denormalize(diff_norm, diff))) +
      facet_wrap(prize_phase ~ src, scale='free') +
      geom_point(aes(color=denormalize(diff_norm, diff) > 0)) +
      geom_line(aes(y=denormalize(pred_diff_pwr, diff)), color='red') +
      geom_line(aes(y=denormalize(exp(pred_diff_lm), diff)), color='blue') +
      scale_x_continuous(breaks = pretty_breaks(n=11)) +
      scale_y_continuous(breaks = pretty_breaks(n=11)) +
      ylab('Cost difference') +
      xlab('P(no)') +
      ggtitle(sprintf('Filter: P(go) <= %s', go_lim_up)) +
      theme(legend.position = 'none')

    p2 <- sub %>%
      ggplot(aes(pred_no, diff_norm)) +
      facet_wrap(prize_phase ~ src, scale='free') +
      geom_point(aes(color=diff_norm > 0)) +
      geom_line(aes(y=pred_diff_pwr), color='red') +
      geom_line(aes(y=exp(pred_diff_lm)), color='blue') +
      scale_y_continuous(trans = 'log',
                         breaks = c(1 %o% 10^(0:10)),
                         minor_breaks = NULL,
                         labels = trans_format('log10', math_format(10^.x))) +
      scale_x_continuous(trans = 'log',
                         breaks = c(1 %o% 10^(0:10)),
                         minor_breaks = NULL,
                         labels = trans_format('log10', math_format(10^.x))) +
      ylab('Normalized cost difference') +
      xlab('P(no)') +
      ggtitle(sprintf('Filter: P(go) <= %s', go_lim_up)) +
      theme(legend.position = 'none')

    print(p1)
    print(p2)
  }
}
```

## Cost difference (non-parametric)
TODO: Normalization causes zeroes. Normalize to [1,2] instead of [0,1]?

Build models:
```{r, diff_nonparametric_models}
normalize <- function(x, xs)
  (x + abs(min(xs))) / (max(xs) - min(xs))
denormalize <- function(x, xs)
  (x * (max(xs) - min(xs)) - abs(min(xs)))

# Cubic spline?
# https://datascienceplus.com/cubic-and-smoothing-splines-in-r/
diff_model_spline <- function (df)
  lm(diff_norm ~ bs(pred_go, knots=c(0.8, 0.95)), data=df)
  # What works?
  # (0.9)
  # (0.8, 0.95)
  # (0.8, 0.9, 0.95, 0.98)
  # (0.8, 0.85, 0.9, 0.95, 0.965, 0.98) too much wiggle

# Cubic spline?
# https://datascienceplus.com/cubic-and-smoothing-splines-in-r/
# TODO: No knots specified
diff_model_logspline <- function (df)
  lm(diff_norm ~ bs(log(pred_no), knots=c(log(0.8), log(0.95))), data=df)

diff_model_gam <- function (df)
  gam(diff_norm ~ s(pred_go), data=df, method='REML')
```

Apply models:
```{r, diff_nonparametrics_apply_models, dependson=-1}
diff_models_nonparametric <- go_models %>%
  unnest_legacy(pred_go, data) %>%
  # Remove high P(go)
  filter(pred_go <= go_lim_up) %>%
  # Only compare cost diff in projs with go-decison after intervention
  filter(int_priv_enpv >= 0) %>%
  # Prepare independent variables
  mutate(pred_no = 1 - pred_go, # P(no) = 1 - P(go)
         diff = dir_enpv - ind_enpv,
         diff_norm = normalize(diff, diff)) %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(# Spline
         diff_spline_fit = map(data, diff_model_spline),
         pred_diff_spline = map2(diff_spline_fit, data, predict_response),
         tidied_diff_spline = map(diff_spline_fit, tidy),
         # Spline in log-log
         diff_logspline_fit = map(data, diff_model_logspline),
         pred_diff_logspline = map2(diff_logspline_fit, data, predict_response),
         tidied_diff_logspline = map(diff_logspline_fit, tidy),
         # GAM
         diff_gam_fit = map(data, diff_model_gam),
         pred_diff_gam = map2(diff_gam_fit, data, predict_response),
         tidied_diff_gam = map(diff_gam_fit, tidy)) %>%
  ungroup
```

### Model output
```{r, diff_model_nonparametric_tables, echo=FALSE}
diff_models_nonparametric %>%
  select(src, prize_phase, tidied_diff_spline) %>%
  unnest_legacy(tidied_diff_spline) %>%
  mutate(p.value = sprintf('< 10^%s', sapply(p.value, p_lower_than))) %>%
  kable(caption = 'Spline')

diff_models_nonparametric %>%
  select(src, prize_phase, tidied_diff_logspline) %>%
  unnest_legacy(tidied_diff_logspline) %>%
  mutate(p.value = sprintf('< 10^%s', sapply(p.value, p_lower_than))) %>%
  kable(caption = 'Spline in log-log')

diff_models_nonparametric %>%
  select(src, prize_phase, tidied_diff_gam) %>%
  unnest_legacy(tidied_diff_gam) %>%
  mutate(p.value = sprintf('< 10^%s', sapply(p.value, p_lower_than))) %>%
  kable(caption = 'GAM')
```

### Plots
```{r, diff_model_nonparametric_figs, echo=FALSE}
for (curr_src in unique(diff_models_nonparametric$src)) {
  for (curr_ph in unique(diff_models_nonparametric$prize_phase)) {
    sub <- diff_models_nonparametric %>%
      unnest_legacy(data, pred_diff_spline, pred_diff_logspline, pred_diff_gam) %>%
      filter(src == curr_src & prize_phase == curr_ph) %>%
      # Remove high P(go) samples
      filter(pred_go <= go_lim_up) %>%
      # Only compare cost diff in projs with go-deciison after intervention
      filter(int_priv_enpv >= 0)
    p1 <- sub %>%
      ggplot(aes(pred_go, diff_norm)) +
      facet_wrap(prize_phase ~ src, scale='free') +
      geom_point(aes(color=diff_norm > 0)) +
      geom_line(aes(y=pred_diff_logspline), color='black', size=1) +
      geom_line(aes(y=pred_diff_spline), color='yellow', linetype='dashed', size=1) +
      geom_line(aes(y=pred_diff_gam), color='black', linetype='dotted', size=1) +
      scale_x_continuous(breaks = pretty_breaks(n=11)) +
      scale_y_continuous(trans = 'log',
                         breaks = c(1 %o% 10^(0:10)),
                         minor_breaks = NULL,
                         labels = trans_format('log10', math_format(10^.x))) +
      ylab('Cost difference') +
      xlab('P(go)') +
      ggtitle(sprintf('Filter: P(go) <= %s', go_lim_up)) +
      theme(legend.position = 'none')
    p2 <- sub %>%
      ggplot(aes(1-pred_go, diff_norm)) +
      facet_wrap(prize_phase ~ src, scale='free') +
      geom_point(aes(color=diff_norm > 0)) +
      geom_line(aes(y=pred_diff_logspline), color='black', size=1) +
      geom_line(aes(y=pred_diff_spline), color='yellow', linetype='dashed', size=1) +
      geom_line(aes(y=pred_diff_gam), color='black', linetype='dotted', size=1) +
      scale_x_continuous(trans = 'log',
                         breaks = c(1 %o% 10^(0:10)),
                         minor_breaks = NULL,
                         labels = trans_format('log10', math_format(10^.x))) +
      scale_y_continuous(trans = 'log',
                         breaks = c(1 %o% 10^(0:10)),
                         minor_breaks = NULL,
                         labels = trans_format('log10', math_format(10^.x))) +
      ylab('Cost difference') +
      xlab('P(no)') +
      ggtitle(sprintf('Filter: P(go) <= %s', go_lim_up)) +
      theme(legend.position = 'none')
    print(p1)
    print(p2)
  }
}
```



## Cost difference (linear)

NOTE: This is the old cost difference prediction from before I realized that indirect ENPV is non-linearly correlated to P(go).

```{r, lkjkj1hghjk}
diff_model <- function(df)
  glm(inef_dir_enpv - ind_enpv ~ publ_inefficiency + pred_go, data = df)

market_diff_model <- function(df)
  glm((inef_dir_enpv - ind_enpv) / prob_to ~ publ_inefficiency + pred_go, data = df)

diff_models <- go_models %>%
  unnest_legacy(data, pred_go) %>%
  # Only compare cost diff in projs with go-deciison after intervention
  filter(int_priv_enpv >= 0) %>%
  # Remove large P(go)
  filter(pred_go < go_lim_up) %>%
  group_by(src, prize_phase) %>%
  nest %>%
  mutate(diff_fit = map(data, diff_model),
         pred_diff = map2(diff_fit, data, predict_response),
         tidied_diff = map(diff_fit, tidy),
         market_diff_fit = map(data, market_diff_model),
         pred_market_diff = map2(market_diff_fit, data, predict_response),
         tidied_market_diff = map(market_diff_fit, tidy)) %>%
  ungroup
```

### P-values
All p-values are significant.

#### P-values of go-decision model
```{r, akhjlshlkdsf, dependson=-1, echo=FALSE}
diff_models %>%
  unnest_legacy(tidied_diff) %>%
  select(src, prize_phase, term, p.value) %>%
  spread(prize_phase, p.value)

diff_models %>%
  unnest_legacy(tidied_diff) %>%
  select(src, prize_phase, term, p.value) %>%
  mutate(p.value = sapply(p.value, p_lower_than)) %>%
  spread(prize_phase, p.value) %>%
  kable(caption='P-value is lower than 10^x')

ends <- diff_models %>%
  unnest_legacy(tidied_diff) %>%
  mutate(p.value = sapply(p.value, p_lower_than)) %>%
  summarize(max = max(p.value),
            min = min(p.value))
```

Highest p-value is lower than 10^`r ends$max` and lowest lower than 10^`r ends$min`.


#### P-values of market entry model
```{r, akhjlshlkdsf2, dependson=-1, echo=FALSE}
diff_models %>%
  unnest_legacy(tidied_market_diff) %>%
  select(src, prize_phase, term, p.value) %>%
  spread(prize_phase, p.value)

diff_models %>%
  unnest_legacy(tidied_market_diff) %>%
  select(src, prize_phase, term, p.value) %>%
  mutate(p.value = sapply(p.value, p_lower_than)) %>%
  spread(prize_phase, p.value) %>%
  kable(caption='P-value is lower than 10^x')

ends <- diff_models %>%
  unnest_legacy(tidied_market_diff) %>%
  mutate(p.value = sapply(p.value, p_lower_than)) %>%
  summarize(max = max(p.value),
            min = min(p.value))
```

Highest p-value is lower than 10^`r ends$max` and lowest lower than 10^`r ends$min`.




### Two-dimensional plotting
#### 2D-plotting of go-decision model
```{r, jlshdfk, dependson=-1, echo=FALSE}
diff_models %>%
  unnest_legacy(pred_diff, data) %>%
  ggplot(aes(pred_go, pred_diff)) +
  geom_point(shape=1) +
  geom_point(aes(y = inef_dir_enpv - ind_enpv), color='red', shape=1, alpha=0.5) +
  facet_wrap(prize_phase ~ src, scale='free') +
  theme(legend.position = 'none',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm')))

diff_models %>%
  unnest_legacy(pred_diff, data) %>%
  ggplot(aes(publ_inefficiency, pred_diff)) +
  geom_point(shape=1) +
  facet_wrap(prize_phase ~ src, scale='free') +
  theme(legend.position = 'none',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm')))
```

#### 2D-plotting of market entry model
```{r, jlshdfk2, dependson=-1, echo=FALSE}
diff_models %>%
  unnest_legacy(pred_market_diff, data) %>%
  ggplot(aes(pred_go, pred_market_diff)) +
  geom_point(shape=1) +
  facet_wrap(prize_phase ~ src, scale='free') +
  theme(legend.position = 'none',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm')))

diff_models %>%
  unnest_legacy(pred_market_diff, data) %>%
  ggplot(aes(publ_inefficiency, pred_market_diff)) +
  geom_point(shape=1) +
  facet_wrap(prize_phase ~ src, scale='free') +
  theme(legend.position = 'none',
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        panel.spacing = unit(0, 'cm'),
        strip.text.x = element_text(margin = margin(0, 0, 0, 0, 'cm')),
        strip.text.y = element_text(margin = margin(0, 0, 0, 0, 'cm')))
```


### Difference heatmap
Using the above model we can generate a dataset of independent variables spanning the full range from 0 to 1 and then predict the cost difference for all points.
From this dataset and its predictions we can begin exploring these three-dimensional relationships.

```{r, sjhlfsdf}
steps <- 21
x <- data.frame(pred_go = seq(0, 1, length.out=steps))
y <- data.frame(publ_inefficiency = seq(public_inefficiency_min,
                                        public_inefficiency_max,
                                        length.out=steps))
# Build cartesian product of independent variables
variations <- merge(x, y)
```

Predict dependent variable, i.e. the cost difference between direct and indirect funding, from the broad range of independent variables generated above.

```{r, alhlkjsadhf, dependson=-1}
span <- diff_models %>%
  select(src, prize_phase, diff_fit, market_diff_fit) %>%
  group_by(src, prize_phase) %>%
  mutate(data = list(variations),
         pred_diff = map2(diff_fit, data, predict_response),
         pred_market_diff = map2(market_diff_fit, data, predict_response)) %>%
  ungroup
```

The predictions yield the following heatmaps.

#### Go-decision heatmaps
```{r, jkhflsf, dependson=-1, echo=FALSE}
for (curr in unique(tmp$prize_phase)) {
  p <- span %>%
    filter(prize_phase == curr) %>%
    unnest_legacy(data, pred_diff) %>%
    ggplot(aes(pred_go*100, publ_inefficiency*100, z=pred_diff/10^6)) +
      facet_wrap(. ~ src) +
      geom_tile(aes(fill=pred_diff/10^6)) +
      #geom_contour(bins=steps, color='black', alpha=0.25) +
      scale_fill_gradient2(low='blue', high='red', mid='white', name='Diff', breaks=pretty_breaks(n=7)) +
      ylab('Public inefficiency') +
      xlab('P(go)') +
      ggtitle(curr) +
      theme(legend.position = 'right',
        panel.spacing = unit(1, 'mm'))
  print(p)
}
```

#### Market entry heatmaps
```{r, jkhflsf2, dependson=-1, echo=FALSE}
for (curr in unique(tmp$prize_phase)) {
  p <- span %>%
    filter(prize_phase == curr) %>%
    unnest_legacy(data, pred_market_diff) %>%
    ggplot(aes(pred_go*100, publ_inefficiency*100, z=pred_market_diff/10^6)) +
      facet_wrap(. ~ src) +
      geom_tile(aes(fill=pred_market_diff/10^6)) +
      #geom_contour(bins=steps, color='black', alpha=0.25) +
      scale_fill_gradient2(low='blue', high='red', mid='white', name='Diff', breaks=pretty_breaks(n=7)) +
      ylab('Public inefficiency') +
      xlab('P(go)') +
      ggtitle(curr) +
      theme(legend.position = 'right',
        panel.spacing = unit(1, 'mm'))
  print(p)
}
```

### Difference tables
#### Go-decision diff tables
```{r, sdfkjlhsdfg, dependson=-1, echo=FALSE, results='asis'}
for (curr_phase in c('M1')) {
  for (curr_src in unique(span$src)) {
    tmp <- span %>%
      filter(prize_phase == curr_phase) %>%
      filter(src == curr_src) %>%
      unnest_legacy(data, pred_diff) %>%
      group_by(pred_go, publ_inefficiency) %>%
      summarize(mean_pred_diff = mean(pred_diff)) %>%
      ungroup %>%
      select(pred_go, publ_inefficiency, mean_pred_diff) %>%
      mutate(mean_pred_diff = round(mean_pred_diff/10^6, 1),
             pred_go = round(pred_go * 100, 0),
             publ_inefficiency = round(publ_inefficiency * 100, 0)) %>%
      spread(publ_inefficiency, mean_pred_diff) %>%
      kable(caption=sprintf('%s %s Mean cost-savings of direct funding per go-decision', curr_phase, curr_src)) %>%
      print
  }
}
```

#### Market entry diff tables
```{r, sdfkjlhsdfg2, dependson=-1, echo=FALSE, results='asis'}
for (curr_phase in c('M1')) {
  for (curr_src in unique(span$src)) {
    tmp <- span %>%
      filter(prize_phase == curr_phase) %>%
      filter(src == curr_src) %>%
      unnest_legacy(data, pred_market_diff) %>%
      group_by(pred_go, publ_inefficiency) %>%
      summarize(mean_pred_market_diff = mean(pred_market_diff)) %>%
      ungroup %>%
      select(pred_go, publ_inefficiency, mean_pred_market_diff) %>%
      mutate(mean_pred_market_diff = round(mean_pred_market_diff/10^6, 1),
             pred_go = round(pred_go * 100, 0),
             publ_inefficiency = round(publ_inefficiency * 100, 0)) %>%
      spread(publ_inefficiency, mean_pred_market_diff) %>%
      kable(caption=sprintf('%s %s Mean cost-savings of direct funding per market entry', curr_phase, curr_src)) %>%
      print
  }
}
```


## Resampled scenarios
To consider the cost difference between direct and indirect funding in a specific scenario we combine the prize sizes identified in the section [Effective prize sizes](#effective-prize-sizes) with a public sector inefficiency of 50%:

We replace the old values in the `valuable` dataset with these new values and then rerun it through the `valuation` function.
However, we only keep the `r M = 500; M` first samples for each source/indication since significantly fewer samples are needed when prize size and public inefficiencies are not stochastic.

```{r, sdhlkfdss, include=FALSE}
take_subjects <- function(df, m)
  df %>%
    group_by(src, prize_phase) %>%
    filter(subject >= min(subject) & subject < min(subject) + m) %>%
    ungroup

```

We construct two scenarios, A and B, where the prize size in A is indication and phase specific, and the prize size in B is the largest effective prize size for that phase (i.e. only phase specific).
Scenario B thus models the idea of not targeting interventions but still choosing a known effective prize size.

```{r, scenarios}
scenarioA <- valuable %>%
  take_subjects(M) %>%
  # Remove old stochastic values
  select(-prize_size, -publ_inefficiency) %>%
  # Replace with scenario
  merge(.,
        effectives %>% rename(prize_size = pred_prize_size),
        by=c('src', 'prize_phase')) %>%
  # Add public inefficiency estimate
  mutate(publ_inefficiency = 0.5) %>%
  # Run valuation but only keep final
  valuation %>%
  final_valuation

scenarioB <- valuable %>%
  take_subjects(M) %>%
  # Remove old stochastic values
  select(-prize_size, -publ_inefficiency) %>%
  # Replace with scenario
  merge(.,
        (effectives %>%
         group_by(prize_phase) %>%
         summarize(prize_size = max(pred_prize_size)) %>%
         ungroup),
        by=c('prize_phase')) %>%
  # Add public inefficiency estimate
  mutate(publ_inefficiency = 0.5) %>%
  # Run valuation but only keep final
  valuation %>%
  final_valuation
```

```{r, ljhfsk, dependson=-1, echo=FALSE, results='asis'}
tmp <- scenarioA %>%
  group_by(src, prize_phase) %>%
  summarize(mean_diff = mean(inef_dir_enpv - ind_enpv))
tmp %>%
  mutate(mean_diff = round(mean_diff/10^6)) %>%
  spread(prize_phase, mean_diff) %>%
  kable(caption='Narrow prizes (per go-decision) (million usd)')
tmp %>%
  mutate(favorable = ifelse(mean_diff > 0, 'Direct', 'Indirect')) %>%
  ggplot(aes(src, abs(mean_diff) / 10^6, fill=favorable)) +
  geom_col() +
  facet_wrap(.~prize_phase, ncol=1, strip.position='right') +
  scale_y_continuous(breaks = pretty_breaks(n = 15)) +
  xlab(element_blank()) +
  ylab('Difference (million usd)') +
  coord_flip() +
  ggtitle('Narrow prizes (per go-decision)')

tmp <- scenarioA %>%
  group_by(src, prize_phase) %>%
  summarize(mean_diff = mean(inef_dir_enpv/prob_to - ind_enpv/prob_to))
tmp %>%
  mutate(mean_diff = round(mean_diff/10^6)) %>%
  spread(prize_phase, mean_diff) %>%
  kable(caption='Narrow prizes (per market entry) (million usd)')
tmp %>%
  mutate(favorable = ifelse(mean_diff > 0, 'Direct', 'Indirect')) %>%
  ggplot(aes(src, abs(mean_diff) / 10^6, fill=favorable)) +
  geom_col() +
  facet_wrap(.~prize_phase, ncol=1, strip.position='right') +
  scale_y_continuous(breaks = pretty_breaks(n = 15)) +
  xlab(element_blank()) +
  ylab('Cost difference (million usd)') +
  coord_flip() +
  ggtitle('Narrow prizes (per market entry)')

tmp <- scenarioB %>%
  group_by(src, prize_phase) %>%
  summarize(mean_diff = mean(inef_dir_enpv - ind_enpv))
tmp %>%
  mutate(mean_diff = round(mean_diff/10^6)) %>%
  spread(prize_phase, mean_diff) %>%
  kable(caption='Broad prizes (per go-decision) (million usd)')
tmp %>%
  mutate(favorable = ifelse(mean_diff > 0, 'Direct', 'Indirect')) %>%
  ggplot(aes(src, abs(mean_diff) / 10^6, fill=favorable)) +
  geom_col() +
  facet_wrap(.~prize_phase, ncol=1, strip.position='right') +
  scale_y_continuous(breaks = pretty_breaks(n = 15)) +
  xlab(element_blank()) +
  ylab('Cost difference (million usd)') +
  coord_flip() +
  ggtitle('Broad prizes (per go-decision)')

tmp <- scenarioB %>%
  group_by(src, prize_phase) %>%
  summarize(mean_diff = mean(inef_dir_enpv/prob_to - ind_enpv/prob_to))
tmp %>%
  mutate(mean_diff = round(mean_diff/10^6)) %>%
  spread(prize_phase, mean_diff) %>%
  kable(caption='Broad prizes (per market entry) (million usd)')
tmp %>%
  mutate(favorable = ifelse(mean_diff > 0, 'Direct', 'Indirect')) %>%
  ggplot(aes(src, abs(mean_diff) / 10^6, fill=favorable)) +
  geom_col() +
  facet_wrap(.~prize_phase, ncol=1, strip.position='right') +
  scale_y_continuous(breaks = pretty_breaks(n = 15)) +
  xlab(element_blank()) +
  ylab('Cost difference (million usd)') +
  coord_flip() +
  ggtitle('Broad prizes (per market entry)')
```


## Comparing resampled and predicted (linear)

Quick comparison of resampled and predicted cost differences for the specific scenario outlined in the resampling section (for sanity checking purposes).

TODO: Should also compare to the non-linear models outputs.

```{r, ksalshfds, echo=FALSE, dependson=-1}
predicted <- span %>%
  unnest_legacy(data, pred_diff)  %>%
  mutate(pred_go = pred_go * 100,
         publ_inefficiency = publ_inefficiency * 100,
         pred_diff = round(pred_diff / 10^6)) %>%
  rename(diff = pred_diff) %>%
  filter(pred_go == 95 & publ_inefficiency == 50) %>%
  select(-pred_go, -publ_inefficiency)

resampled <- scenarioA %>%
  group_by(src, prize_phase) %>%
  summarize(diff = mean(inef_dir_enpv - ind_enpv)) %>%
  mutate(diff = round(diff/10^6))

predicted %>%
  spread(prize_phase, diff) %>%
  kable(caption='Predicted: diff per go-decision.')

resampled %>%
  spread(prize_phase, diff) %>%
  kable(caption='Re-sampled: diff per go-decision.')

bind_rows(predicted %>% mutate(type = 'predicted'),
          resampled %>% mutate(type = 'resampled')) %>%
ggplot(aes(as.numeric(prize_phase), diff, color=type)) +
  facet_wrap(. ~ src, scale='free') +
  geom_point() +
  geom_line() +
  ylab('Cost difference') +
  theme(legend.position='top')
```

Black is resampled, red is predicted.
Large black dot is mean, small black dot is median.

```{r, lkhsdfksdhf, echo=FALSE, dependson=-1}
predicted <- span %>%
  unnest_legacy(data, pred_diff)  %>%
  mutate(pred_go = pred_go * 100,
         publ_inefficiency = publ_inefficiency * 100,
         pred = round(pred_diff / 10^6)) %>%
  filter(pred_go == 95 & publ_inefficiency == 50) %>%
  select(-pred_go, -publ_inefficiency)

resampled <- scenarioA %>%
  group_by(src, prize_phase) %>%
  mutate(inef_dir_enpv = inef_dir_enpv / 10^6,
         ind_enpv = ind_enpv / 10^6) %>%
  summarize(mean_resampled = mean(inef_dir_enpv - ind_enpv),
            median_resampled = median(inef_dir_enpv - ind_enpv),
            hi_resampled = mean_resampled + sd(inef_dir_enpv - ind_enpv),
            lo_resampled = mean_resampled - sd(inef_dir_enpv - ind_enpv)
            )

merge(predicted, resampled) %>%
  ggplot(aes(prize_phase, pred)) +
  geom_pointrange(aes(ymin = lo_resampled,
                     ymax = hi_resampled,
                     y = mean_resampled)) +
  geom_point(aes(y = median_resampled)) +
  geom_point(color='red') +
  facet_wrap(. ~ src, scale='free')
```

Let's look at the heatmap of CIAI and CUTI again since it appears that the last phase is not the most expensive phase, comparatively.

```{r, fkhflsf, dependson=-1, echo=FALSE}
for (curr in c('CIAI', 'CUTI')) {
  p <- span %>%
    filter(src == curr) %>%
    unnest_legacy(data, pred_diff) %>%
    ggplot(aes(pred_go*100, publ_inefficiency*100, z=pred_diff/10^6)) +
    facet_wrap(. ~ prize_phase, nrow=1) +
    geom_tile(aes(fill=pred_diff/10^6)) +
    scale_fill_gradient2(low='blue', high='red', mid='white', name='Diff', breaks=pretty_breaks(n=7)) +
    ylab('Public inefficiency') +
    xlab('P(go)') +
    ggtitle(curr) +
    theme(legend.position = 'right',
          panel.spacing = unit(1, 'mm'))
  print(p)
}
```

