# Input data
We begin by setting `N` which will dictate the number of project samples that we'll draw from each dataset.
Meaning that if one considers all the data sets at once then the number of random projects drawn is in total `N` times the number of data sets considered.
In other words, `N` is the number of random samples that we'll draw from a stochastic representation of a hypothetical antibiotic R&D project.
The distribution of the stochastic project depends on the data set we're using, and in a simple attempt to avoid drawing data set specific conclusions, we'll look at multiple data sets.

```{r, set-n}
N = 100
```

```{r, basics, include=FALSE}
# Dependencies
library(knitr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(triangle)
library(scales)

# Cache code chunks
knitr::opts_chunk$set(cache=TRUE, collapse=TRUE)

# Set seed for reproducibility
set.seed(1)

# NOTE: I can't figure out how to sample from multiple triangular distributions
# at the same time using rtriangle, so here's my sloppy quickfix implementation.
# Usage is the same as rtriangle, but you can pass vectors in mins, maxs and mids.
rtriangle2 <- function(n, mins, maxs, mids) {
  xs <- c()
  while (length(xs) < n)
    for (x in 1:length(mins))
      xs <- c(xs, rtriangle(1, mins[x], maxs[x], mids[x]))
  xs
}

# The phases we'll be working with
DEVELOPMENT_PHASES <- c('PC','P1','P2','P3','P4')
MARKET_PHASES <- paste('M', 1:20, sep='')
PHASES <- c(DEVELOPMENT_PHASES, MARKET_PHASES)
```

The data in this analysis stem from two publications, Sertkaya et. al (2014) and DRIVE-AB (2018).
The latter considers only a single hypothetical antibiotic, while the former considers six hypothetical antibiotics targeting different indications.
Our analysis is thus applied to seven data sets stemming from two publications.
Disclaimer: The data in DRIVE-AB (2018) is heavily based on Sertkaya et. al (2014) and is thus not an independently collected dataset.
Nevertheless the spread of data across the seven datasets will hopefully convince the reader that our conclusions hold over a wide range of input data variation.



## Sertkaya distribution

The following is an approximation of the data used by Sertkaya et. al (2014).
We say approximation as a few of their assumptions are not reconcilable with our modeling choices.
These compromises are described when encountered.

Sertkaya et. al (2014) differentiates between antibiotics on the basis of target indication.
In our analysis we sometimes use the term dataset source (or src for short) to refer to indication since we treat each indication as a separate dataset.

The table below summarize the distribution of development data as per 3.2.2 -- 3.2.5 in Sertkaya et. al (2014).
Time is expressed in months, cost and revenues in million USD, and probabilities and market shares in percentage.

We use the term phase 4 (abbreviated P4) while Sertkaya et. al (2014) use the term NDA/BLA.

```{r, dist-sertkaya, echo=FALSE}
INDICATIONS <- c('ABOM', 'ABSSSI', 'CABP', 'CIAI', 'CUTI', 'HABP/VABP')

sertkaya2014_phase_dist <-
  rbind(data.frame(indication = INDICATIONS,
                   phase = 'PC',
                   time_min = 52,
                   time_mid = 66,
                   time_max = 72,
                   cost_min = 19,
                   cost_mid = 21.1,
                   cost_max = 23.2,
                   prob_min = 17.5,
                   prob_mid = 35.2,
                   prob_max = 69
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P1',
                   time_min = 9,
                   time_mid = 10.5,
                   time_max = 21.6,
                   cost_min = 7.3,
                   cost_mid = 9.7,
                   cost_max = 12,
                   prob_min = 25,
                   prob_mid = 33,
                   prob_max = 83.7
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P2',
                   time_min = c(12, 9, 12, 10, 10, 16),
                   time_mid = c(15, 10, 15, 11, 11, 18),
                   time_max = 30,
                   cost_min = c(7.4, 7.12, 7.28, 7.68, 7.28, 12.48),
                   cost_mid = c(9.2, 8.9, 9.1, 9.6, 9.1, 15.6),
                   cost_max = c(11, 10.68, 10.92, 11.52, 10.92, 18.72),
                   prob_min = 34,
                   prob_mid = 50,
                   prob_max = 74
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P3',
                   time_min = c(20, 10, 10, 17, 17, 35),
                   time_mid = c(24, 12.5, 12.5, 21.5, 21.5, 39),
                   time_max = 47,
                   cost_min = c(33.36, 26.88, 31.04, 40.48, 35.04, 81.12),
                   cost_mid = c(41.7, 33.6, 38.8, 50.6, 43.8, 101.4),
                   cost_max = c(50.04, 40.32, 46.56, 60.72, 52.56, 121.68),
                   prob_min = 31.4,
                   prob_mid = 67,
                   prob_max = 78.6
                   ),
        data.frame(indication = INDICATIONS,
                   phase = 'P4',
                   time_min = 6,
                   time_mid = 9,
                   time_max = 12.5,
                   cost_min = 1.9588, # NDA/BLA submission cost
                   cost_mid = 1.9588, # NDA/BLA submission cost
                   cost_max = 1.9588, # NDA/BLA submission cost,
                   prob_min = 83,
                   prob_mid = 85,
                   prob_max = 99
                   )
        ) %>% arrange(phase, indication)
kable(sertkaya2014_phase_dist, caption='sertkaya2014_phase_dist')
```

Sertkaya et. al (2014) seems to not directly sample the market share distribution, but rather to sample a product launch success probability distribution, and then apply that sample to reach an estimate for every year's market share.
This ensures that the market share does not vary widely between the lower and upper bound on a yearly basis.
Instead, the point between the lower and upper bound remains constant, while the lower and upper bounds themselves vary.
Also, this ensures that no year has a lower market share than the previous year before peak year sales.

```{r, sertkaya2014-market-dist, echo=FALSE}
sertkaya2014_market_size_dist <-
  data.frame(indication = INDICATIONS,
             min = c(2720, 3070, 2290, 2530, 5760, 1780),
             max = 9230)

sertkaya2014_market_share_dist <-
  data.frame(year = 1:20,
             min = c(0.05, 0.87, 1.57, 2.57, 3.92, 5.79, 7.52, 8.52, 10.10, rep(12.27, 11)),
             max = c(0.11, 1.91, 3.47, 5.68, 8.64, 12.77, 16.59, 18.80, 22.30, rep(27.08, 11)))

kable(sertkaya2014_market_size_dist, caption='sertkaya2014_market_size_dist')
kable(sertkaya2014_market_share_dist, caption='sertkaya2014_market_share_dist')
```

In line with 3.2.11 of Sertkaya et. al (2014) we assume a total product life (i.e. market life) of 20 years.
In line with 3.2.9-3.2.10 of Sertkaya et. al (2014) we assume that patent expiry leads to a reduction in revenues due to generic entry.
The time (year) of generic entry (i.e. patent expiry) and the corresponding reduction of revenues are distributed as per the table below.
In summary this means that the captured market share will increase from year 1 to year 10, and then remain constant until generic entry (i.e. patent expiry), upon which it will be reduced to a lower constant until year 20.

The table below also reports the discount rates which stem from 3.2.1 of Sertkaya et. al (2014) who use the term real opportunity cost of capital.

```{r, echo=FALSE}
sertkaya2014_market_reduction_min <- 25 # percent
sertkaya2014_market_reduction_mid <- 50 # percent
sertkaya2014_market_reduction_max <- 75 # percent
sertkaya2014_generic_entry_min <- 10 # years
sertkaya2014_generic_entry_mid <- 12 # years
sertkaya2014_generic_entry_max <- 14 # years
sertkaya2014_launch_success_min <- 40 # percent
sertkaya2014_launch_success_mid <- 60 # percent
sertkaya2014_launch_success_max <- 80 # percent
sertkaya2014_private_discount_rate_min <- 9 # percent
sertkaya2014_private_discount_rate_mid <- 11 # percent
sertkaya2014_private_discount_rate_max <- 24 # percent

tmp <- data.frame(parameter = c('Product launch success probability (%)',
                                'Market reduction due to generic entry (%)',
                                'Generic entry (year)',
                                'Private developer discount rate (%)'),
                  min = c(sertkaya2014_launch_success_min,
                          sertkaya2014_market_reduction_min,
                          sertkaya2014_generic_entry_min,
                          sertkaya2014_private_discount_rate_min),
                  mid = c(sertkaya2014_launch_success_mid,
                          sertkaya2014_market_reduction_mid,
                          sertkaya2014_generic_entry_mid,
                          sertkaya2014_private_discount_rate_mid),
                  max = c(sertkaya2014_launch_success_max,
                          sertkaya2014_market_reduction_max,
                          sertkaya2014_generic_entry_max,
                          sertkaya2014_private_discount_rate_max))
kable(tmp)
```

Sertkaya et. al (2014) also consider costs for a few additional (1) supply chain activities, (2) non-clinical work, and (3) post-approval studies as listed below.
The costs are spread across various phases as indicated by the percentages under the corresponding phase columns.
These figures and percentages stem from Table 9, section 3.2.7, and section 3.2.8 of Sertkaya et. al (2014) respectively.

Sertkaya et. al (2014) report that the cost of post-approval studies may last up to three years following market entry.
As such we assume that the cost is evenly distributed over three years.

```{r, sertkaya-additional-dist, echo=FALSE}
activity <-
  c('Sample preparation for animal/human studies',
    'Process research/development/design',
    'Plant design',
    'Plant build',
    'Non-clinical work',
    'Post-approval studies')
sertkaya2014_additional_dist <-
  data.frame(activity,
             min = c(2.4, 18.7, 10.7, 69.6, 3.4, 8),
             mid = c(2.7, 26.8, 13.4, 83,   3.7, 10),
             max = c(2.9, 34.8, 16.1, 96.3, 4,   12),
             PC = 0,
             P1 = c(1/3, 0.5, 0,    0, 0,   0),
             P2 = c(1/3, 0.5, 0,    0, 1/3, 0),
             P3 = c(1/3, 0,   0.75, 0, 1/3, 0),
             P4 = c(0,   0,   0.25, 1, 1/3, 0),
             M1 = c(0,   0,   0,    0, 0,   1/3),
             M2 = c(0,   0,   0,    0, 0,   1/3),
             M3 = c(0,   0,   0,    0, 0,   1/3)
  )

sertkaya2014_additional_dist %>%
  mutate(P1 = round(P1, 2) * 100,
         P2 = round(P2, 2) * 100,
         P3 = round(P3, 2) * 100,
         P4 = round(P4, 2) * 100,
         M1 = round(M1, 2) * 100,
         M2 = round(M2, 2) * 100,
         M3 = round(M3, 2) * 100) %>%
  kable(caption='sertkaya2014_additional_dist (million usd and rounded percentages)')
```

## Sertkaya sample
We first sample development data from the Sertkaya distributions.

```{r, sample-sertkaya-dev}
sertkaya2014 <- sertkaya2014_phase_dist %>%
  # Replicate every row N number of times
  uncount(N) %>%
  # Add subject ids but group by phase so identifiers are reused across phases
  group_by(phase) %>% mutate(subject = 1:n()) %>% ungroup() %>%
  # Sample distributions
  mutate(time = rtriangle2(n(), time_min, time_max, time_mid),
         cost = rtriangle2(n(), cost_min, cost_max, cost_mid),
         prob = rtriangle2(n(), prob_min, prob_max, prob_mid)) %>%
  # Normalize sampled data
  mutate(cost = cost * 10^6,    # Convert million usd to usd
         time = time / 12,      # Convert months to years
         prob = prob / 100) %>% # Convert percentage to fraction
  # Add hard-coded development data
  mutate(sales = 0) %>%
  # Rename indication column and remove temp cols
  rename(src = indication) %>%
  select(subject, src, phase, time, cost, prob, sales)
```

We then sample market data from the Sertkaya distributions and merge it with our development phases sample.
<!-- First, we pair up every subject with the market size distribution of its indication.... -->

```{r, sample-sertkaya-market}
sertkaya2014 <- sertkaya2014 %>%
  # Only keep source and subject column and distinct rows
  select(src, subject) %>% unique %>%
  # Left join with size distribution
  merge(., sertkaya2014_market_size_dist, by.x='src', by.y='indication', all.x=TRUE) %>%
  # Sample market size distribution, then remove distribution columns
  mutate(market_size = runif(n(), min, max)) %>%
  select(-min, -max) %>%
  # Normalize sampled market size
  mutate(market_size = market_size * 10^6) %>% # Convert million usd to usd
  # Sample non-indication specific market parameters
  mutate(launch_success = rtriangle2(n(),
                                     sertkaya2014_launch_success_min,
                                     sertkaya2014_launch_success_max,
                                     sertkaya2014_launch_success_mid),
         generic_entry = rtriangle2(n(),
                                    sertkaya2014_generic_entry_min,
                                    sertkaya2014_generic_entry_max,
                                    sertkaya2014_generic_entry_mid),
         generic_reduction = rtriangle2(n(),
                                        sertkaya2014_market_reduction_min,
                                        sertkaya2014_market_reduction_max,
                                        sertkaya2014_market_reduction_mid)) %>%
  # Normalize non-indication specific market parameters
  mutate(launch_success = launch_success / 100, # Percentage to fraction
         generic_reduction = generic_reduction / 100, # Percentage to fraction
         generic_entry = round(generic_entry)) %>% # Round to full year
  # Cartesian product merge with market share dist
  merge(., sertkaya2014_market_share_dist) %>%
  # Normalize market share dist
  mutate(min = min / 100, max = max / 100) %>% # Percentages to fractions
  # Compute captured market share per year according to Sertkaya et. al (2014) Table 12
  mutate(share = (min * (1 - launch_success)) + (max * launch_success)) %>%
  # Compute reduction in market due to generic entry (Sertkaya et. al, 2014, 3.2.10)
  mutate(share = ifelse(year < generic_entry, share, (share - (share * generic_reduction)))) %>%
  # Compute yearly sales
  mutate(sales = share * market_size) %>%
  # Convert year to character factor matching MARKET_PHASES
  mutate(phase = sprintf('M%s', year)) %>%
  # Add hard-coded market data
  mutate(time = 1, prob = 1, cost = 0) %>%
  # Remove temporary columns
  select(subject, src, phase, time, cost, prob, sales) %>%
  # Add to initial dev sample
  rbind(sertkaya2014, .)
```

Finally, we sample the additional supply chain activity costs data,
distribute the samples across the development and market phases according to their corresponding fractions,
and sum up the additional costs per phase for every subject.
From this point onwards it is therefore impossible to distinguish additional supply chain costs (e.g. non-clinical work costs) from clinical phase costs.
These additional costs are then added into the full sample.

```{r, sample-sertkaya-additional}
sertkaya2014 <- sertkaya2014 %>%
  # Only keep source column and distinct rows
  select(subject) %>% unique %>%
  # Cartesian product with additional costs distribution
  merge(., sertkaya2014_additional_dist) %>%
  # Normalize additional costs
  mutate(min = min * 10^6,      # Convert million usd to usd
         mid = mid * 10^6,      # Convert million usd to usd
         max = max * 10^6) %>%  # Convert million usd to usd
  # Sample additional costs
  mutate(tot_add_cost = rtriangle2(n(), min, max, mid)) %>%
  # Remove temporary columns
  select(-min, -mid, -max, -activity) %>%
  # Wide to long: make phase names rows instead of columns
  gather(phase, fraction, -subject, -tot_add_cost) %>%
  # Compute actual additional costs in phase
  mutate(add_cost = tot_add_cost * fraction) %>%
  # Sum up costs per phase for every subject
  group_by(subject, phase) %>%
  summarize(add_cost = sum(add_cost)) %>%
  # Left join with full sample (NOTE: causes NAs where no additional costs)
  merge(sertkaya2014, ., by=c('subject', 'phase'), all.x = TRUE) %>%
  # Add additional cost to regular cost and remove temp column
  mutate(cost = ifelse(is.na(add_cost), cost, cost + add_cost)) %>%
  select(-add_cost)
# TODO: I could also do a merge earlier on subject and not phase, and then add
# if phase matches. That might look a bit messier but it should be more
# efficient and doesn't introduce NAs. Is this a better solution?
```

Finally we sample developer discount rates and add it to our Sertkaya sample.

```{r, sertkaya-add-discount-rate}
min <- sertkaya2014_private_discount_rate_min / 100 # Percentage to fraction
max <- sertkaya2014_private_discount_rate_max / 100 # Percentage to fraction
mid <- sertkaya2014_private_discount_rate_mid / 100 # Percentage to fraction
sertkaya2014 <- sertkaya2014 %>%
  # Group by subject
  group_by(subject) %>%
  # Sample single discount rate per subject, then ungroup
  mutate(discount_rate_priv = rtriangle2(1, min, max, mid)) %>%
  ungroup()
```


## DRIVE-AB distribution
The following is an approximation of the data used in DRIVE-AB final report (2018).
Some deviations (reported below) have been made as some of the DRIVE-AB assumptions are not compatible with our assumptions.

In the table below, time is reported in months, cost in million USD, and probability in percentages.

```{r, driveab-dist, echo=FALSE}
driveab2018_phase_dist <-
  data.frame(phase = DEVELOPMENT_PHASES,
             time_min = c(52, 9, 9, 10, 6),
             time_mid = c(66, 10.5, 13.33, 21.8, 9),
             time_max = c(72, 21.6, 30, 47, 12.5),
             # TODO: Cost of P2 is incorrectly reported in DRIVE-AB Final Report!
             # We're using the numbers from Okhravi et al. (2018)
             cost_min = c(14.25, 13.1,  12.95,  27.99, 55.5),
             cost_mid = c(21.1,  24,    24.55,  62.6,  88.35),
             cost_max = c(29,    37.96, 46.36,  168.4, 127.91),
             prob_min = c(17.5, 25, 34, 31.4, 83),
             prob_mid = c(35.2, 33, 50, 67, 85),
             prob_max = c(69, 83.7, 74, 78.6, 99))

kable(driveab2018_phase_dist, caption = 'driveab2018_phase_dist')
```


```{r, driveab-additional-params, echo=FALSE}
driveab2018_tot_sales_min = 0
driveab2018_tot_sales_mid = 2559.5
driveab2018_tot_sales_max = 4336
driveab2018_market_years = 10
driveab2018_discount_rate_priv_min = 0.05 * 100
driveab2018_discount_rate_priv_max = 0.3 * 100
```

In line with the assumptions of DRIVE-AB (2018) we assume that sales revenue linearly increase over a period of `r driveab2018_market_years` years.
Patent expiry is assumed to occur at this point and we assume that all revenues drop to 0 beyond this point.
Specifically, we assume that the revenues of year 0 is 0 and then linearly increase until the area under the curve is equal to the total global net sales.
The first year will thus be non-zero if the market revenues of the project is non-zero.

```{r, print-driveab-additional-params, echo=FALSE}
data.frame(parameter = 'Total sales (million usd)',
           min = driveab2018_tot_sales_min,
           mid = driveab2018_tot_sales_mid,
           max = driveab2018_tot_sales_max) %>% kable

data.frame(parameter = 'Private developer discount rate (%)',
           min = driveab2018_discount_rate_priv_min,
           max = driveab2018_discount_rate_priv_max) %>% kable
```


## DRIVE-AB sample

We first sample development data from the DRIVE-AB distribution.

```{r, sample-driveab-dev}
first_id <- max(sertkaya2014$subject) + 1
driveab2018 <- driveab2018_phase_dist %>%
  # Replicate every row N number of times
  uncount(N) %>%
  # Add subject ids but group by phase so identifiers are reused across phases
  group_by(phase) %>%
  mutate(subject = first_id:(first_id + n() - 1)) %>%
  ungroup() %>%
  # Sample phase data and add hard-coded data
  mutate(time = rtriangle2(n(), time_min, time_max, time_mid),
         cost = rtriangle2(n(), cost_min, cost_max, cost_mid),
         prob = rtriangle2(n(), prob_min, prob_max, prob_mid),
         sales = 0) %>%
  # Remove temp columns
  select(subject, phase, time, cost, prob, sales)
```

We then sample market data from the DRIVE-AB distribution and add it to the sampled development data.
Lastly, we sample discount rates and also add it into the full sample.

```{r, sample-driveab-market}
driveab2018 <- driveab2018 %>%
  # Select only subject column and unique rows
  select(subject) %>% unique %>%
  # Sample market size
  mutate(tot_sales = rtriangle2(n(),
                                driveab2018_tot_sales_min,
                                driveab2018_tot_sales_max,
                                driveab2018_tot_sales_mid)) %>%
  # Compute market slope
  mutate(slope = (tot_sales * 2 / (driveab2018_market_years + 1)) / driveab2018_market_years) %>%
  # Replicate as many times as we want market years
  uncount(driveab2018_market_years) %>%
  # Add market years to every subject
  group_by(subject) %>%
  mutate(market_year = 1:driveab2018_market_years) %>%
  ungroup %>%
  # Add hard-coded data
  mutate(time = 12, cost = 0, prob = 100) %>%
  # Compute (interpolate) yearly sales
  mutate(sales = market_year * slope) %>%
  # Convert market year to character factor
  mutate(phase = paste('M', market_year, sep='')) %>%
  # Drop temp columns
  select(subject, phase, time, cost, prob, sales) %>%
  # Add to initial dev sample
  rbind(driveab2018, .) %>%
  # Add data source name
  mutate(src = 'DRIVE-AB (2018)') %>%
  # Convert phase to ordered factor
  mutate(phase = factor(phase, levels=PHASES, ordered=TRUE)) %>%
  # Sample discount rate on a per-subject basis
  group_by(subject) %>%
  mutate(discount_rate_priv = runif(1,
                                    driveab2018_discount_rate_priv_min,
                                    driveab2018_discount_rate_priv_max)) %>%
  ungroup %>%
  # Normalize sampled values
  mutate(time = time / 12,     # months -> years
         cost = cost * 10^6,   # million usd -> usd
         sales = sales * 10^6, # million usd -> usd
         prob = prob / 100,    # percentage -> fraction
         discount_rate_priv = discount_rate_priv / 100 # percentage -> fraction
         ) %>%
  # Arrange data nicely for printing purposes
  arrange(subject, phase)
```


## New assumptions
Before proceeding, we'll combine all datasets (sources) into a single dataset containing all sources.

```{r, combine}
phases <-
  rbind(sertkaya2014, driveab2018) %>%
  arrange(subject, src, phase) %>%
  # Convert phase column to ordered factor
  mutate(phase = factor(phase, levels=PHASES, ordered=TRUE))
```

```{r, precompute, include=FALSE, dependson=-1}
# To avoid recomputing all the time we'll also store the names of the different sources:
sources <- unique(phases$src)
# To avoid misspellings of factors, let's use levels:
metrics <- c('cashflow', 'ev', 'pv', 'epv')
cum_metrics <- c('cum', 'env', 'npv', 'enpv')
```

Let us add a "public" or "social" discount rate.
Meaning the cost of capital for the benefactor, i.e. for the body that pays the intervention with no expectation of monetary return.
This parameter is used in the analysis, where its raison d'etre is also further explained.

We assume that the benefactor is the public sector and use a uniformly distributed discount rate.
Public discount rate varies across subjects but not within.

In the analysis we will also explore the effect of various levels of public sector inefficiency.
The assumptions are explained in further detail in the analysis section.
In most of the calculations, the inefficiency parameter is ignored, and when used we will control for it.
As such, we can explore a wide range of inefficiencies.

```{r, sample-discount-rates-and-inefficiency, dependson=-1}
public_discount_rate_min = 0.035
public_discount_rate_max = 0.045

public_inefficiency_min <- 0
public_inefficiency_max <- 3

subjects <- unique(phases$subject)
agents <- data.frame(
  subject = subjects,
  inefficiency = runif(length(subjects), public_inefficiency_min, public_inefficiency_max),
  discount_rate_publ = runif(length(subjects), public_discount_rate_min, public_discount_rate_max))

phases <- merge(phases, agents, by='subject', all.x=TRUE)
```


## Summary statistics
We now present some summary statistics to sanity check that our samples seem to be properly drawn from their respective distributions.
We first plot development data.

```{r, summary-violins, echo=FALSE, warning=FALSE, message=FALSE}
phases %>%
  filter(phase %in% DEVELOPMENT_PHASES) %>%
  ggplot(aes(phase, cost/10^6, color=src, fill=src)) +
  geom_violin() +
  facet_wrap(. ~ phase, scale='free') +
  theme(legend.position='top',
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank()) +
  ylab('Cost (million USD)')
phases %>%
  filter(phase %in% DEVELOPMENT_PHASES) %>%
  ggplot(aes(phase, time, color=src, fill=src)) +
  geom_violin() +
  facet_wrap(. ~ phase, scale='free') +
  theme(legend.position='top',
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank()) +
  ylab('Time (months)')
phases %>%
  filter(phase %in% DEVELOPMENT_PHASES) %>%
  ggplot(aes(phase, prob*100, color=src, fill=src)) +
  geom_violin() +
  facet_wrap(. ~ phase, scale='free') +
  theme(legend.position='top',
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank()) +
  ylab('Probability of success (%)')
```

Below we plot the sampled sales data.

```{r, summary-sales, echo=FALSE}
phases %>%
  filter(phase %in% MARKET_PHASES) %>%
  ggplot(aes(phase, sales/10^6, color=src, fill=src)) +
  geom_violin() +
  facet_wrap(. ~ phase, scale='free') +
  theme(legend.position='top',
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank()) +
  ylab('Sales (million USD)')

# TODO: Replace this plot with lines of max/min or similar
phases %>%
  filter(phase %in% MARKET_PHASES) %>%
  ggplot(aes(phase, sales/10^6, color=src, fill=src)) +
  geom_violin() +
  theme(legend.position='top',
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank()) +
  ylab('Sales (million USD)')

phases %>%
  group_by(src, subject) %>%
  summarize(tot_sales = sum(sales)) %>%
  ggplot(aes(src, tot_sales/10^6, fill=src)) +
  geom_violin() +
  theme(legend.position='top',
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank()) +
  ylab('Total sales (million USD)')
```

Finally, we plot the discount rate data.

```{r, summary-discount-rates, echo=FALSE}
phases %>%
  filter(phase %in% DEVELOPMENT_PHASES) %>%
  ggplot(aes(phase, discount_rate_priv*100, color=src, fill=src)) +
  geom_violin() +
  ylab('Discount rate (%)') +
  xlab('Phase') +
  theme(legend.position = 'top')
```

Interestingly, some parameters are quite dispersly distributed between phases.
Below, we plot the mean percentage of a given property occuring in a given phase for all phases.

```{r, summary-phase-distribution-means, echo=FALSE, warning=FALSE, message=FALSE}
# Transform: phase properties to long from wide
phase_props <- phases %>%
  filter(phase %in% DEVELOPMENT_PHASES) %>%
  select(src, subject, phase, cost, time, prob) %>%
  gather(key='prop', value='value', -src, -subject, -phase) %>%
  group_by(src, subject, prop) %>%
  mutate(total = sum(value),
         ratio = value / total) # NOTE: will cause NaN if 0/0

phase_props %>%
  group_by(src, phase, prop) %>%
  summarise(ratio = mean(ratio)) %>%
  filter(is.finite(ratio)) %>%
  ggplot(aes(src, ratio * 100)) +
  geom_bar(stat='identity', aes(fill=phase), position='stack') +
  labs(fill='') +
  facet_wrap(. ~ prop) +
  theme(axis.text.x = element_text(angle=90, hjust=1))

phase_props %>%
  group_by(src, phase, prop) %>%
  summarise(ratio = mean(ratio)) %>%
  filter(is.finite(ratio)) %>%
  ggplot(aes(phase, ratio * 100)) +
  geom_bar(stat='identity', aes(fill=src), position='dodge') +
  labs(fill='') +
  facet_wrap(. ~ prop) +
  theme(axis.text.x = element_text(angle=90, hjust=1),
        legend.position = 'top')
```

